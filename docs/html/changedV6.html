<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Vortex OpenSplice - documentation index</title>
    <style type="text/css" media="all">
      @import url("../css/maven-base.css");
      @import url("../css/maven-theme.css");
      @import url("../css/site.css");
    </style>
    <link rel="stylesheet" href="./docs/css/print.css" type="text/css" media="print" />
    <meta name="Date-Revision-yyyymmdd" content="20141002" />
    <meta http-equiv="Content-Language" content="en" />

 </head>
 <body class="composite">
    <div id="banner">
       <a href="http://www.prismtech.com" id="bannerLeft">
          <img src="../images/Prismtech_Logo.png" alt="Prismtech" />
       </a>
       <a href="http://www.prismtech.com/vortex/vortex-opensplice" id="bannerRight">
          <img src="../images/Wide_OpenSplice.png" alt="Vortex Lite" />
       </a>
       <div class="clear">
          <hr/>
       </div>
    </div>
    <div id="breadcrumbs">
       <div class="xright">
          <a href="http://www.prismtech.com" class="externalLink" title="Prismtech on the Web: www.prismtech.com">Prismtech on the Web: www.prismtech.com</a>
          | <a href="http://www.prismtech.com/vortex/vortex-opensplice" title="OpenSplice">Vortex OpenSplice</a>
       </div>
       <div class="clear">
         <hr/>
       </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
         <h5>Vortex OpenSplice</h5>
         <ul>
            <li class="none">
            <a href="../../index.html" title = "Intro">Introduction</a>
            </li>
            <li class="none">
              <a href="guides.html" title="APIs">User Guides</a>
            </li>
            <li class="none">
              <a href="changedV6.html" title="Changes"><strong>Changes</strong></a><br>
            </li>
            <li class="none">
              <a href="platforms.html" title="Supported Platforms">Supported Platforms</a><br>
            </li>
            <li class="none">
              <a href="known_issues.html" title="Changes">Known Issues</a><br>
            </li>
            <li class="none">
              <a href="ddsi2_release_notes.html" title="Changes">DDSI Release Notes</a><br>
            </li>
            <li class="none">
               <a href="rebuilding_APIs.html" title = "User Guide">Rebuilding the APIs</a><br>
            </li>
            <li class="none">
               <a href="upgrading.html" title = "Upgrading">Upgrading OpenSplice</a><br>
            </li>
            <li class="none">
               <a href="../../examples/dcps/README.html" title = "Examples">Examples</a><br>
            </li>
            <li class="none">
               <a href="third_party_licenses.html" title = "Third Party Licenses">Third party licenses</a><br>
            </li>
            <li class="none">
              <a href="./../../LICENSE" title="License">License</a>
            </li>
          </ul>
      </a>
      </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">

    <h2><a name="highlights">Release Highlights</a></h2>
    This page contains a list of all bug fixes and changes incorporated in
    OpenSplice V6.x series of releases<br>
    <li><a href="#issues_not_api">Fixed Bugs and Changes not affecting API</a></li>
    <li><a href="#issues_api">Fixed Bugs and Changes affecting API / behaviour</a></li>
    <h2>Vortex OpenSplice V6.7.2 Community contains the following new features:</h2>
    <p>
        <ul>
            <li>The license for OpenSplice has been changed from LGPLv3 to Apache 2.0.</li>
        </ul>
    </p>
    <h2>Vortex OpenSplice V6.7.2 contains the following new features:</h2>
    <p>
        <ul>
            <li>The OpenSplice configuration allows for the specification of a report plugin which is a dynamically loaded library 
            to which all the report messages are forwarded. A report plugin is associate with a particular domain. 
            To enable the plugin to associate a report message to a particular domain the message that is reported is extended 
            with the domain_id of the domain from which the message originates. Further report messages are only forwarded to 
            the plugin associated with the domain the report message originates. In a multi-domain configuration this enables 
            the report plugin to determine the corresponding domain. Note that not all report message can be associated with a 
            particular domain. In that case these message will be forwarded to all plugins with a domain_id of -1.</li>
            <li>The validation of the OpenSplice XML configuration file has been made more stricter. On startup of OpenSplice the 
            provided configuration file is validated and OpenSplice is not started when the configuration file contains errors. 
            For example the validation of the configuration file checks for missing mandatory elements and attributes, 
            unknown element or arttributes, or the specification of incorrect. 
            When the validation finds errors then these errors are reported in the error log.</li>
        </ul>
    </p>
    <h2>Vortex OpenSplice V6.7.1p3 newly introduces support for the following new platforms:</h2>
    <p>
        <ul>
            <li>Raspberry Pi 3 Raspbian Jessie (32 bit) Host and Target, gcc 4.9.2, ARMv7 (Debian 8)</li>
            <li>RHEL7.1 host, ElinOS 6.1 hypervised by Pike OS 4.1 Core i7 x86_64 target</li>
            <li>Ubuntu 16.04 64 bit host and target</li>
        </ul>
    </p>
    <h2>Vortex OpenSplice V6.7.1 contains the following new features:</h2>
    <p>
        <ul>
            <li>The C99 DDS API (beta) - Instead of the legacy C API which is
            based on the OMG IDL to C mapping, the C99 API offers a more natural
            and modern DDS API for the C programming language. This allows users
            to be more productive. Besides thatm the API allows users to switch
            their existing applications between Lite and OpenSplice products
            without changing their code (a recompile suffices).</li>
        </ul>
    </p>
    <h2>Vortex OpenSplice V6.7.0 contains the following new features:</h2>
    <p>
        <ul>
            <li>Group coherency support for Tuner and Tester - Both of these tools now has the ability to publish and
            subscribe with custom Presentation policies. New UI workflows have been established for working with coherent
            and ordered sets of data for group and topic level access scope. See the new sections in the
            <a href="../pdf/OpenSplice_TunerGuide.pdf">Tuner user guide</a> and the
            <a href="../pdf/OpenSplice_TesterUserGuide.pdf">Tester user guide</a> for more details.</li>
            <li>Vortex OpenSplice previously used internally time stamps that would wrap around somewhere in the year 2038.
            To allow time stamps that exceed 2038 OpenSplice now uses a new internal time representation. This time
            representation will wrap around somewhere in the year 2262, which is believed to be sufficient for all
            applications that are being written today. The internal time representation will by default not be exposed to
            applications to avoid backward compatibility issues, so existing applications should not notice this change.
            However, users can enable these extended time stamps to be exposed, so that applications can use these
            extended time stamps. This is explained in more detail in the <a href="../pdf/OpenSplice_DeploymentGuide.pdf">DeploymentGuide</a>
            section 'Time stamps and year 2038 limit'.
            <li>New TimeBasedFilterQosPolicy behaviour (See
            <a href="#issues_api">Fixed Bugs and Changes affecting API / behaviour</a>).</li>
        </ul>
    </p>
    <h2>Vortex OpenSplice V6.6.4 contains the following new features:</h2>
    <p>
        <ul>
          <li>Future Airborne Capability Environment
          (FACE) Transport Services Segment (TSS) V1.2 API for Java and C++ (BETA) -
          The FACE Standard defines the software computing environment and
          interfaces designed to support the development of portable components
          across the general-purpose, safety, and security profiles. FACE uses
          industry standards for distributed communications, programming
          languages, graphics, operating systems, and other areas as
          appropriate. The goal of FACE is to reduce software development and
          integration costs and reduce time to field new avionics capabilities.
          FACE establishes a common computing software infrastructure supporting
          portable, capability-specific software components across Department of
          Defense (DoD) avionics systems.<br/><br/>
          The FACE C++ and Java API's supplied
          with OpenSplice are targeting developers and integrators who want to
          take advantage of the Data Distribution Service (DDS) standard. For
          more information on usage, please check the examples/face directory
          within your OpenSplice installation.<br/><br/>
          Please note that more documentation will be added in a future release.</li>
       </ul>
    </p>
    <h2>Vortex OpenSplice V6.6.3p3 contains the following new features:</h2>
    <p>
        <ul>
          <li>Added Yocto 3.0.6.32 Freescale P4080ds (e500mc) target platform for RHEL 6.4 64 bit host</li>
          <li>Added RedHat Enterprise Linux 7.2 for Intel 64 bit host and target</li>
       </ul>
    </p>
    <h2>Vortex OpenSplice V6.6.3 contains the following new features:</h2>
    <p>
    <ul>
       <li>Client-side durability - In OpenSplice, the realization of the
       non-volatile properties is the responsibility of the durability service.
       This service is on one hand responsible for maintaining the set of
       historical data and on the other hand responsible for providing
       historical data to late-joining subscribers. The configurations of the
       different services drive the behavior on where and when specific data
       will be maintained and how it will be provided to late-joiners. To
       prevent the need of equipping each application with a full durability
       service in single process mode, it is now possible to equip applications
       only with a client-side durability part and run the server part on a
       different location in the system (or on multiple locations driven by
       fault-tolerance requirements). This reduces resource usage in
       single process applications drastically. See section 12.2.27
       "DurablePolicies" in the
       <a href="../pdf/OpenSplice_DeploymentGuide.pdf">deployment guide</a> for more
       details about the configuring client part and section 12.3.2
       "ClientDurability" about enabling the server part.
       </li>
       <li>Launcher updates:
          <ul>
            <li>Product release information is now accessible in
             Launcher through the "Documentation" section under the new
             "Product Information" tab. The tab contains references to the
             Vortex OpenSplice Intro, the release notes and the known issues.
            </li>
            <li>All guides and manuals are now available in Launcher.</li>
            <li>The "Memory Statistics" tool is now available in Launcher under
            the "Tools" section alongside Tester, Tuner, Configurator and the
            Console. Clicking on the mmstat button opens up a command prompt and
            run the "mmstat" command. Upon termination of the command, the
            command prompt holds to allow the user to further interact with it.</li>
            <li>Add ability to open the Visual Studio example solution files to
            Launcher - On Microsoft Windows platforms, users now have the
            ability to open the example solution files for the Vortex OpenSplice
            examples in order to inspect and build them. The new section is only
            displayed on Windows platforms in the Examples pane. Clicking on the
            Open Visual Studio solution file icon will open the selected example
            solution file in the Microsoft Visual Studio editor. The version of
            Microsoft Visual Studio that is used to open the example solution
            files is used by either inferring from the VCINSTALLDIR environment
            variable or by looking in the PATH to see if devenv.exe is found. If
            the executable cannot be found in either locations, a notification
            will pop up instructing the user to set the appropriate environment
            variables. Note: The example solution files will no longer be
            available to users from the Start Menu in Windows.</li>
          </ul>
        </li>
        <li>Improved reference docs for DDS API's - the reference documentation
        for ISOC++v2, Java5 and C# DDS API's have been significantly improved.</li>
        <li>Added Windows 7 - Visual Studio 2015 support (64-bit)</li>
        <li>Added Windows 10 - Visual Studio 2015 support (64-bit)</li>
        <li>Google Protocol Buffers support for Tuner - The Tuner tool has been
        extended with functionality to be able to write/read Topics that have
        been modeled using Google Protocol Buffers (instead of OMG IDL) similar
        to the support already available in the Tester tool. Refer to the new
        section in the <a href="../pdf/OpenSplice_TunerGuide.pdf">Tuner user
        guide</a> for more details.</li>
        <li>C++ RMI service threading and scheduling policies and service
        priority - Different threading and scheduling policies have been added
        to the C++ RMI library. The threading policies control the way a RMI
        server allocates threads to handle the incoming requests, whereas the
        scheduling policies control how these threads are scheduled within the
        operating system. Three threading policies have been defined:
        'Single Thread', 'Multi-thread' and 'Thread Per Service'. In addition,
        the notion of service priority has been introduced to express the
        business importance of each service w.r.t. the others. Requests coming
        to a service with high priority should be handled before the ones coming
        to services having less priority value. These new features have
        introduced some additions to the RMI C++ API. Please, refer to the
        <a href="../pdf/OpenSplice_RMIUserGuide.pdf">RMI documentation</a>
        for more details.
    </ul>
    <h2>Vortex OpenSplice V6.6.2 contains the following new features:</h2>
    <p>
    <ul>
       <li>
       Detach from all domains - A new operation has been added to the
       DomainParticipantFactory in the DDS API. The operation allows you to
       detach your application safely from all Domains it is participating in
       with one single call.
       </li>
       <li>
       Improved entity naming - The naming of DomainParticipant, Publisher,
       Subscriber, DataWriter and DataReader entities has been improved. Tooling
       visualizes these names and the improved naming makes it easier for users
       to relate entities to their applications.
       </li>
       <li>
       Launcher - In Vortex OpenSplice Launcher, a new section called "Controls"
       has been introduced. This section contains buttons to easily start and
       stop an OpenSplice domain based on the active ospl config file in
       Launcher. The start and stop buttons are disabled if the active ospl
       config file describes a single process deployment. When the user clicks
       on the start/stop button, a progress bar is displayed in the bottom
       tool bar to indicate that Launcher is processing the request. When
       Launcher has completed the operation, notification pop-up messages are
       displayed indicating the result of the requested operation.
       <b>Note</b>: in general all items that are now available in the
       Launcher no longer appear in the start menu on Windows platforms.
       </li>
    </ul>
    <h2>Vortex OpenSplice V6.6.1p1 contains the following new features:</h2>
    <p>
    <ul>
        <li>Added debian6 ARMv6 target platform for debian6 ia32 (x86) host</li>
        <li>Added debian7 ARMv7 hardfloat target platform for debian7 ia32 (x86) host</li>
    </ul>
    <h2>Vortex OpenSplice V6.6.1 contains the following new features:</h2>
    <p>
    <ul>
        <li>Improved out-of-memory handling for shared memory - operations that
        require memory now reserve memory prior starting the operation to be
        able to ensure beforehand whether or not the operation will succeed.
        This removes the need for configuring large memory thresholds in
        dynamic systems with large shared memory segments.</li>
        <li>Improved mutex and condition variable implementation for Windows
        platforms. This improves overall performance of the product on those
        platforms.</li>
    </ul>
    <h2>Vortex OpenSplice V6.6.0 contains the following new features:</h2>
    <p>
    <ul>
        <li>ISOC++ DCPS API version 2 - A brand new version of
        the ISOC++ specification (dcpsisocpp2) has been implemented.
        This implementation replaces the initial ISOC++ DCPS API (dcpsisocpp),
        which is deprecated now and will be removed in a future release. The new
        version addresses various issues in the initial version of the specification.
        Check out what you need to do to migrate in the
        <a href="../html/isocpp_migration.html">ISOC++V2 migration guide</a>
        </li>

        <li>Full PresentationQosPolicy support
        <ul><li>GROUP-level coherence support - The GROUP level access_scope of the
        PresentationQosPolicy is now supported. This allows representing changes
        to data instances of different DataWriters underneath a single Publisher
        to be presented to the subscribing application as a single (coherent) set.
        Please check out the API reference manual for your favorite programming
        language for more details on the PresentationQosPolicy.
        </li>
        <li>Ordered access for group and topic level access scope -
        The ordered_access property of the PresentationQosPolicy has now
        been implemented for topic and group level access scope (instance scope
        already existed). As a consequence applications now have the ability to
        see the relative order of (coherent) changes. The reference manuals contain
        further details about the behavior related to the ordered_access property.</li>
        </ul></li>

        <li>Google Protocol Buffers integration
        <ul><li>Google Protocol Buffers integration for Java5 and ISOC++V2 PSM's -
        OpenSplice is capable of using the Google Protocol Buffer (GPB)
        mechanism for modelling types in a DDS system. This makes it possible to
        use GPB as an alternative to OMG-IDL for those who prefer to use GPB
        rather than IDL. With the seamless integration of GPB and DDS technologies
        there is no need for OMG-IDL knowledge or visibility when working with
        GPB data models, and no OMG-DDS data-types are needed in the application
        (no explicit type-mapping between GPB and DDS types is required).
        Additionally, the GPB data structure can be evolved without breaking
        deployed programs that are compiled against an ‘old’ format. Check out the
        <a href="../pdf/OpenSplice_GPBTutorial.pdf">OpenSplice Google Protocol Buffers Tutorial</a>
        for more details.</li>
        <li>Google Protocol Buffers integration for the Tester tool - It is able
        to display its samples as regular field name and value pairs, just as if it
        were from a regular IDL defined topic. This feature is enabled for
        versions of OpenSplice
        that support Google Protocol Buffers. For more information, please see the new
        chapter on Google Protocol Buffers in the
        <a href="../pdf/OpenSplice_Tester_usermanual.pdf">OpenSplice Tester Guide</a>.
        </li></ul></li>

        <li>Durability historical data alignment efficiency improvements
        <ul>
        <li>Until recently, a
        late joining durability service would always request the data set from
        its master. This would even happens in case the exact same
        set is already available on disk in its local persistent store. Rather than
        aligning the data set, the late joining durability service could better inject
        the data from its own persistent store in this case. This optimization reduces
        network bandwidth and increases startup time. This feature relies on the
        ability to compare data sets using the
        //OpenSplice/DurabilityService/NameSpaces/Policy[ @equalityCheck]
        attribute in the
        <a href="../pdf/OpenSplice_DeploymentGuide.pdf">deployment guide</a>.</li>

        <li>The durability service now supports so-called 'align on change'
        functionality. temporary disconnections can cause durability services to
        get out-of-sync, meaning that their data sets may diverge. To recover
        from such situations merge policies exist that allow users to specify
        how to combine diverged data sets when they become reconnect again. Many
        of these situations involve the transfer of data sets from one
        durability service to the other. This may generate a considerable amount
        of traffic for large data sets. In case the data sets did not get
        out-of-sync during disconnection (because no data was written during
        the period of disconnection), it is not necessary to transfer data sets
        from one durability service to the other. A new configuration option
        has been implemented to force the durability service to compare data
        sets before alignment. When enabling this check a hashes are calculated
        of the data sets and compared for equality. When equal, no data will be
        aligned. This may save valuable bandwidth during alignment. If the
        hashes are different then the complete data sets will aligned.For
        further explanation and details on configuration, please check section
        about 'prevent aligning equal data sets' and the
        //OpenSplice/DurabilityService/NameSpaces/Policy[ @equalityCheck]
        attribute in the
        <a href="../pdf/OpenSplice_DeploymentGuide.pdf">deployment guide</a></li>
        <li>Durability service static content filtering -
        The durability service has been extended with support for static filtering of
        historical data. This allows for a reduction of the amount of aligned historical
        data if only a subset is required. Only data matching the statically configured
        filters will be aligned.</li>
        </ul></li>

        <li>Durability service CATCHUP merge-policy -
        The durability service now supports a new merge policy flavor called
        CATCHUP, which is similar to the existing REPLACE, but any instances that
        already exist in your local data-set and still exist in the 'remote'
        data-set, will not be marked with a NEW instance state as a result of
        the merge. This is useful in scenario's where one knows that an
        instance, once in use, is not re-used to describe a different object
        during the life-time of the data-set. For further explanation and
        details on configuration, please check section about merge-policies in
        the <a href="../pdf/OpenSplice_DeploymentGuide.pdf">deployment guide</a> </li>

        <li>The new Python Scripting Engine is now available. Python Scripting is a
        scripting environment for running unit tests and adhoc scripts against a Vortex
        OpenSplice environment and is meant as an alternative to the product specific
        Scenario language found in Vortex OpenSplice Tester. Python Scripting connects to the
        Vortex OpenSplice environment through the Configuration and Management API. It is based
        on the Python scripting language, and requires Jython 2.7.0 or later, the
        Java-based implementation of Python. For more information, please see the new
        chapter on the Python Scripting Engine in the
        <a href="../pdf/OpenSplice_Tester_usermanual.pdf">OpenSplice Tester Guide</a>.
        </li>
    </ul>
    <h2>Vortex OpenSplice V6.5.2p1 contains the following new features:</h2>
    <p>
    <ul>
        <li>Support for Ubuntu 14.04 64 bit host to Ubuntu 14.04 for custom ARM V7 Marvell</li>
    </ul>
    </p>
    <h2>Vortex OpenSplice V6.5.2 contains the following new features:</h2>
    <p>
    <ul>
        <li>Source-specific multicast support for DDSI2E (Prototype) -
        DDSI2E now supports SSM on platforms that support it (all tier-1 platforms,
        i.e., Linux, Windows). It is enabled by specifying an SSM address in a
        network partition instead of a regular multicast, and only used when both
        reader and writer have it enabled. Interoperability with other vendors is
        limited to those also supporting the proposal informally known as "D84".
        Please note that although we'll do our best to solve any issues, <b>this
        feature is not officially supported as it has only been marginally
        tested and is supplied without official documentation.</b>. For more
        details on how to use this functionality, please check
        <a href="./ddsi2_release_notes.html#SSM">SSM notes</a>
        </li>
        <li>RnR service support for QoS transformations - This feature allows
        replaying data that was recorded earlier with different QoS settings
        compared to the QoS settings of the recorded data. For more details
        check the <a href="../pdf/OpenSplice_RnRService.pdf">RnR API guide</a></li>
        <li>RnR service support to split recordings over multiple files - This
        feature allows limiting file sizes (particularly useful when recording
        large volumes of data). For more details check the
        //OpenSplice/RnRService/Storage/rr_storageAttrXML/MaxFileSize or
        //OpenSplice/RnRService/Storage/rr_storageAttrCDR/MaxFileSize settings
        in the <a href="../pdf/OpenSplice_DeploymentGuide.pdf">Deployment guide</a></li>
    </ul>
    </p>
    <h2>Vortex OpenSplice V6.5.1p1 contains the following new features:</h2>
    <p>
    <ul>
        <li>The generation of the OpenSplice system id can now be influenced through settings in the Domain/SystemId section of the OpenSplice XML domain configuration file. The Range element allows limiting the range, the UserEntropy element allows adding an arbitrary string to the initial entropy pool. This can be useful on platforms where all of the default inputs can be the same on two machines or across runs.</li>
    </ul>
    </p>
    <h2>Vortex OpenSplice V6.5.1 contains the following new features:</h2>
    <p>
    <ul>
        <li>Support for dynamic network partitions for RTNetworking - This
        feature allows users to amend the configuration of the RTNetworking
        service at runtime by means of a Topic-API. For more details, check
        the <a href="./rtn_dynamic_partitions.html">RTNetworking dynamic partitions guide</a></li>
        <li>Compression support for durability KV-persistence - The
        durability service currently has a KV-persistency implementation that
        allows persisting data to disk in either SQLite or LevelDB. Testing
        performance indicates that the disk is the bottleneck when trying to
        achieve a high throughput in some use cases. To improve throughput,
        samples can now be compressed by durability before persisting them (and
        obviously uncompressed before re-publishing them in DDS). Durability
        supports compression as a configurable option for KV persistence. For
        more details on how to configure it, please check section 4.3.3.9.3 of
        the Deployment Guide</li>
        <li>Support for Ubuntu 14.04 Linux For Tegra Nvidia Tegra K1 CPU ARM Cortex-A15 R3 host and target</li>
    </ul>
    </p>
    <h2>Vortex OpenSplice V6.5.0p10 contains the following new features:</h2>
    <p>
    <ul>
        <li>Support for Ubuntu 14.04 64 bit host to Ubuntu 14.04 for custom ARM V7</li>
    </ul>
    </p>
    <h2>Vortex OpenSplice V6.5.0 contains the following new features:</h2>
    <p>
    <ul>
        <li>A major refactoring of the API implementations (Java, C++, C#) to support
        future maintenance and feature extensions. This does not affect the API
        signature. A small performance benefit is also introduced.</li>
        <li>Following the introduction of C++ Streams at V6.2, this feature has now
        been added to the ISO C++ API. The ISO C++ Streams API provides a mechanism
        for batching samples together into a stream. This has the effect of
        increasing data throughput due to a reduced overhead which occurs when
        transmitting samples individually. The samples are appended to a stream
        and then later flushed which causes the samples to be written together as
        one batch. It is performant when a large number of samples need to be sent
        continuously. Full documentation is included in the ISO C++ documentation.</li>
        <li><b>Due to a bug fix in OpenSplice, Vortex Gateway 2.1.1 (and above)is
        required to route with OpenSplice V6.5 and above.</b></li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.4.3 contains the following new features:</h2>
    <p>
    <ul>
        <li>Java 5 language PSM for DDS - Next to the already existing OMG Java
        IDL language PSM for DDS API, OpenSplice now also supports the
        new OMG Java 5 language PSM for DDS (see
        <a href="http://www.omg.org/spec/DDS-Java/1.0/">http://www.omg.org/spec/DDS-Java/1.0/</a>).
        The API is shipped as dcpssaj5.jar (stand-alone) / dcpscj5.jar (CORBA-cohabitated).</li>
        <li>Google Protocol Buffers integration for Java 5 PSM (BETA) - OpenSplice
        now also supports transparent publishing/subscribing of data types modeled in
        Google Protocol Buffers IDL (Java5 PSM API only for now). For more
        information about Google Protocol Buffers, click
        <a href="https://developers.google.com/protocol-buffers/">here</a>. An
        example on how to use protocol buffers with OpenSplice can be found in
        the examples/protobuf directory within your OpenSplice installation.
        More elaborate documentation as well as support for other DDS
        language bindings will be introduced in future versions of the product.</li>
        <li>Networking bridge service - The new OpenSplice NetworkingBridge is a
        pluggable service that allows bridging of data between OpenSplice
        networking services. When a networking service is selected that best
        suits a specific deployment, sometimes a part of the data needs to be
        obtained from or disclosed to a system that is using a different kind of
        networking service. The NetworkingBridge allows DCPSPublications and
        DCPSSubscriptions to be matched and the related data forwarded between
        a (secure) RTNetworking system and a DDSI2(E) system and vice versa.
        More background information about the service as well as details on
        configuration can be found in the
        <a href="../pdf/OpenSplice_DeploymentGuide.pdf">OpenSplice Deployment Guide</a>.</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.4.2 contains the following new features:</h2>
    <p>
    <ul>
        <li>OpenSplice is now robust against time discontinuities caused by adjusting
            the clock forward/backward or resuming after being hibernated/suspended. Read
            Section "Time Jumps" of the Deployment guide for more detailed information.</li>
        <li>Support for QNX 6.5</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.4.1 contains the following new features:</h2>
    <p>
    <ul>
        <li>The new Launcher tool (accessible from bin/ospllauncher or the
        Windows Start Menu is available to make the evaluation process of
        OSPL easier). Only available on Windows and Linux.</li>
        <li>A new Node Monitor application is available that publishes the
        following system-monitoring data into the OpenSplice backbone:
           <ul>
              <li>CPU information and statistics</li>
              <li>Memory statistics</li>
              <li>Network interface information and statistics</li>
              <li>Operating System information</li>
              <li>Process information and statistics</li>
           </ul> </br>
           For more information, please see the
           <a href="../pdf/OpenSplice_NodeMonitorGuide.pdf"> Node Monitor Guide</a>
        </li>
        <li>DDSI2 secure communication via SSL/TLS is now supported. This uses the
        OpenSSL library which can be enabled in conjunction with TCP support.
        The certificates and keys required for communication should be placed
        in a PEM format file, which is then referenced from the SSL component
        for the DDS configuration file. The pass phrase for this file should also
        be configured. By default clients verify servers by checking their X509
        security certificates, however server verification of clients can also
        be configured. It is also possible via configuration to disable the
        validation of certificates, or enable self signed certificates, which by
        default are rejected. The set of Ciphers that can be used for encryption
        can also be configured if required, as can a file used to contain entropy
        (randomness) for the encryption ciphers. It is recommended that the
        O'Reilly book "Network Security with OpenSSL" is used as a reference to
        understand how SSL is best configured and deployed. See the Deployment
        Guide for further information.</li>
        <li>DDSI can discover entities and can generate builtin topic
        information. This enables non-OSPL Enterprise nodes in the DDSI network
        to become visible. Also, in cases where DDSI generates builtin
        topic information there is no need for durability to align builtin
        topic information, which saves bandwidth.</li>
        <li>RTnetworking now supports Differentiated Services Code Point (DSCP)
         field in IP packets on windows.</li>
        <li>Exception handling
        <p>Up to this release, in a shared memory deployment mode, each process
        that used OpenSplice would attempt to clean up its shared resources in
        case it crashed. If for some reason the clean up failed, some of the shared
        resources could leak or in some edge cases the process could end up in
        a deadlock situation while cleaning up. In the V6.4.1 release, a new
        mechanism has been introduced that allows asynchronous clean-up of
        shared resources by the splice-daemon. The splice-daemon now detects when
        crashed processes leave shared resources behind and will try to cleans up
        if it is certain these resources have been left in a consistent state.
        In case (part of) the resources have not been left in a consistent state
        (this happens when a process crashes while modifying shared resources),
        it will terminate the entire middleware and return an error code.</p>
        <p>Together with the introduction of this garbage collection mechanism a
        new configuration element //OpenSplice/Domain/InProcessExceptionHandling
        is introduced. This element controls whether processes will try to release
        its shared resources upon a crash or not. By default processes will try
        to release resources, this is useful in case of application crashes which
        are unrelated to DDS operations, it will give any ongoing DDS operations
        time to become consistent before leaving the system and avoid any
        unnecessary system shutdown. In case an accurate core file is required
        this configuration element can be set to TRUE, meaning that processes
        will terminate immediately and leave all shared resources to be released
        by the service.</p>
        <p>For now, this feature only works on POSIX-compliant operating systems,
        but in the future this mechanism will be implemented for other operating
        systems as well.</p></li>
    </ul>
    <p>Please note: Due to a Java IDL bug fix (OSPL-4333) introduced at 6.4.0p5,
    any customer using the Java API and upgrading to 6.4.1 from version 6.4.0 to
    6.4.0p4 inclusive will need to recompile their IDL using idlpp.
    Application code does not need to be recompiled.</p>
    </p>
    <h2>OpenSplice DDS V6.4.0p4 contains the following new features:</h2>
    <p>
    <ul>
        <li>The ISO C++ PSM API's has been extended with
        a QoS provider API that allows users to define QoS settings for DCPS
        entities in an XML file as standardized in the DDS For Lightweight CCM
        OMG (DDS4CCM) standard.</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.4.0 contains the following new features:</h2>
    <p>
    <ul>
      <li>The ISO C++ API has been taken out of beta status and is now the
      primary C++ API for OpenSplice DDS. Additionally, the new DDS-PSM-CXX
      specification IDL type mapping is now implemented.
      This allows simplified usage of IDL generated types through full compatibility with the STL.
      The older CORBA compatible types are also supported.
      Please see the
      <a href="../isocpp/html/index.html">ISO C++ Release Notes</a> for further details</li>
      </li>
      <li>A performance boost has been made to OpenSplice. The libraries are now
      consolidated to a single dcps library ddskernel, but the other libraries are still
      available as stubs to avoid user build system changes. Additionally, other
      optimisations have been introduced to increase performance and reduce CPU.
      <br><i>Note: If you are using a static library OpenSplice build and 'ddskernel'
      is before any of the other dds libraries e.g. -lddskernel -lddsrrstorage -lddsos then
      you may get unresolved symbols that were previously satisfied by the 'ddsos'
      at the end of the link line. To resolve this move 'ddskernel' to the end of the link line.
      </i>
      </li>
      <li>TCP support for DDSI has been implemented. Configuration options are available
      to run DDSI over a single connection.
      </li>
      <li>The CORBA cohabitation libraries have been upgraded to use TAO 2.1.
      Where customer demand exists for TAO 1.6 support, these have been left at TAO 1.6.
      </li>
      <li>The DCPS API's for Java, C, C++ and C# have been extended with a
      A QoS provider API that allows users to define QoS settings for DCPS
      entities in an XML file as standardized in the DDS For Lightweight CCM
      OMG (DDS4CCM) standard. Please see the Reference manuals for more information.</li>
      <li>Support for Visual Studio 2012 and 2013 on both 32-bit and 64-bit Windows
      platforms.</li>
      <li>Support for Raspberry Pi.</li>
      <li><a href="../pdf/OpenSplice_EvaluationGuide.pdf"> New evaluation and
      benchmarking guide</a> and examples. Please see the RoundTrip
      and Throughput examples.</li>
    </ul>
    <h2>OpenSplice DDS V6.3.3p2 contains the following new features:</h2>
    <p>
    <ul>
        <li>The DCPS API's for Java, C, C++ and C# have been extended with a
        QoS provider API that allows users to define QoS settings for DCPS
        entities in an XML file as standardized in the DDS For Lightweight CCM
        OMG (DDS4CCM) standard.</li>
        <li>Support for Visual Studio 2013 on both 32-bit and 64-bit Windows
        platforms.</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.3.2 contains the following new features:</h2>
    <p>
    <ul>
      <li>The Durability Service now supports the REPLACE and DELETE merge policies.

      <ul>
         <li>With the REPLACE merge policy it is possible to dispose
              and delete historical data on a node, and replace it with the
              transient and persistent data from another node. Immediately after
              successful completion of the REPLACE merge action the replacement
              data will be available to late joining readers, the data in the
              reader queue of existing readers will be disposed and replaced with
              the replacement data, and the generation count of the replacement
              data is increased.</li>
          <li>With the DELETE merge policy it is possible to dispose and delete
              historical data on a node. Immediately after successful completion
              of the DELETE merge action the historical data in the reader queue
              of existing readers will be disposed and is not available any more
              to late joining readers.</li>
       </ul>
         Please see the deployment guide for more details.
      </li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.3.1 contains the following new features:</h2>
    <p>
    <ul>
      <li>The Durability Service has new pluggable persistent stores. In previous
      versions, XML was allowed on all platforms and Memory Mapped File was also
      available on Linux only. From V6.3.1, Memory Mapped File is now deprecated.
      The new persistence stores are:</li>
      <ul>
         <li><a href="http://www.sqlite.org/">SQLLite</a></li>
         <li><a href="https://code.google.com/p/leveldb/">LevelDB</a></li>
      </ul>
      <li>The ISO C++ API (beta) has had a new set of features added. See the
      <a href="../isocpp/html/index.html">ISO C++ Release Notes</a> for further details</li>
      </li>
      <li>Support for Yocto Linux on Freescale P4080ds.</li>
    </ul>
    </p>

    <h2>OpenSplice DDS V6.3.0 contains the following new features:</h2>
    <p>
    <ul>
      <li>Record and Replay is taken out of beta and now considered a GA service. It
      includes the following new features:</li>
      <ul>
         <li>Compitability with the DDSI2/DDSI2E Services.</li>
         <li>The ability to remove record and/or replay interest without stopping a scenario.</li>
         <li>The ability to clear the contents of a storage during a scenario.</li>
      </ul>
      <li>DDSI2 and DDSI2E are now taken out of beta and considered as GA services.
          In completing the work on
          DDSI2 and DDSI2E, which includes adding support for IPV6, there has been a change
          to the configuration options. In all previous versions DDSI2 and DDSI2E
          used "Unsupported". This has now been changed to "Internal". Old configuration
          files will still be supported on V6 series, but a WARNING message
          will be raised to highlight the deprecated "Unsupported". The Internal
          configuration parameters are marked as such as they are subject to change
          in the future. </li>
      <li>A beta version of the ISO C++ PSM for DDS is now available. Documentation on
      what is supported in this beta version is available <a href="../../docs/isocpp/html/index.html">here.</a>
      <li>Performance optimisation for Java and C++ DataReaders to allow them to
      specify the number of threads to use when demarshalling data. Demarshalling
      data to a language-binding specific format can take considerable processing
      depending on the type of the data. Before this change this process was single-threaded
      and would therefore limit throughput, especially on multi-core platforms.
      See OSPL-1078 in the API changes section.</li>
      <li>The ospl tool is enhanced (see ospl -help for detailed info). It no
      longer returns negative error numbers. Its default is no longer stop, but -help.</li>
      <li>Support for kernel mode is added to the RTP support for VxWorks V6 series.</li>
      <li>Support for SMP (Symmetric MultiProcessing) on VxWorks V6 series.</li>
      <li>DLRL is deprecated. It will be removed from the product at V6.4.</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.2.3 contains the following new features:</h2>
    <p>
    <ul>
      <li>Support for PikeOS 3.1.7 on PowerPC for Windows hosts.</a>.
      <li>Support for VxWorks 6.9 on Pentium for Windows hosts.</li>
      <li>Support for VxWorks 6.9 on Pentium3 for Windows hosts.</li>
      <li>Support for VxWorks 6.9 on PowerPC P1010/P1013 for Windows hosts.</li>
    </ul>
    </p>

    <h2>OpenSplice DDS V6.2.2 contains the following new features:</h2>
    <p>
    <ul>
      <li>Support for PikeOS 3.3. As part of this, the PikeOS port builds against
      the BSD Posix library rather than the NewLib implementation which is removed
      in 3.3.</li>
    </ul>
    </p>
    <p>
    <ul>
      <li>Changes to the way read- and take-operations access data. The read- and
      take-operations now provide data circularly, meaning that these operations
      will &apos;resume&apos; a previous read just as if read_next_instance was
      successively called. This way all instances can be read even when for
      example lower key-values get updated between two read-operations with a
      max_samples limit.</li>
      <li>Support has been added for kernel mode for VxWorks 6.8 PENTIUM4 target.</li>
      <li>Support has been added for SMP kernels for the VxWorks 6.8 PENTIUM4 kernel
      mode build.</li>
    </ul>
    <p>
    <ul>
     <li>Adds configurability for data compression in the networking service.
     This allows tuning of the existing (zlib) method, adds two alternative
     fast compressors (lzf and snappy) and provides a plugin api for using
     other mechanisms.</li>
    </ul>
    </p>
    <p>
    <ul>
      <li>Recording and Replay Service (beta version)<br><br>
      A second beta version of PrismTech's new recording and replay service. This version
      supports:
      <ul>
         <li>Replay the recorded data with different speeds</li>
         <li>The ability to add time conditions to a command that can delay its processing</li>
         <li>Statistics on the record and replay of data</li>
      </ul>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.2.0 contains the following new features:</h2>
    <h4>New product add-ons:</h4>
    <p>
    <ul>
      <li>DDSI-Extended (ddsi2e):
      DDSI2E is a new beta version of the extended version of PrismTech's DDSI2
      networking service, giving extra features for:
      <ul>
      <li>Network partitions : Network partitions provide the ability to use
      alternative multicast addresses for combinations of DCPS topics and partitions
      to separate out traffic flows, for example for routing or load reduction.</li>
      <li>Security : Encryption can be configured per network partition. This allows
      configuring encrypted transmission for subsets of the data.</li>
      <li>Bandwidth limiting and traffic scheduling : Any number of "network channels"
      can be defined, each with an associated transport priority. Application data
      is routed via the network channel with the best matching priority. For each
      network channel, outgoing bandwidth limits can be set and the IP "differentiated services"
      options can be controlled.</li>
      </ul>
      </li>
      <li>OpenSplice Streams :
      OpenSplice Streams API supports a common data-distribution pattern where
      continuous flows or streams of data have to be transported with minimal overhead
      and therefore maximal achievable throughput. For more information see the
      <a href="../pdf/OpenSplice_refman_Streams.pdf">
      OpenSplice Streams Reference Manual</a>.</li><br>
      <li>Recording and Replay Service (beta version)<br><br>
      A first version of PrismTech's new recording and replay service. This version
      supports:
      <ul>
         <li>Record topics to an xml storage</li>
         <li>Manipulate the recorded data</li>
         <li>Ability to control the recording and replay service remotely</li>
         <li>A new tool (separate installer) to drive the recording and replay service and view its output.</li>
      </ul>
      </li><br>
    </ul>
    </p>
    <h4>Other new features:</h4>
    <p>
    <ul>
      <li>Extensible and Dynamic Topic Types annotations now handled in IDL :
      As a first step to the implementation of the OMG standard for
      Extensible and Dynamic Topic Types for DDS, a developer can now annotate their
      IDL as per the specification and when the implementation of the specification
      becomes available (OpenSplice DDS V7) then no further IDL changes should be
      required.
      </li><br>
      <li>OpenSplice RMI extensions:
      <ul>
         <li>Instances Management: Requests/Replies topic instances are per-client basis.
         The topic key is {client_name, client_instance}. To support multithreaded clients
         a configuration option named '--RMIClientThreadingModel' has been added to take two
         possible values 'ST' (Single Thread) or 'MT' (Multi Thread). The MT option will set
         the history qos to KEEP_ALL and allow then the client to be multithreaded.</li>
         <li>Service activation: The server activation has been changed to decouple the service
         activation from the service registration. This activation supports two modes: a blocking mode
         where the run operation blocks the calling thread, and a non blocking mode where
         the calling thread can perform other tasks after calling "run".</li>
         <li>Error handling and exceptions: Error reporting has been enhanced with the definition of two new exceptions :
         INTERNAL_ERROR to report any internal operation failure and SERVICE_NOT_FOUND
         to report the failure to find a requested service.</li>
         <li>The default Replies waiting timeout has beed increased to 10min (it was set to 1millisec).</li>
         <li>Asynchronous invocation mode has been added, which is similar to the CORBA
         Asynchronous Messaging Interface (AMI) call-back model, in addition to
         the conventional synchronous mode. The RMI documentation has been
         updated and new examples (Printer and HelloWorld) have been added to
         illustrate the usage of this invocation mode.</li>
      </ul><br>
      For more information see the <a href="../pdf/OpenSplice_RMIUserGuide.pdf">
      OpenSplice RMI GettingStarted Guide</a>.</li><br>
      </li><br>
      <li>Windows Services improvements and it is now able to write to the System Event Log, for more details see OSPL-593 below.
      </li><br>
      <br>
    </ul>
    </p>
    <h4>Potential upgrade change:</h4>
    <p>
    <ul><li>In previous releases only a subset of the OpenSplice CORBA co-habitation DCPS Java API (dcpscj.jar) was
    generated using a CORBA ORB. Starting from this release (issue ospl-347), the entire CORBA-Java API is generated
    by the ORB IDL compiler. This enables customers to use internal types like ReturnCode_t and InstanceHandle_t in
    a CORBA environment, i.e. Helper and Holder classes are now available for those types as well. This changes the
    steps required to process your application Topic definition IDL files, to compile the generated and application
    source code, to run applications, and how Listener classes must be written. Additionally extra care must be taken
    not to mix the CORBA and Standalone DCPS APIs on the same compilation or VM classpath.
    <ul><li>IDL to Java code generation changes
    <p>Given the below IDL definition:<br><br>
    /* @file: Foo.idl */<br>
    struct Bar {/<br>
        long key;/<br>
    };/<br>
    #pragma keylist Bar key/<br>
    </p>
    <p>At OSPL 6.1 the required CORBA-Java DCPS code-generation steps were:
    <br><br>
    1 / $JACORB_HOME/bin/idl Foo.idl/<br><br>
    This produced files:/<br>
    BarHelper.java/<br>
    BarHolder.java/<br>
    Bar.java/<br><br>
    2/ $OSPL_HOME/bin/idlpp -C -l java Foo.idl/<br><br>
    This produced files:/<br>
    BarDataReaderHelper.java/<br>
    BarDataReaderHolder.java/<br>
    BarDataReaderImpl.java/<br>
    BarDataReader.java/<br>
    BarDataReaderOperations.java/<br>
    BarDataReaderViewHelper.java/<br>
    BarDataReaderViewHolder.java/<br>
    BarDataReaderViewImpl.java/<br>
    BarDataReaderView.java/<br>
    BarDataReaderViewOperations.java/<br>
    BarDataWriterHelper.java/<br>
    BarDataWriterHolder.java/<br>
    BarDataWriterImpl.java/<br>
    BarDataWriter.java/<br>
    BarDataWriterOperations.java/<br>
    BarMetaHolder.java/<br>
    BarSeqHolder.java/<br>
    BarTypeSupportHelper.java/<br>
    BarTypeSupportHolder.java/<br>
    BarTypeSupport.java/<br>
    BarTypeSupportOperations.java/<br><br>

    Now at OSPL 6.2 the required steps are:<br><br>
    1/ $JACORB_HOME/bin/idl Foo.idl<br><br>

    This step is the same as 6.1 and produces the same files.<br><br>
    2/ $OSPL_HOME/bin/idlpp -C -l java Foo.idl<br><br>
    This step is the same command as 6.1 however the files produced are now:
    BarDataReaderImpl.java<br>
    BarDataReaderViewImpl.java<br>
    BarDataWriterImpl.java<br>
    BarMetaHolder.java<br>
    BarTypeSupport.java<br>
    FooDcps.idl<br><br>
    3/ $JACORB_HOME/bin/idl -I$OSPL_HOME/etc/idl FooDcps.idl<br><br>
    This step is new at 6.2 and processes the *Dcps.idl file now produced by step 2 with the ORB IDL compiler. This produces the below files, per the IDL to Java mapping:
    BarDataReaderHelper.java *<br>
    BarDataReaderHolder.java *<br>
    BarDataReader.java *<br>
    _BarDataReaderLocalBase.java<br>
    BarDataReaderLocalTie.java<br>
    BarDataReaderOperations.java *<br>
    BarDataReaderViewHelper.java *<br>
    BarDataReaderViewHolder.java *<br>
    BarDataReaderView.java *<br>
    _BarDataReaderViewLocalBase.java<br>
    BarDataReaderViewLocalTie.java<br>
    BarDataReaderViewOperations.java *<br>
    BarDataWriterHelper.java *<br>
    BarDataWriterHolder.java *<br>
    BarDataWriter.java *<br>
    _BarDataWriterLocalBase.java<br>
    BarDataWriterLocalTie.java<br>
    BarDataWriterOperations.java *<br>
    BarSeqHelper.java<br>
    BarSeqHolder.java *<br>
    BarTypeSupportInterfaceHelper.java<br>
    BarTypeSupportInterfaceHolder.java<br>
    BarTypeSupportInterface.java<br>
    _BarTypeSupportInterfaceLocalBase.java<br>
    BarTypeSupportInterfaceLocalTie.java<br>
    BarTypeSupportInterfaceOperations.java<br><br>

    Files marked with an * were previously produced by step 2 (i.e. idlpp); note the file BarTypeSupportOperations.java is no longer produced by any step.
    </p>
    </li>
    <li>CORBA co-habitation Java DCPS compilation<br><br>
    At 6.1 it was sometimes possible to compile code generated as above and / or applications like:<br><br>
    javac -classpath javac -cp .:$OSPL_HOME/jar/dcpscj.jar BarDataReader.java<br><br>
    For compilation errors to be avoided this must now always be:<br><br>
    javac -endorseddirs $JACORB_HOME/lib/endorsed -cp .:$OSPL_HOME/jar/dcpscj.jar BarDataReader.java<br>
    ...or...<br>
    javac -Djava.endorsed.dirs=$JACORB_HOME/lib/endorsed -cp .:$OSPL_HOME/jar/dcpscj.jar BarDataReader.java<br><br>
    See <a href="http://docs.oracle.com/javase/7/docs/technotes/guides/standards/">http://docs.oracle.com/javase/7/docs/technotes/guides/standards/</a>
    </li>
    <li>Running CORBA co-habitation Java DCPS applications<br><br>
    At 6.1 it was possible to sometimes run applications like:<br><br>
    java -classpath .:$OSPL_HOME/jar/dcpscj.jar FooApp<br><br>
    At 6.2 this form must be used instead:<br><br>
    java -classpath .:$OSPL_HOME/jar/dcpscj.jar -Djava.endorsed.dirs=$JACORB_HOME/lib/endorsed FooApp<br><br>
    See <a href="http://docs.oracle.com/javase/7/docs/technotes/guides/standards/">http://docs.oracle.com/javase/7/docs/technotes/guides/standards/</a>
    </li>
    <liListener classes</li>
    Because all classes are now properly implementing CORBA interfaces and inheriting from CORBA base classes,
    derived classes will be required to provide an implementation for all abstract operations mentioned
    in the generated interface from the LocalObject base.<br><br>
    One option is to add no-op implementations of the abstract Object base methods e.g.:<br><br>
    public org.omg.CORBA.Object _set_policy_override(org.omg.CORBA.Policy[] policies, org.omg.CORBA.SetOverrideType set_add) { return null; }<br>
    public org.omg.CORBA.DomainManager[] _get_domain_managers() { return null; }<br>
    public org.omg.CORBA.Policy _get_policy(int policy_type) { return null; }<br>
    public org.omg.CORBA.Request _create_request(org.omg.CORBA.Context ctx, java.lang.String operation, org.omg.CORBA.NVList arg_list, org.omg.CORBA.NamedValue result, org.omg.CORBA.ExceptionList exclist, org.omg.CORBA.ContextList ctxlist) { return null; <br>
    public org.omg.CORBA.Request _create_request(org.omg.CORBA.Context ctx, java.lang.String operation, org.omg.CORBA.NVList arg_list, org.omg.CORBA.NamedValue result) { return null; }<br>
    public org.omg.CORBA.Request _request(java.lang.String operation) { return null; }<br>
    public org.omg.CORBA.Object _get_interface_def() { return null; }<br>
    public void _release() {}<br>
    public org.omg.CORBA.Object _duplicate() { return null; }<br>
    public int _hash(int maximum) { return 0; }<br>
    public boolean _non_existent() { return false; }<br>
    public boolean _is_equivalent(org.omg.CORBA.Object other) { return false; }<br>
    public boolean _is_a(java.lang.String repositoryIdentifier) { return false; }<br>
    public org.omg.CORBA.Object _get_component() { return null; }<br>
    public org.omg.CORBA.InterfaceDef _get_interface ( ) { return null;<br><br>

    Alternately, for the case of Listeners, OpenSplice supplies the ListenerBase class as a convenience.
    When Listeners extend from this base class they are no longer required to implement these operations themselves.
    So a Listener defined like:<br><br>
    public class MyReaderListener implements DataReaderListener {<br>
    ... <br><br>
    could instead be defined:<br><br>
    public class MyReaderListener extends org.opensplice.dds.dcps.ListenerBase implements DataReaderListener {<br>
    ... <br><br>
    </li>
    <li>CORBA & Standalone Java DCPS API jar incompatibility<br><br>
    Previously the dcpscj.jar & dcpssaj.jar were largely identical and could often reside safely on the same CLASSPATH.
    In fact, the release.com/.bat script provided with OpenSplice placed both jars on the exported/set CLASSPATH environment
    variable. This is no longer the case - users must take care to specify the correct jar on their CLASSPATH
    when building and running applications as they are now incompatible. For this reason the release.com/.bat
    scripts no longer exports/sets any CLASSPATH value.
    </li>
    </ul>
    </li>
    </ul>
    </p>

    <h2>OpenSplice DDS V6.1.1p7 contains the following new features:</h2>
    <p>
    <ul>
      <li>Support for Client Side Durability with Vortex Lite. Vortex Lite
      can now act provide a persistence capability using a running instance of
      the OpenSplice Durability Service.</li>
    </ul>
    <h2>OpenSplice DDS V6.1.1p5 contains the following new features:</h2>
    <p>
    <ul>
      <li>Support for Secure Networking on LynxOS 5.0 added</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.1.1p3 contains the following new features:</h2>
    <p>
    <ul>
      <li>Support for Linux host to WindRiver Linux PowerPC and x86 targets.</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.1.1p2 contains the following new features:</h2>
    <p>
    <ul>
      <li>Support for Windows host to VxWorks 6.9 PENTIUM 4 target.</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.1.1p1 contains the following new features:</h2>
    <p>
    <ul>
      <li>Official support for ElinOS 5.1 with gcc 4.4 for x86</li>
    </ul>
    </p>
    <h2>OpenSplice DDS V6.1.1 contains the following new features:</h2>
    <p>
    <ul>
      <li>Official support for RHEL 6.0 platform with gcc 4.4</li>
    </ul>
    <h2>OpenSplice DDS V6.1.0 contains the following new features:</h2>
    <p>
    <ul>
      <li>A new OpenSplice DDS deployment architecture which offers daemon-less deployment
      to go alongside the existing daemon (shared memory) deployment is now available.
      The new daemon-less library mode is what we call a <b>Single Process</b> deployment option
      where an application will dynamically link at runtime all OpenSplice libraries and
      services into a single operating-system process.
      For more information see Chapter 1 of the Deployment Guide. The choice of
      daemon or daemon-less architecture can be made by simply changing the ospl configuration
      file, and therefore does not require any application code changes nor re-generation of the application.
      Users on windows will find this particularly useful to avoid the need to find a free
      location in shared memory for the nodal administration database.</li>
      <li>OpenSplice RMI, a new set of APIs available with OpenSplice DDS that complement the
      pub/sub integration pattern of DDS with a new/client server pattern, but over 1
      middleware.
      For more information on this exciting new addition to the OpenSplice DDS product line,
      please see the OpenSplice RMI Getting Started Guide.</li>
      <li>OpenSplice Tester, a tool aimed at automated regression testing and debugging of OpenSplice DDS
      based systems is now available. For more information on this exciting new addition to the
      OpenSplice DDS product line, please see the OpenSplice Tester Guide and/or
      use the builtin help of the Tester itself.</li>
      <li>Support for data reader TIME_BASED_FILTER Qos Policy</li>
      <li>The OpenSplice configurator is context sensitive to the enabled features and will
      only allow access to attributes of relevance (for either the community or commercial edition used).
      </li>
      <li>DDSI2 now allows setting the multicast time-to-live for outgoing packets using the
      General/MulticastTimeToLive element. Setting the time-to-live differently for
      each multicast address is not yet possible.</li>
      <li>Code for a plugin for <A HREF=http://www.wireshark.org>Wireshark</A>, which
      will allow decoding of Real Time networking packets is provided. For more
      information see the README in tools/wireshark-plugins/ospl.</li>
      <li>New platform support</li>
      <ul>
          <li>Support for Windows x86 64 bit.</li>
          <li>Support for Solaris SPARC 32 bit with gcc compliler.</li>
          <li>Support for RTEMS 4.10.0 on leon3 SPARC architecture.</li>
          <li>Support for LynxOS 5 on PPC architecture.</li>
          <li>Support for PikeOS 3.2 on x86.</li>
      </ul>
      <li>Changes to source code available</li>
      <ul>
         <li>DLRL is now open source.</li>
         <li>SOAP Service is now open source.</li>
         <li>As stated in later 5.x series releases, the DDSIv1 code has been removed. It was replaced
         by DDSiv2.</li>
         <li>As DDSIv2 is the interoperable standard and open source, Real Time networking is closed source.</li>
         <li>As daemon-less architecture is now commercial quality and open source,
         shared memory is now closed source.</li>
      </ul>
    </ul>
    </p>
    <h2><a name="issues_not_api">Fixed Bugs and Changes not affecting API</a></h2>
     <h2>6.7.2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-9085 / 16827
          </td>
          <td>
            <b>Race condition between multiple simultaneous started OpenSplice daemons with the same domain configuration.</b><br/>
            <i>
             Starting multiple OpenSplice daemons in shared memory mode for the same domain configuration at the same time could lead to a crash.</i>
             <br/>
             <b>Solution: The race condition between the multiple OpenSplice daemons is fixed and
             a consecutive started daemon will be detected correctly and exit properly.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9672
          </td>
          <td>
            <b>Java5 Helloworld example creates topic with wrong QoS.</b><br/>
            <i>
             The Java5 Helloworld examples creates a topic with the default QoS. This differs from all other language binding HelloWorld examples. 
             As a result of this an error message will be reported when this example wants to communicate with other language binding HellowWorld examples.</i>
             <br/>
             <b>Solution: The defect is fixed and the Java5 example now creates a topic with the correct QoS so it can also communicate with 
                the HellowWorld examples from other languages.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9684
          </td>
          <td>
            <b>A failing networking service does not always restart with FailureAction configuration restart.</b><br/>
            <i>
             When a networking service detects a failure, it'll terminate as gracefully as possible. The splice daemon was not able to detect 
             whether the networking service terminated due to a valid stop or due to a detected failure. This means that the splice daemon did 
             not restart the networking service when it failed gracefully.</i>
             <br/>
             <b>Solution: Set the networking service state to died when it failed gracefully.
             </b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-9738
          </td>
          <td>
            <b>When the durability service terminates a mutex is not cleaned up properly and potentially causes a memory leak.</b><br/>
            <i>
             The durability services uses various mutexes to protect access to shared resources by different threads. 
             One such mutex is used to protect updates to a sequence number. This mutex is not cleaned up properly.</i>
             <br/>
             <b>Solution: The mutex is now cleaned up properly.
             </b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-9797 / 17608
          </td>
          <td>
            <b>DataWriter deletion is not always reflected in the instance_states of the DataReader.</b><br/>
            <i>
            When two nodes have unaligned clocks and are using BY_RECEPTION_TIMESTAMP destination ordering, 
            and the clock of the sending node runs ahead of the clock on the receiving node, then the deletion 
            of the DataWriter on the sending node may not always correctly be reflected in the instance_states 
            of the DataReader. These instance_states should go to either NOT_ALIVE_DISPOSED or NOT_ALIVE_NO_WRITERS, 
            depending on the auto_dispose_unregistered_instances setting of the DataWriter. However, when the clock 
            skew is bigger than the lifespan of an instance, its instance_state might remain ALIVE after deletion of the DataWriter.</i>
             <br/>
             <b>Solution: The algorithm now always applies the correct instance_state in the above mentioned case.
             </b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.7.1p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-9430 / 17158
          </td>
          <td>
            <b>The isocpp2 API does not report an error when the generated
            discriminant setter related to a union is used incorrectly.</b><br/>
            <i>
             For a union type the idl preprocessor generates an setter for the
             discriminant of the union. This is the _d(val) function. However,
             this setter may only be used to set the discriminant to a label
             that corresponds to the current state of the union. A union case
             may have more than one label associated with an certain case. With
             the use of the _d(val) function it is only allowed to set the
             discriminant to one of the alternative labels associated with the
             current selected case. However when this setter is used incorrectly
             the isocpp2 API does not raise an exception and it may cause a
             crash of the application.
             <br/>
             <b>Solution: The generated discriminant setter function _d(val)
             checks if the specified value corresponds with the current state of
             the union and raise an exception otherwise.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9444
          </td>
          <td>
            <b>Move iShapes example from isocpp to isocpp2.</b><br/>
            <i>
              The iShapes example should work on top of the isocpp2 i.s.o. the
              deprecated isocpp API.
             <br/>
             <b>Solution: The example has been ported to the isocpp2 API
             </b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.7.1p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8405 / 16519
          </td>
          <td>
            <b>The discovery channel of the networking service may miss
            disconnects when it is configured for a very small detection period.</b><br/>
            <i>
             When the discovery channel of the networking service is configured
             to detect the death (disconnect) of an node within a few 100 msec,
             then it may occur that the disconnect is not detected. In that case
             the discovery channel may not detect the disconnect because of an
             incorrect scheduling of the time the evaluation of the discovery
             heartbeats occurs. This may cause that a reliable channel becomes
             stalled when a reconnect of a node occurs which is not detected.
             <br/>
             <b>Solution: The scheduling of the heartbeat evaluation is moved to
             the receive thread of the discovery channel to make it independent
             of the scheduling of the main networking thread. Furthermore,
             the heartbeat evaluation time is made more strictly related to the
             maximum interval in which a heartbeat should have been received
             from a node.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9027
          </td>
          <td>
            <b>idlpp is not robust to paths with whitespace.</b><br/>
            <i>
             The idlpp tool is not able to handle paths that contain whitespaces
             when compiling for cpp and thus using cppgen.
             <br/>
             <b>Solution: Add quotes to the arguments for cppgen, which are
             generated by idlpp.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9030
          </td>
          <td>
            <b>OSPL_URI not set correctly in console run.</b><br/>
            <i>
             The OSPL_URI can be set using the Launcher Settings dialog. The
             user can select a file using the browse dialog by clicking the
             "..." button. The file path must be prepended with "file://". When
             using the browse dialog, the "file://" was not being included.
             <br/>
             <b>Solution: When using the OSPL_URI browse dialog, the "file://"
             is now prepended to selected file path.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9363 / 17024<br/>
            OSPL-9075 / 16821
          </td>
          <td>
            <b>Possible spliced deadlock when other services crash.</b><br/>
            <i>
             When another service or application crashes and leaves the kernel
             in an undefined state, it can happen that the splice daemon will
             deadlock and will never shutdown. 
             <br/>
             <b>Solution: Added a thread watchdog to spliced and abort when the
             shutdown-thread deadlocks.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9389
          </td>
          <td>
            <b>Potential crash when removing certain entities concurrently in
            classic java PSMs</b><br/>
            <i>
             When a datawriter or datareader is removed by one thread, while a
             different thread is removing the corresponding subscriber or
             publisher, in Classic Java PSMs (SAJ and CJ), the application can
             crash. 
             <br/>
             <b>Solution: The issue was resolved by changing the locking
             strategy so that a publisher/subscriber cannot be removed when one
             of its datawriters or datareaders is in use by a different thread.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9491
          </td>
          <td>
            <b>Terminating Streams example causes invalid memory free</b><br/>
            <i>
             When the Streams subscriber or publisher example is terminated
             before it completes, i.e. by pressing Ctrl-C, it can trigger an
             invalid free of the partition name. 
             <br/>
             <b>Solution: The issue is resolved by using a copy of the partition
             string that can be freed under all circumstances.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9506
          </td>
          <td>
            <b>RnR service crash when replaying DCPSTopic</b><br/>
            <i>
             The RnR service can crash when replaying DCPSTopics. This is due to
             an improper free.
             <br/>
             <b>Solution: The improper free in the RnR service is fixed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9540 / 17190
          </td>
          <td>
            <b>The networking service exits with a fatal error when a
            best-effort channel runs out of buffer space.</b><br/>
            <i>
             When the situation occurs that all the defragmentation buffers for
             a best-effort channel are in use and a new buffer is needed then
             the networking service will first reclaim fragment buffers from
             messages which are not yet complete (fragments missing) or from the
             list of buffers waiting to be defragmented. When it fails to free a
             buffer then the networking service will terminate with a fatal
             error indicating that it has run out of defragmentation buffers.
             <br/>
             <b>Solution: When the situation occurs that a best-effort channel
             runs out of defragmentation buffers and is not able to reclaim a
             buffer then it will stop reading from the socket until buffers
             become available again. Note that this may cause that fragments
             will be lost when the maximum receive buffer size of the socket is
             exceeded. For a best-effort channel this is allowed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9551
          </td>
          <td>
            <b>Some buttons on Tuner's Writer Pane are connected to wrong Writer
            functions</b><br/>
            <i>
             Some Buttons on the Tuner's Writer Pane are connected to the wrong
             Writer functions:
             <ul>
             <li>The Register button is connected to the Dispose function
             (Should be register).</li>
             <li>The Unregister button is connected to the Dispose function.
             (Should be unregister)</li>
             </ul>
             <br/>
             <b>Solution: The handlers for the various buttons have been
             corrected.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9552 / 17191
          </td>
          <td>
            <b>The networking service should log the selected network
            interface.</b><br/>
            <i>
             The networking service does not report the network interface that
             is has selected. To support the analyses of problems on hosts which
             have more than one network interface configured it the networking
             service should report the selected network interface.
             <br/>
             <b>Solution: On startup of the networking service the selected
             network interface is reported in the info log. The name of the
             interface is also included in the report which indicates that
             networking has detected a state change of the network interface
             (down/up).
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9608
          </td>
          <td>
            <b>Samples can be purged prematurely</b><br/>
            <i>
             Purging samples depends (among other things) on the
             service_cleanup_delay. If opensplice is started when the up time of
             the node is smaller than the service_cleanup_delay, then disposed
             samples will be purged permaturely.
             <br/>
             <b>Solution: Improve check between node up time and the
             service_cleanup_delay.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-437 / 17181
          </td>
          <td>
            <b>OpenSplice Tester crash</b><br/>
            <i>
             When running tester scripts in headless mode,
             ArrayIndexOutOfBoundsExceptions are sometimes thrown and logged to
             the console. The exceptions were the result of a race condition on
             startup.
             <br/>
             <b>Solution:  A fix was made in the MainWindow to prevent the race
             condition when populating filtered topics.
             </b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.7.1p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-9047 / 16793
          </td>
          <td>
            <b>Deadline-missed events are not necessarily triggered per instance</b><br/>
            <i>
             When multiple deadlines are missed around the same time and a
             listener is used to monitor these events, only a single listener
             callback may be performed.
             <br/>
             <b>Solution: A separate listener callback is performed for each
             missed deadline.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            RNR-704 / 17155
          </td>
          <td>
            <b>RnR CDR recording cannot be imported</b><br/>
            <i>
             RnR is not properly processing the situation where there is no
             active union case.
             <br/>
             <b>Solution: RnR now accepts the option of not having an active
             union case.
             </b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.7.1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-7942<br/>
            OSPL-9207 / 16854
          </td>
          <td>
            <b>Thread specific memory leaking away after thread end</b><br/>
            <i>
             Threads that are created from a user application don't use the
             ospl internal thread wrapper. Thread specific memory allocated by
             the ospl software stack isn't freed when these threads exit.
             <br/>
             <b>Solution: Use OS supplied callback functionality to call a
             destructor function when a user created thread exits to free
             allocated memory.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8425 / 16551
          </td>
          <td>
            <b>When a lot of fragmented, best effort data is received, the
            receiver will run out of buffer space</b><br/>
            <i>
             Defragmentation buffers is shared with the receive buffer. In case
             a lot of data is received, the networkstack isn't able to free
             claimed buffer space as it doesn't get to time to defragment data
             in the buffers. As a result, incoming data can not be stored and
             networking can't recover from this situation as these buffers
             remain locked.
             <br/>
             <b>Solution:  In case no buffers can be freed, drop data in buffers
             to create free space to be able to receive new data and continue to
             function. Data in the dropped buffers is lost but as this is
             best-effort data, this is allowed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8813
          </td>
          <td>
            <b>Calling RMI C++ CRuntime::stop() can cause deadlocks</b><br/>
            <i>
             When RMI C++ CRuntime::stop() is called, it can deadlock due to
             timing. Two threads will wait until the other is stopped.
             <br/>
             <b>Solution: By caching certain information, it's not necessary for
             one thread to wait for the other while stopping.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8958
          </td>
          <td>
            <b>DDSI can regurgitate old T-L samples for instances that have
            already been unregistered</b><br/>
            <i>
            DDSI maintains a writer history cache for providing historical data
            for transient-local writers and for providing reliability. An
            instance is removed from this cache when it is unregistered by the
            writer, but its samples are retained until they have been
            acknowledged by all (reliable) readers. Already acknolwedged samples
            that were retained because they were historical data could survive
            even when the instance was removed. When this happened, a
            late-joining reader would see some old samples reappear.
            <br/>
             <b>Solution: deleting an instance now also removes the already
             acknowledged samples from the history.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9058 / 16796<br/>
            OSPL-9206 / 16852
          </td>
          <td>
            <b>Incompatibility with versions before V6.5.0p5</b><br/>
            <i>
             An internal change to builtin heartbeat topic caused an
             incompatibility with older versions. When adding a node running a
             recent version of OpenSplice to a domain with nodes running a
             version before V6.5.0p5, the existing nodes would incorrectly
             dispose participants (and corresponding entities) belonging to the
             new nodes after a single heartbeat period, normally done only when
             a heartbeat expires.
             <br/>
             <b>Solution: To resolve this, the change to the heartbeat topic
             was reverted.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9059 / 16803
          </td>
          <td>
            <b>Custom Lib missing for ISOCPP2 on DDSCE-P708-V6x-MV-A</b><br/>
            <i>
             Custom LIb missing for ISOCPP2 on DDSCE-P708-V6x-MV-A target
             platform.
             <br/>
             <b>Solution: Custom lib got added for this platform too.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9113
          </td>
          <td>
            <b>When the persistent store contains an unfinished transaction a
            non-coherent reader may not receive the corresponding historical
            data.</b><br/>
            <i>
             When the durability service injects persistent data and the
             persistent data set contains unfinished transactions and there is
             a non-coherent reader present then this reader will not receive the
             data of these unfinished transactions. For persistent data the
             durability service unregisters each instance after injecting.
             However, the injection the historical samples of a transaction
             expect that the instance is still registered.
             <br/>
             <b>Solution: When retrieving historical data and the historical
             data contains unfinished transaction then the corresponding samples
             are injected into the non-coherent reader independent from the
             existence of a registration for the corresponding instance.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9208</td>
          <td>
            <b>DDSI not sending an SPDP ping at least every SPDPInterval</b><br/>
            <i>DDSI has a Discovery/SPDPInterval setting that is meant to set an
            upper bound to the SPDP ping interval, that is otherwise derived
            from the lease duration set in the
            //OpenSplice/Domain/Lease/ExpiryTime setting. The limiting only
            occurred when the lease duration is &gt; 10s.
              <br/>
              <b>Solution: The limiting has been changed to ensure the interval
              never becomes larger than what is configured.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9216
          </td>
          <td>
            <b> Calling RMI C++ CRuntime::run() can cause deadlocks</b><br/>
            <i>
             When RMI C++ CRuntime::run() is called when the runtime is already
             running, then the running state is detected and that call will
             leave the runtime immediately. However, it does so without
             unlocking a mutex. Further interaction with that runtime is likely
             to hit that locked mutex and thus deadlock.
             <br/>
             <b>Solution:  Unlock the runtime when a problem is detected during
             the run() call.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9240 / 16923
          </td>
          <td>
            <b>Memory leak in entities with listener</b><br/>
            <i>
             The listener administration shared by all language bindings, leaks
             a small amount of heap memory each time an entity is removed that
             has a listener attached to it.
             <br/>
             <b>Solution: The issue was resolved by properly freeing admin data
             when a listener is detached from an entity manually or when the
             entity is removed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9248
          </td>
          <td>
            <b>Error while building FACE C++ example on windows 64 bit</b><br/>
            <i>
              When building the FACE C++ example on windows 64 bit an error
              LNK2001: unresolved external symbol coming from the DataState
              class could occur.
             <br/>
             <b>Solution: The defect in the DataState class has been fixed and
             the error will not occur anymore.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9250 / 16928
          </td>
          <td>
            <b>RMI Java runtime stop can cause NullPointerException.</b><br/>
            <i>
              By external interference of the RMI internals or specific timing,
              it is possible that RMI Java runtime stop() can throw a
              NullPointerException.
             <br/>
             <b>Solution: Added various null pointer checks.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9291
          </td>
          <td>
            <b>Possibly wrong durability master confirmation due to not waiting
            heartbeat expiry period.</b><br/>
            <i>
              When a reconnect occurred it was possible that durability services
              confirmed a federation as master while the one they confirmed
              themselves confirmed a different federation as master. This would
              lead to durable data not being aligned until a new durability
              conflict is triggered. The cause of this problem was that the
              durability service should wait the heartbeat expiry period before
              confirming a master, however the wait did not occur.
             <br/>
             <b>Solution: During master selection the wait heartbeat expiry
             period now works again.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9361 / 17014
          </td>
          <td>
            <b>Incorrect IDL code generation for IDL that contains an array of a
            typedef which refers to a string type.</b><br/>
            <i>
             For an IDL definition that contains an array of a typedef and the
             typedef refers to a string then the copy-in routines generated by
             the IDL preprocessor (idlpp) is incorrect. The string members are
             copied to a wrong memory location.
             <br/>
             <b>Solution: The code that is generated by the IDL preprocessor for
             this array type is corrected.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9388 / 17030
          </td>
          <td>
            <b>Durability service might deadlock when the networking queue is flooded.</b><br/>
            <i>
             When the network queue is overrun by the durability service, the
             normal mode of operation is to sleep a bit and retry again later.
             However, there is a slight chance that the sending thread of the
             network service that needs to make room again by consuming elements
             in the queue will indirectly block on the sleeping thread in the
             durability service itself.
             <br/>
             <b>Solution: The network service can no longer indirectly run into
             a lock that is held by the durability service while the network
             queue is flooded.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9413
          </td>
          <td>
            <b>Memory leak in RnR service when using XML storage.</b><br/>
            <i>
             The RnR service leaks memory for every data update when using XML
             storage.
             <br/>
             <b>Solution: Free internal RnR service variable.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9441
          </td>
          <td>
            <b>IsoCPP dds::sub::CoherentAccess destruction will throw an exception when end() was called on that object.</b><br/>
            <i>
             The dds::sub::CoherentAccess destructor will end a coherent access.
             When the coherent access was already ended by a call to
             dds::sub::CoherentAccess::end() prior to the destruction of that
             object, the destructor will throw an precondition_no_met exception
             when it tries to end the already ended coherent access. An
             exception in a destructor can cause undefined behaviour (like a
             crash on VS14).
             <br/>
             <b>Solution: Check if the coherent access is already ended before
             ending it again in the destructor.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-434
          </td>
          <td>
            <b>Participant reader and writer tables - QoS column display and tooltip incorrect</b><br/>
            <i>
            In the Browser tab, if the user navigated to a participant, the
            corresponding reader and writer tables did not display the qos
            values correctly.
             <br/>
             <b>Solution: This regression was introduced in 6.7.0 and has been
             fixed in 6.7.1.
             </b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.7.0</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8709 / 16660
          </td>
          <td>
            <b>Problem with triggered read condition when order access is used</b><br/>
            <i>
            When using order access there is a possibility that a reader continues to receive data available events, but reading or taking results in no data available.
            This is caused by a disposed not being purged, causing invalid samples to retain in the data reader.
             <br/>
             <b>Solution: When taking data, purge invalid samples in stead of retaining them in the reader.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8950 / 16756
          </td>
          <td>
            <b>Memory increase of in-active RMI proxies</b><br/>
            <i>
            An RMI proxy will receive replies (of the related service) meant for other proxies on other nodes. The middleware takes care
            of filtering the proper replies for the proxies on the local node. However, when the RMI proxy is in-active
            (it is created but not used to send requests), the replies are not cleared from the proxy reply reader memory.
            When the RMI proxy becomes active (sending a new request) all replies are taken from the reader and the memory is
            back to normal. However, the memory usage can increase on a node when the proxy is in-active.
             <br/>
             <b>Solution: The proxy reply reader is always monitored and unrelated replies are removed from its memory.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8438 / 16569
          </td>
          <td>
            <b>Builtin types not properly supported in ISOCPP2</b><br/>
            <i>
           When using any of the builtin types (types defined in dds_dcps.idl or defined in its included files) in your
           own IDL file (properly including the relevant IDL file in which it is defined), then the copy functions
           generated by idlpp would have compilation issues where referenced functions could not be found.
             <br/>
             <b>Solution: The previously missing referenced functions have now been added to the ISOCPP2 library.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9122
          </td>
          <td>
            <b>Error report during termination when LivelinessLost listener callback was set</b><br/>
            <i>
           When using a LivelinessLost listener callback in your application it could happen that during termination of the application an error report
           "Internal error DataWriter has no Publisher reference" occurs in the ospl-error.log file.
             <br/>
             <b>Solution: The defect is fixed and the error will not occur anymore.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9015
          </td>
          <td>
            <b>Deletion of a reader after all group coherent transactions were received and read could lead to new transactions never becoming complete.</b><br/>
            <i>
            When deleting a group coherent reader, which was part of a previous complete and read group transaction, it was possible that other readers in that
            group coherent subscriber would not be able to read a newly send transaction.
            The new transaction was never marked complete since it still expected data for the deleted reader.
             <br/>
             <b>Solution: Updated the coherent administration so that the reader is not longer part of future group transactions.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8900
          </td>
          <td>
            <b>Tuner - When writing data with the standalone writer frame, it is not possible to edit collections.</b><br/>
            <i>
            Normally, in the reader-writer frame, right-clicking on a sequence user data field brings up a context menu to Add Details to the collection. The same action is missing from the standalone writer frame.
             <br/>
             <b>Solution: The writer frame's table now has the ability to edit collections in the same manner as the reader-writer frame does.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8577
          </td>
          <td>
            <b>Tuner - When writing data with the standalone writer frame, the data model is not properly initialized.</b><br/>
            <i>
            When writing data in the standalone writer frame, accessed via the context menu option "Write data" on any Writer tree entity, clicking the Write button without first editing any of the default table values results in write error.
             <br/>
             <b>Solution: The table's backing DDS data object is properly initialized with the table's values when a writer action is called.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8507
          </td>
          <td>
            <b>Transactions with explicit registration messages never become complete.</b><br/>
            <i>
            When doing begin_coherent_changes then register an instance and finally call end_coherent_changes the total transaction could fail because the register_instance is not always sent.
             <br/>
             <b>Solution: The defect in the transaction mechanism is solved and registration messages are not properly sent.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8930
          </td>
          <td>
            <b>The lease manager has to support leases which use the monotonic clock and leases which use the elapsed time clock.</b><br/>
            <i>
            The lease manager is used to handle events that have to be processed at a certain time. For example the monitor the liveliness of the services
            the lease manager has to evaluate the corresponding leases at regular intervals. These leases are using the monotonic clock to support hibernation.
            However the lease manager is also used to monitor the liveliness of instances or when the deadline QoS is set to monitor if the deadline settings
            are satisfied. For these leases the elapsed time clock is used. Internally the lease manager used the monotonic clock. This may cause problems for
            leases which use the elapsed time clock when hibernation occurs.
             <br/>
             <b>Solution: The defect in the transaction mechanism is solved and registration messages are not properly sent.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9171
          </td>
          <td>
            <b>A Condition detach from a Waitset can block.</b><br/>
            <i>
            If a thread is waiting on a waitset and another thread detaches a condition from that waitset, the waitset is triggered after which the
            detach can take place. However, if the first thread is very quick (or the detaching thread is slow), it can happen that the first thread
            already enters the wait again while the detach didn't detect the trigger yet. When that happens, the detach will block (at least) until 
            the waitset is triggered again.
             <br/>
             <b>Solution: Wait for a detach to finish when entering a waitset wait.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-7980
          </td>
          <td>
            <b>DDSI2 retransmits full sample on a retransmit request for the
            sample, even if the sample is huge</b><br/>
            <i>
            The DDSI reliable protocol offers two different ways of requesting
            a retransmit of some data: a sample retransmit request and, for
            fragmented data (i.e., large samples), a fragment retransmit request.
            DDSI2 would always retransmit the full sample upon receiving a
            sample retransmit, even if that sample is huge, instead of
            retransmitting a "reasonable" amount and relying on further fragment
            retransmit requests.
             <br/>
             <b>Solution: The DDSI2 service now retransmits a limited amount of
             data when receiving a retransmit request for a full sample.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8017
          </td>
          <td>
            <b>DDSI2 did not renew a participant lease for every received message</b><br/>
            <i>
            The DDSI2 service discovers remote participants and automatically
            deletes them if they do not renew their leases in time. The lease
            renewal was tied to reception of data and of explicit lease renewal
            messages, and hence reception of, e.g., an acknowledgement would not
            lead to a lease renewal, even though it obviously requires the
            remote participant to be alive.
             <br/>
             <b>Solution: DDSI2 now renews leases regardless of the type of
             message.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8636
          </td>
          <td>
            <b>Streams throughput example hang on windows</b><br/>
            <i>
            When using the streams throughput example on windows the subscriber
            application could hang on termination.
             <br/>
             <b>Solution: The defect is fixed and the subscriber will not hang
             anymore.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8660
          </td>
          <td>
            <b>RMI cpp does not reduce thread pool size immediately.</b><br/>
            <i>
            The threadPoolSize of the RMI cpp ServerThreadingPolicy can be
            reduced. The threads within the threadpool are quit after they
            handled a request task when the number of threads are larger then
            the required threadPoolSize. Because the request tasks are handled
            before the number of threads are reduced, it is possible for the
            Server to get more parallel calls then expected.
             <br/>
             <b>Solution: Quit a thread within the threadpool until the number
             of threads are equal to the threadPoolSize. Then continue handling
             the request tasks.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8739 / 16698
          </td>
          <td>
            <b>Too generic c_insert symbol</b><br/>
            <i>
            The c_insert symbol is exported in one of the OpenSplice libraries.
            This is too generic and causes clashes with other software packages.
             <br/>
             <b>Solution: The c_insert function has been renamed by prefixing
              it with the ospl_ prefix.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8828
          </td>
          <td>
            <b>Possible bufferoverflow when using Google Protocol Buffers on Windows</b><br/>
            <i>
            When using Google Protocol Buffers on Windows with Isocpp it can
            happen that the application crashes with a buffer overflow. This is
            due to a defect in the translation from gpb to dds datatypes.
             <br/>
             <b>Solution: The defect is fixed and the overflow will not occur
             anymore.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9097</td>
          <td>
            <b>DDSI transmit path can lock up on packet loss to one node while another node has crashed</b><br/>
            <i>A successful retransmit to one remote reader while another remote reader that has not yet acknowledged all samples disappears (whether because of a loss of connectivity or a crash), and when all other remote readers have acknowledged all samples, and while the writer has reached the maximum amount of unacknowledged data would cause the transmit path in DDSI to lock up because the writer could then only be unblocked by the receipt of an acknowledgement message that covers a previously unacknowledged sample, which under these circumstances will not come because of the limit on the amount of unacknowledged data.
              <br/>
              <b>Solution: deleting a reader now not only drops all unacknowledged data but also clears the retransmit indicator of the writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9096</td>
          <td>
            <b>Durability service DIED message even though the durability service is still running</b><br/>
            <i>The d_status topic is published periodically by the durability service to inform its fellows of its status. By using a KEEP_ALL policy, the thread writing the status message and renewing the serivce lease could be blocked by a flow-control issue on the network, which could cause the durability service to be considered dead by the splice daemon when in fact there was no problem with the durability service.
              <br/>
              <b>Solution: use a KEEP_LAST 1 history QoS policy for the writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9067</td>
          <td>
            <b>Large topics are sent published but not received</b><br/>
            <i>
            Loss of the initial transmission of the final fragments of a large sample failed to cause retransmit requests for those fragments until new data was published by the same writer.
              <br/>
              <b>Solution: ensure the receiving side will also request retransmission of those fragments based on heartbeats advertising the existence of the sample without giving specifics on the number of fragments.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9077 / 00016820
          </td>
          <td>
            <b>Potential crash in durability service during CATCHUP policy</b><br/>
            <i>
            The durability service could crash while processing a CATCHUP event. This crash was caused
            by the garbage collector purging old instances while the CATCHUP policy was walking through
            the list of instances to do some bookkeeping.
             <br/>
             <b>Solution: The CATCHUP policy now creates a private copy of the instance list while the
             garbage collector is unable to make a sweep. This private list is then used to do the bookkeeping.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9068 / 00016813
          </td>
          <td>
            <b>Catchup policy may leak away some instances</b><br/>
            <i>
            When a node that performs a catchup to the master contains an instance that the master has already
            purged, then the node catching up would need to purge this instance as well. It would need to do
            this by re-registering the instance, inserting a dispose message and then unregistering this instance
            again. However, the unregister step was missing, causing the instance to effectively leak away since
            an instance is only purged by the durability service when it is both disposed AND unregistered.
             <br/>
             <b>Solution: The durability will now both dispose AND unregister the instance at the same time.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9081 / 00016824
          </td>
          <td>
            <b>Potential deadlock in the OpenSplice kernel</b><br/>
            <i>
            The OpenSplice kernel has a potential deadlock where two different code paths may claim
            locks in the opposite order. The deadlock occurs when one thread is reading/taking the
            data out of a DataReader while the participant's listener thread is processing the creation
            of a new group (i.e. a unique partition/topic combination) to which this Reader's Subscriber
            is also attached.
             <br/>
             <b>Solution: The locking algorithm has been modified in such a way that the participant's
             listener thread no longer requires to hold both locks at the same time.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9064 / 00016808
          </td>
          <td>
            <b>Changes caused by OSPL-8914 in 6.6.3p4f4 have been reverted</b><br/>
            <i>
            OSPL-8914 in the 6.6.3p4f4 release has made several changes to the durability service in order to solve
            problems where a rapid disconnect/reconnect cycle would leave the durability service in an undefined
            state. In these situations, a disconnect had not yet been fully processed when the reconnect occurs.
            However, the solutions provided in 6.6.4 caused other, previously non-existing errors during normal operation.
             <br/>
             <b>Solution: All changes made as part of OSPL-8914 in the 6.6.4 release have been reverted.
             As an alternative solution to rapid disconnect/reconnect cycle issues, ddsi has offered temporary
             blacklisting of recently disconnected participants (see OSPL-8956).
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8956
          </td>
          <td>
            <b>Temporary blacklisting of remote participants in DDSI2</b><br/>
            <i>
            The DDSI2 service now provides an option to temporarily block rediscovery of proxy participants. Blocking
            rediscovery gives the remaining processes on the node extra time to clean up. It is strongly advised that
            applications are written in such a way that they can handle reconnects at any time, but when issues are
            found, this feature can reduce the symptoms.
            <br/>
             <b>Solution: A new setting in the DDSI section of the configuration has been added:
             Internal/RediscoveryBlacklistDuration along with an attribute Internal/RediscoveryBlacklistDuration
             [@enforce]. The former sets the duration (by default 10s), the second whether to really wait out the
             full period (true), or to allow reconnections once DDSI2 has internally completed cleaning up
             (false, the default). It strongly discouraged to set the duration to less than 1s.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9071
          </td>
          <td>
            <b>v_groupFlushAction passes a parameter that is not fully initialized.</b><br/>
            <i>
            Valgrind reported that the v_groupFlushAction function passes a parameter that is not fully initialized.
            Although one of these parameters was evaluated in a subsequent function invocation, it never caused issues
            because the value was only used as an operand for a logical AND where the other operand was always FALSE.
            <br/>
             <b>Solution: All attributes of the parameter in question are now explicitly initialized.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9055
          </td>
          <td>
            <b>Potential Sample drop during delivery to a local Reader</b><br/>
            <i>
            In some cases, a dispose followed by an unregister does not result in NOT_ALIVE_DISPOSED state on a Reader
            residing on the same node as the Publisher. In those cases, the Reader has an end state set to
            NOT_ALIVE_NO_WRITERS, and reports that a sample has been Lost.
            <br/>
             <b>Solution: We have no clue what could cause this behaviour, but added some logging to capture the
             context of the erroneous sample drop. This is just a temporary measure, and will be reverted when
             the root cause has been found and fixed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9056
          </td>
          <td>
            <b>Potential deadlock during early abort of an application</b><br/>
            <i>
            When an application aborts so quickly that the participant's leaseManager thread and its resendManager
            thread have not yet had the opportunity to get started, then the exit handler will block indefinitely
            waiting for these threads to exit the kernel. However, both threads are already blocked waiting to access
            a kernel that is already in lockdown.
            <br/>
             <b>Solution: The constructor of the participant will not return before both the leaseManager and resendManager
             threads have entered the kernel successfully.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8953
          </td>
          <td>
            <b>Potential deadlock between reader creation and durability notification</b><br/>
            <i>
            A thread that creates a new DataReader and a thread from the durability service that notifies a
            DataReader when it has completed its historical data alignment grab two of their locks in reverse
            order, causing a potential deadlock to occur.
            <br/>
             <b>Solution: The locking algorithm has been modified so that these two threads do no longer grab
             both locks in reverse order.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8886
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
              When the disconnection period is shorter than twice the heartbeat a durability service may not have been able to determine a new master before the node is reconnected again. In that case no master conflict is generated. In case the durability service is &quot;late&quot; in confirming a master it might even occur that the master has updated its namespace, but the namespace update is discarded because no confirmed master has been selected yet. As a consequence no request will for data will be sent to the master, and the durability service will not be aligned.
             <br/>
             <b>Solution: In case a durability service receives a namespace update for a namespace for which no confirmed master is selected yet, the update is rescheduled for evaluation at a later time instead of discarding the update.
             </b>
            </td>
        </tr>
        <tr>
          <td>
            OSPL-8914
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
            When a node becomes disconnected it may loose its master. As a result the node will look for a new master. In doing so, the node would first unconfirm its current master and then wait for other fellows to propose a new master. The time to look for a new master is specified in the configuration file (DurabilityService.Network.Heartbeat.ExpiryTime). When the disconnection was shorter than the DurabilityService.Network.Heartbeat.ExpiryTime, no merge is triggered.
             <br/>
             <b>Solution: whenever a node is discovered that is not simply starting and it has no confirmed master, a merge is triggered, just like when there are conflicting masters.
             </b><br></br>
            </i>
            <b> NOTE: This change has been reverted in 6.6.3p4f7.</b>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8948 / 16755<br/>
            OSPL-8987
          </td>
          <td>
            <b>Race condition between durability data injection and garbage collecting of empty instances</b><br/>
            <i>
              The durability service cached instance handles when injecting a historical data set in a way that could result in the historical samples being thrown away if the instance was empty and no known writers had registered it.<br/>
             <b>Solution: the instance handle is no longer cached..</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8971
          </td>
          <td>
            <b>Catchup policy may incorrectly mark unregistered instances as
            disposed.</b><br/>
            <i>
              When an instance is unregistered on the master node during a
              disconnect from another node that has specified a CATCHUP policy
              with that master, then upon a reconnect that unregister message
              will still be delivered to that formerly disconnected node.
              However, the reconnected node will dispose all instances for which
              it did not receive any valid data, so if the unregister message it
              the only message received for a particular instance, then its
              instance will be disposed.
              <br/>
              <b>Solution:The Catchup policy is now instructed to dispose
              instances for which it did not receive any valid data OR for which
              it did not receive any unregister message.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8984
          </td>
          <td>
            <b>DDSI handling of non-responsive readers needs improvement</b><br/>
            <i>
              When a writer is blocked for ResponsiveTimeout seconds, DDSI will
              declare the matching proxy readers that have not yet acknowledged
              all data "non-responsive" and continue with those readers
              downgraded to best-effort. This prevents blocking outgoing traffic
              indefinitely, but at the cost of breaking reliability.
              For historical reasons it was set to 1s to limit the damage a
              non-responsive reader could cause, but past improvements to the
              handling of built-in data in combination with DDSI (such as fully
              relying on DDSI discovery for deriving built-in topics) mean there
              is no longer a need to have such an aggressive setting by default.
             <br/>
             <b>Solution: The default behaviour has been changed to never
             declare a reader non-responsive and maintain reliability also when
             a remote reader is not able to make progress. The changes also
             eliminate some spurious warning and error messages in the log files
             that could occur with a longer timeout.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8920
          </td>
          <td>
            <b>DDSI2 Crash</b><br/>
            <i>
            Version 6.6.3p4 introduced a fix for OSPL-8872, taking the sequence number most recently transmitted by a writer when it matched reader into account to force heartbeats out until all historical data has been acknowledged by the reader. The change also allowed a flag forcing the transmission of heartbeats informing readers of the availability of data to be set earlier than before in the case where the writer had not published anything yet at the time the reader was discovered. While logically correct, this broke the determination of the unique reader that had not yet acknowledged all data in cases where there is such a unique reader. This in turn could lead to a crash.
             <br/>
             <b>Solution: the aforementioned flag is once again never set before a sample has been acknowledged.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.4p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8425 / 16551
          </td>
          <td>
            <b>When a lot of fragmented, best effort data is received, the
            receiver will run out of buffer space</b><br/>
            <i>
              Defragmentation buffers is shared with the receive buffer. In case
              a lot of data is received, the networkstack isn't able to free
              claimed buffer space as it doesn't get to time to defragmentate
              data in the buffers. As a result, incoming data can not be stored
              and networking can't recover from this situation as these buffers
              remain locked.
             <br/>
             <b>Solution: In case no buffers can be freed, drop data in buffers
             to create free space to be able to receive new data and continue to
             function. Data in the dropped buffers are lost but as this is
             best-effort data, this is allowed. 
             </b>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8886
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
              When the disconnection period is shorter than twice the heartbeat a durability service may not have been able to determine a new master before the node is reconnected again. In that case no master conflict is generated. In case the durability service is &quot;late&quot; in confirming a master it might even occur that the master has updated its namespace, but the namespace update is discarded because no confirmed master has been selected yet. As a consequence no request will for data will be sent to the master, and the durability service will not be aligned.
             <br/>
             <b>Solution: In case a durability service receives a namespace update for a namespace for which no confirmed master is selected yet, the update is rescheduled for evaluation at a later time instead of discarding the update. 
             </b>
            </td>
        </tr>
        <tr>
          <td>
            OSPL-8914
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
            When a node becomes disconnected it may loose its master. As a result the node will look for a new master. In doing so, the node would first unconfirm its current master and then wait for other fellows to propose a new master. The time to look for a new master is specified in the configuration file (DurabilityService.Network.Heartbeat.ExpiryTime). When the disconnection was shorter than the DurabilityService.Network.Heartbeat.ExpiryTime, no merge is triggered.
             <br/>
             <b>Solution: whenever a node is discovered that is not simply starting and it has no confirmed master, a merge is triggered, just like when there are conflicting masters.
             </b><br></br>
            </i>
            <b> NOTE: This change has been reverted in 6.6.3p4f7.</b>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8920
          </td>
          <td>
            <b>DDSI2 Crash</b><br/>
            <i>
            Version 6.6.3p4 introduced a fix for OSPL-8872, taking the sequence number most recently transmitted by a writer when it matched reader into account to force heartbeats out until all historical data has been acknowledged by the reader. The change also allowed a flag forcing the transmission of heartbeats informing readers of the availability of data to be set earlier than before in the case where the writer had not published anything yet at the time the reader was discovered. While logically correct, this broke the determination of the unique reader that had not yet acknowledged all data in cases where there is such a unique reader. This in turn could lead to a crash.
             <br/>
             <b>Solution: the aforementioned flag is once again never set before a sample has been acknowledged.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8928
          </td>
          <td>
            <b>Improve FACE documentation</b><br/>
            <i>
              FACE documentation is very rudimentary.
             <br/>
             <b>Solution: Getting started, API documentation, configuration
             documentation and example have been added. 
             </b>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8948 / 16755<br/>
            OSPL-8987
          </td>
          <td>
            <b>Race condition between durability data injection and garbage collecting of empty instances</b><br/>
            <i>
              The durability service cached instance handles when injecting a historical data set in a way that could result in the historical samples being thrown away if the instance was empty and no known writers had registered it.<br/>
             <b>Solution: the instance handle is no longer cached..</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8953
          </td>
          <td>
            <b>Potential deadlock between reader creation and durability notification</b><br/>
            <i>
            A thread that creates a new DataReader and a thread from the durability service that notifies a 
            DataReader when it has completed its historical data alignment grab two of their locks in reverse 
            order, causing a potential deadlock to occur.
            <br/>
             <b>Solution: The locking algorithm has been modified so that these two threads do no longer grab 
             both locks in reverse order.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8956
          </td>
          <td>
            <b>Temporary blacklisting of remote participants in DDSI2</b><br/>
            <i>
            The DDSI2 service now provides an option to temporarily block rediscovery of proxy participants. Blocking 
            rediscovery gives the remaining processes on the node extra time to clean up. It is strongly advised that 
            applications are written in such a way that they can handle reconnects at any time, but when issues are 
            found, this feature can reduce the symptoms.
            <br/>
             <b>Solution: A new setting in the DDSI section of the configuration has been added: 
             Internal/RediscoveryBlacklistDuration along with an attribute Internal/RediscoveryBlacklistDuration
             [@enforce]. The former sets the duration (by default 10s), the second whether to really wait out the 
             full period (true), or to allow reconnections once DDSI2 has internally completed cleaning up 
             (false, the default). It strongly discouraged to set the duration to less than 1s.
             </b>
            </i>
          </td>
        </tr> 
        <tr>
          <td>
            OSPL-8971
          </td>
          <td>
            <b>Catchup policy may incorrectly mark unregistered instances as
            disposed.</b><br/>
            <i>
              When an instance is unregistered on the master node during a
              disconnect from another node that has specified a CATCHUP policy
              with that master, then upon a reconnect that unregister message
              will still be delivered to that formerly disconnected node.
              However, the reconnected node will dispose all instances for which
              it did not receive any valid data, so if the unregister message it
              the only message received for a particular instance, then its
              instance will be disposed.
              <br/>
              <b>Solution:The Catchup policy is now instructed to dispose
              instances for which it did not receive any valid data OR for which
              it did not receive any unregister message.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8984
          </td>
          <td>
            <b>DDSI handling of non-responsive readers needs improvement</b><br/>
            <i>
              When a writer is blocked for ResponsiveTimeout seconds, DDSI will
              declare the matching proxy readers that have not yet acknowledged
              all data "non-responsive" and continue with those readers
              downgraded to best-effort. This prevents blocking outgoing traffic
              indefinitely, but at the cost of breaking reliability.
              For historical reasons it was set to 1s to limit the damage a
              non-responsive reader could cause, but past improvements to the
              handling of built-in data in combination with DDSI (such as fully
              relying on DDSI discovery for deriving built-in topics) mean there
              is no longer a need to have such an aggressive setting by default.
             <br/>
             <b>Solution: The default behaviour has been changed to never
             declare a reader non-responsive and maintain reliability also when
             a remote reader is not able to make progress. The changes also
             eliminate some spurious warning and error messages in the log files
             that could occur with a longer timeout.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9027
          </td>
          <td>
            <b>idlpp is not robust to paths with whitespace</b><br/>
            <i>
             The idlpp tool is not able to handle paths that contain whitespaces
             when compiling for cpp and thus using cppgen.
            <br/>
             <b>Solution: Add quotes to the arguments for cppgen, which are
             generated by idlpp.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9028
          </td>
          <td>
            <b>Bug in serializing messages with sequences of 64 bit</b><br/>
            <i>
            The SAC generic copy routines have an issue with alignment and empty
            sequences. Additionally, the legacy CDR serialiser (the only one up
            to but not including 6.4.1) has this issue because it always inserts
            padding for sequences of primitive and enumerated types even when
            the sequence is empty. 
            <br/>
             <b>Solution:The SAC copy routines have been modified to be able to
             deal with empty sequences. Even though the new CDR serialiser
             (since 6.4.1) does not have this issue and versions from 6.4.1
             onward are not affected, the old code is still a compile-time
             option in the current versions for development purposes and so that
             one is fixed, too.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9055
          </td>
          <td>
            <b>Potential Sample drop during delivery to a local Reader</b><br/>
            <i>
            In some cases, a dispose followed by an unregister does not result in NOT_ALIVE_DISPOSED state on a Reader 
            residing on the same node as the Publisher. In those cases, the Reader has an end state set to 
            NOT_ALIVE_NO_WRITERS, and reports that a sample has been Lost.
            <br/>
             <b>Solution: We have no clue what could cause this behaviour, but added some logging to capture the 
             context of the erroneous sample drop. This is just a temporary measure, and will be reverted when
             the root cause has been found and fixed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9056
          </td>
          <td>
            <b>Potential deadlock during early abort of an application</b><br/>
            <i>
            When an application aborts so quickly that the participant's leaseManager thread and its resendManager
            thread have not yet had the opportunity to get started, then the exit handler will block indefinitely
            waiting for these threads to exit the kernel. However, both threads are already blocked waiting to access
            a kernel that is already in lockdown.
            <br/>
             <b>Solution: The constructor of the participant will not return before both the leaseManager and resendManager
             threads have entered the kernel successfully.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9058 / 16796<br/>
            OSPL-9206 / 16852
          </td>
          <td>
            <b>Incompatibility with versions before V6.5.0p5</b><br/>
            <i>
            An internal change to builtin heartbeat topic caused an
            incompatibility with older versions. When adding a node running a
            recent version of OpenSplice to a domain with nodes running a
            version before V6.5.0p5, the existing nodes would incorrectly
            dispose participants (and corresponding entities) belonging to the
            new nodes after a single heartbeat period, normally done only when
            a heartbeat expires.
             <b>Solution: To resolve this, the change to the heartbeat topic was
             reverted.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9064 / 16808
          </td>
          <td>
            <b>Changes caused by OSPL-8914 in 6.6.3p4f4 have been reverted</b><br/>
            <i>
            OSPL-8914 in the 6.6.3p4f4 release has made several changes to the durability service in order to solve
            problems where a rapid disconnect/reconnect cycle would leave the durability service in an undefined
            state. In these situations, a disconnect had not yet been fully processed when the reconnect occurs.
            However, the solutions provided in 6.6.4 caused other, previously non-existing errors during normal operation.
             <br/>
             <b>Solution: All changes made as part of OSPL-8914 in the 6.6.4 release have been reverted.
             As an alternative solution to rapid disconnect/reconnect cycle issues, ddsi has offered temporary
             blacklisting of recently disconnected participants (see OSPL-8956).
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9067</td>
          <td>
            <b>Large topics are sent published but not received</b><br/>
            <i>
            Loss of the initial transmission of the final fragments of a large sample failed to cause retransmit requests for those fragments until new data was published by the same writer.
              <br/>
              <b>Solution: ensure the receiving side will also request retransmission of those fragments based on heartbeats advertising the existence of the sample without giving specifics on the number of fragments.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9068 / 00016813
          </td>
          <td>
            <b>Catchup policy may leak away some instances</b><br/>
            <i>
            When a node that performs a catchup to the master contains an instance that the master has already 
            purged, then the node catching up would need to purge this instance as well. It would need to do 
            this by re-registering the instance, inserting a dispose message and then unregistering this instance 
            again. However, the unregister step was missing, causing the instance to effectively leak away since 
            an instance is only purged by the durability service when it is both disposed AND unregistered.
             <br/>
             <b>Solution: The durability will now both dispose AND unregister the instance at the same time.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9071
          </td>
          <td>
            <b>v_groupFlushAction passes a parameter that is not fully initialized.</b><br/>
            <i>
            Valgrind reported that the v_groupFlushAction function passes a parameter that is not fully initialized. 
            Although one of these parameters was evaluated in a subsequent function invocation, it never caused issues 
            because the value was only used as an operand for a logical AND where the other operand was always FALSE.
            <br/>
             <b>Solution: All attributes of the parameter in question are now explicitly initialized.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9077 / 00016820
          </td>
          <td>
            <b>Potential crash in durability service during CATCHUP policy</b><br/>
            <i>
            The durability service could crash while processing a CATCHUP event. This crash was caused 
            by the garbage collector purging old instances while the CATCHUP policy was walking through 
            the list of instances to do some bookkeeping.
             <br/>
             <b>Solution: The CATCHUP policy now creates a private copy of the instance list while the 
             garbage collector is unable to make a sweep. This private list is then used to do the bookkeeping.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9081 / 00016824
          </td>
          <td>
            <b>Potential deadlock in the OpenSplice kernel</b><br/>
            <i>
            The OpenSplice kernel has a potential deadlock where two different code paths may claim
            locks in the opposite order. The deadlock occurs when one thread is reading/taking the 
            data out of a DataReader while the participant's listener thread is processing the creation 
            of a new group (i.e. a unique partition/topic combination) to which this Reader's Subscriber 
            is also attached.
             <br/>
             <b>Solution: The locking algorithm has been modified in such a way that the participant's
             listener thread no longer requires to hold both locks at the same time.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9096</td>
          <td>
            <b>Durability service DIED message even though the durability service is still running</b><br/>
            <i>The d_status topic is published periodically by the durability service to inform its fellows of its status. By using a KEEP_ALL policy, the thread writing the status message and renewing the serivce lease could be blocked by a flow-control issue on the network, which could cause the durability service to be considered dead by the splice daemon when in fact there was no problem with the durability service.
              <br/>
              <b>Solution: use a KEEP_LAST 1 history QoS policy for the writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-9097</td>
          <td>
            <b>DDSI transmit path can lock up on packet loss to one node while another node has crashed</b><br/>
            <i>A successful retransmit to one remote reader while another remote reader that has not yet acknowledged all samples disappears (whether because of a loss of connectivity or a crash), and when all other remote readers have acknowledged all samples, and while the writer has reached the maximum amount of unacknowledged data would cause the transmit path in DDSI to lock up because the writer could then only be unblocked by the receipt of an acknowledgement message that covers a previously unacknowledged sample, which under these circumstances will not come because of the limit on the amount of unacknowledged data.
              <br/>
              <b>Solution: deleting a reader now not only drops all unacknowledged data but also clears the retransmit indicator of the writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9388 / 17030
          </td>
          <td>
            <b>Durability service might deadlock when the networking queue is
            flooded.</b><br/>
            <i>
            When the network queue is overrun by the durability service, the
            normal mode of operation is to sleep a bit and retry again later.
            However, there is a slight chance that the sending thread of the
            network service that needs to make room again by consuming elements
            in the queue will indirectly block on the sleeping thread in the
            durability service itself.
             <b>Solution: The network service can no longer indirectly run into
             a lock that is held by the durability service while the network
             queue is flooded.</b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.6.4p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-9016
          </td>
          <td>
            <b>Changes caused by OSPL-8914 in 6.6.4 have been reverted</b><br/>
            <i>
            OSPL-8914 in the 6.6.4 release has made several changes to the durability service in order to solve
            problems where a rapid disconnect/reconnect cycle would leave the durability service in an undefined
            state. In these situations, a disconnect had not yet been fully processed when the reconnect occurs.
            However, the solutions provided in 6.6.4 caused other, previously non-existing errors during normal operation.
             <br/>
             <b>Solution: All changes made as part of OSPL-8914 in the 6.6.4 release have been reverted.
             As an alternative solution to rapid disconnect/reconnect cycle issues, ddsi has offered temporary
             blacklisting of recently disconnected participants (see OSPL-8956).
             </b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.6.4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8953
          </td>
          <td>
            <b>Potential deadlock between reader creation and durability notification</b><br/>
            <i>
            A thread that creates a new DataReader and a thread from the durability service that notifies a
            DataReader when it has completed its historical data alignment grab two of their locks in reverse
            order, causing a potential deadlock to occur.
            <br/>
             <b>Solution: The locking algorithm has been modified so that these two threads do no longer grab
             both locks in reverse order.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8636
          </td>
          <td>
            <b>Streams throughput example hang on windows</b><br/>
            <i>
           When using the streams throughput example on windows the subscriber
           application could hang on termination.
            <br/>
             <b>Solution: The defect is fixed and the subscriber will not hang
             anymore.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8886
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
              When the disconnection period is shorter than twice the heartbeat a durability service may not have been able to determine a new master before the node is reconnected again. In that case no master conflict is generated. In case the durability service is &quot;late&quot; in confirming a master it might even occur that the master has updated its namespace, but the namespace update is discarded because no confirmed master has been selected yet. As a consequence no request will for data will be sent to the master, and the durability service will not be aligned.
             <br/>
             <b>Solution: In case a durability service receives a namespace update for a namespace for which no confirmed master is selected yet, the update is rescheduled for evaluation at a later time instead of discarding the update.
             </b>
            </td>
        </tr>
        <tr>
          <td>
            OSPL-8914
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
            When a node becomes disconnected it may loose its master. As a result the node will look for a new master. In doing so, the node would first unconfirm its current master and then wait for other fellows to propose a new master. The time to look for a new master is specified in the configuration file (DurabilityService.Network.Heartbeat.ExpiryTime). When the disconnection was shorter than the DurabilityService.Network.Heartbeat.ExpiryTime, no merge is triggered.
             <br/>
             <b>Solution: whenever a node is discovered that is not simply starting and it has no confirmed master, a merge is triggered, just like when there are conflicting masters.
             </b><br></br>
            </i>
            <b> NOTE: This change has been reverted in 6.6.4p1 because it may break other durability functionality.</b>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8920
          </td>
          <td>
            <b>DDSI2 Crash</b><br/>
            <i>
            Version 6.6.3p4 introduced a fix for OSPL-8872, taking the sequence number most recently transmitted by a writer when it matched reader into account to force heartbeats out until all historical data has been acknowledged by the reader. The change also allowed a flag forcing the transmission of heartbeats informing readers of the availability of data to be set earlier than before in the case where the writer had not published anything yet at the time the reader was discovered. While logically correct, this broke the determination of the unique reader that had not yet acknowledged all data in cases where there is such a unique reader. This in turn could lead to a crash.
            <br/>
             <b>Solution: the aforementioned flag is once again never set before a sample has been acknowledged.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8948 / 16755<br/>
            OSPL-8987
          </td>
          <td>
            <b>Race condition between durability data injection and garbage collecting of empty instances</b><br/>
            <i>
              The durability service cached instance handles when injecting a historical data set in a way that could result in the historical samples being thrown away if the instance was empty and no known writers had registered it.<br/>
             <b>Solution: the instance handle is no longer cached..</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8956
          </td>
          <td>
            <b>Temporary blacklisting of remote participants in DDSI2</b><br/>
            <i>
            The DDSI2 service now provides an option to temporarily block rediscovery of proxy participants. Blocking
            rediscovery gives the remaining processes on the node extra time to clean up. It is strongly advised that
            applications are written in such a way that they can handle reconnects at any time, but when issues are
            found, this feature can reduce the symptoms.
            <br/>
             <b>Solution: A new setting in the DDSI section of the configuration has been added:
             Internal/RediscoveryBlacklistDuration along with an attribute Internal/RediscoveryBlacklistDuration
             [@enforce]. The former sets the duration (by default 10s), the second whether to really wait out the
             full period (true), or to allow reconnections once DDSI2 has internally completed cleaning up
             (false, the default). It strongly discouraged to set the duration to less than 1s.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8957
          </td>
          <td>
            <b>Unnecessary heartbeat/acknowledgement traffic in DDSI with late joiners for transient-local data</b><br/>
            <i>
            In DDSI, reliable writers send heartbeats to inform their readers of
            the existence of unacknowledged data, and keep doing so until the
            readers have acknowledged everything. The heartbeats indicate the
            range of sequence numbers available. The highest sequence number
            advertised in the heartbeat was the highest sequence number
            available for retransmit, leaving out any subsequent sequence
            numbers that are no longer available for retransmit. If a
            transient-local writer unregistered an instance and then became
            quiescent, it would be advertising a sequence number less than the
            latest sequence number it published, and this would lead to a
            late-joining reader never acknowledging all data up to this latest
            sequence number. As a result, the writer would keep sending
            heartbeats and the readers would keep acknowledging it. Sending new
            samples from the writer would break the cycle.
            <br/>
             <b>Solution: the writer now advertises the latest sequence number
             it published. If there is a gap between the latest available for
             retransmit and the latest published, the reader will be informed
             that these sequence numbers are no longer relevant using a standard
             message.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8958
          </td>
          <td>
            <b>DDSI can regurgitate old T-L samples for instances that have
            already been unregistered</b><br/>
            <i>
            DDSI maintains a writer history cache for providing historical data
            for transient-local writers and for providing reliability. An
            instance is removed from this cache when it is unregistered by the
            writer, but its samples are retained until they have been
            acknowledged by all (reliable) readers. Already acknolwedged samples
            that were retained because they were historical data could survive
            even when the instance was removed. When this happened, a
            late-joining reader would see some old samples reappear.
            <br/>
             <b>Solution: deleting an instance now also removes the already
             acknowledged samples from the history.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8971
          </td>
          <td>
            <b>Catchup policy may incorrectly mark unregistered instances as
            disposed.</b><br/>
            <i>
              When an instance is unregistered on the master node during a
              disconnect from another node that has specified a CATCHUP policy
              with that master, then upon a reconnect that unregister message
              will still be delivered to that formerly disconnected node.
              However, the reconnected node will dispose all instances for which
              it did not receive any valid data, so if the unregister message it
              the only message received for a particular instance, then its
              instance will be disposed.
              <br/>
              <b>Solution:The Catchup policy is now instructed to dispose
              instances for which it did not receive any valid data OR for which
              it did not receive any unregister message.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8972
          </td>
          <td>
            <b>Durability fellow state may be incorrect</b><br/>
            <i>
              A durability service keeps track of the state of each fellow it
              knows. In every message that fellow sends to a durability service
              the state of the fellow is included. The recipient has
              different threads to handle incoming messages. In rare cases,
              these messages are processed in the reverse order, thus
              ending up with a wrong conclusion about the fellow's state.
              <br/>
              <b>Solution: The durability service now uses the source
              timestamp in incoming messages to determine the order in which
              they have been written to ensure it never updates the internal
              state using 'old' information.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8973
          </td>
          <td>
            <b>Additional durability tracing when verbosity is set to FINEST</b><br/>
            <i>
              Durability has been extended with additional tracing in the
              processing of namespace definitions received from fellows, in
              particular when checking for master conflicts.
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8974
          </td>
          <td>
            <b>Durability conflict scheduling fails when multiple namespaces
            have the same policy and differ only in topic names</b><br/>
            <i>
              Durability checks for conflicts between fellows (master, native
              and foreign state) that may require merging data whenever it
              receives a "d_nameSpaces" instance. If a conflict is detected, it
              enqueues it for eventual resolution, but only if an equivalent
              conflict is not yet enqueued. Testing for equivalency is done by
              checking: conflict kind, roles and local and fellow namespaces.
              However, the name space compare function (d_nameSpaceCompare) did
              not take the name into account, nor the full partition+topic
              expressions.
              The consequence is that when namespaces A and B have identical
              policies and differ only in the topic parts of the partition/topic
              expressions, a conflict for namespace A would be considered the
              same as a conflict for namespace B. The result would be a failure
              to merge data in B.
             <br/>
             <b>Solution: The comparison now takes the name of the namespace
             into account. The configuration is required to have no overlap
             between namespaces and to have compatible namespace definitions
             throughout the system. The name alone is therefore sufficient.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8979
          </td>
          <td>
            <b>DDSI incapable of receiving multicasts after restart in
            single-process mode</b><br/>
            <i>
              The tracking of joined multicast groups DDSI could not handle the
              case where DDSI would be restarted in single-process mode (e.g.,
              by creating a participant, deleting it, and creating another one),
              potentially causing the new sockets not to join the multicast groups.
             <br/>
             <b>Solution: DDSI now explicitly leaves all multicast groups on
             termination.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8980
          </td>
          <td>
            <b>With DDSI remote participants expire independently on cable
            disconnect</b><br/>
            <i>
              The DDSI protocol has lease expiry tied to participants and the
              DDSI service faithfully implemented this. This means that a cable
              disconnect occurs caused the leases of the various participants on
              the remote node to expire independently, and therefore also the
              automatic disposing and unregistering of data. A short
              disconnection where the lease of the durability service never
              expired, but where the lease of some application process did
              expire could lead to an inconsistent state of the data space, if
              that application published auto-disposed transient data.
             <br/>
             <b>Solution: DDSI has been modified to implement only a single
             lease per remote federation for OpenSplice peers by internally
             tying the leases of the applications to the lease of the remote
             DDSI service.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8984
          </td>
          <td>
            <b>DDSI handling of non-responsive readers needs improvement</b><br/>
            <i>
              When a writer is blocked for ResponsiveTimeout seconds, DDSI will
              declare the matching proxy readers that have not yet acknowledged
              all data "non-responsive" and continue with those readers
              downgraded to best-effort. This prevents blocking outgoing traffic
              indefinitely, but at the cost of breaking reliability.
              For historical reasons it was set to 1s to limit the damage a
              non-responsive reader could cause, but past improvements to the
              handling of built-in data in combination with DDSI (such as fully
              relying on DDSI discovery for deriving built-in topics) mean there
              is no longer a need to have such an aggressive setting by default.
             <br/>
             <b>Solution: The default behaviour has been changed to never
             declare a reader non-responsive and maintain reliability also when
             a remote reader is not able to make progress. The changes also
             eliminate some spurious warning and error messages in the log files
             that could occur with a longer timeout.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8989
          </td>
          <td>
            <b>Non-atomic dispose+unregister operation on DCPSHeartbeat</b><br/>
            <i>
              A race condition between auto-disposing/unregistering, taking data
              from a data reader and merging of historical data was resolved in
              OSPL-8684 by performing the an atomic dispose+unregister operation
              instead of two separate operations. This fix covered all cases
              except the DCPSHeartbeat built-in topic, which is handled
              specially by the splice daemon and still performed two independent
              operations.
             <br/>
             <b>Solution: The atomic dispose+unregister is now also used for
             DCPSHeartbeat. There are no user-visible consequences of this
             change.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-395
          </td>
          <td>
            <b>Python scripting examples need to be updated to reflect the
            ability to create and specify QoS settings on entities.</b><br/>
            <i>
             the example now shows:
             <ul>
             <li>reading and writing to non-default partition</li>
             <li>use a waitset to poll some data</li>
             <li>pass some user data through one of the QoS</li>
             <li>use a check to show that a reader and a writer don't match on QoS</li>
             </ul>
            </i>
          </td>
        </tr>
      </table>
      <h2>6.6.3p5</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8920
          </td>
          <td>
            <b>DDSI2 Crash</b><br/>
            <i>
            Version 6.6.3p4 introduced a fix for OSPL-8872, taking the sequence number most recently transmitted by a writer when it matched reader into account to force heartbeats out until all historical data has been acknowledged by the reader. The change also allowed a flag forcing the transmission of heartbeats informing readers of the availability of data to be set earlier than before in the case where the writer had not published anything yet at the time the reader was discovered. While logically correct, this broke the determination of the unique reader that had not yet acknowledged all data in cases where there is such a unique reader. This in turn could lead to a crash.
            <br/>
             <b>Solution: the aforementioned flag is once again never set before a sample has been acknowledged.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8914
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
            When a node becomes disconnected it may loose its master. As a result the node will look for a new master. In doing so, the node would first unconfirm its current master and then wait for other fellows to propose a new master. The time to look for a new master is specified in the configuration file (DurabilityService.Network.Heartbeat.ExpiryTime). When the disconnection was shorter than the DurabilityService.Network.Heartbeat.ExpiryTime, no merge is triggered.
             <br/>
             <b>Solution: whenever a node is discovered that is not simply starting and it has no confirmed master, a merge is triggered, just like when there are conflicting masters.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8886
          </td>
          <td>
            <b>Durability failure to merge data after a short disconnect</b><br/>
            <i>
              When the disconnection period is shorter than twice the heartbeat a durability service may not have been able to determine a new master before the node is reconnected again. In that case no master conflict is generated. In case the durability service is &quot;late&quot; in confirming a master it might even occur that the master has updated its namespace, but the namespace update is discarded because no confirmed master has been selected yet. As a consequence no request will for data will be sent to the master, and the durability service will not be aligned.
             <br/>
             <b>Solution: In case a durability service receives a namespace update for a namespace for which no confirmed master is selected yet, the update is rescheduled for evaluation at a later time instead of discarding the update. </b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.6.3p4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8884
          </td>
          <td>
            <b>Spliced assertion failure (_this->state &amp; L_UNREGISTER) in v_registrationMessageCompare</b><br/>
            <i>
            The record of a writer unregistering an instance is retained for a little before it is completely removed from the system.
            In the case of spliced and DCPSHeartbeat, if in this window the spliced needed to auto-dispose the DCPSHeartbeat, it could
            resurrect the registration but leave it in an inconsistent state, resulting in this assertion failure.
             <br/>
             <b>Solution: DDSI now correctly tags the "unregister" events.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8825 / 00016727
            OSPL-8722 / 00016668
          </td>
          <td>
            <b>Invalid timing during replay with non-default speed by Record and Replay service</b><br/>
            <i>
            A regression caused by a previous fix caused the Record and Replay service to not compensate for a non-default replay speed
            in certain circumstances during replay.
             <br/>
             <b>Solution: The issue was resolved by making some adjustments in the replay timing algorithm so the replay-speed is always
             factored in.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8684 / 00016645
          </td>
          <td>
            <b>Instances that were NOT_ALIVE due to a disconnect do not always become ALIVE after a reconnect</b><br/>
            <i>
            When a disconnect occurs, all instances owned by the disconnected Writers should become NOT_ALIVE_DISPOSED (in case a
            disconnected Writer had its auto_dispose_unregistered_instances set to TRUE) or NOT_ALIVE_NO_WRITERS. However, when
            the disconnected node reconnects, and the topic is non-VOLATILE, you might expect the instances to go back to the
            ALIVE state. However, that did not happen in all scenarios.
             <br/>
             <b>Solution: If the NOT_ALIVE instances are taken by the DataReader, they will now instantly be purged. When merge
             policies are configured to re-insert the missing messages back into the DataRerader after the connection is re-established,
             then these re-inserted samples will cause the instance to be re-created starting in the NEW, NOT_READ, ALIVE state.<br/>
             Note however, if the NOT_ALIVE samples are not taken out at the time of the disconnection, their instances will not be purged,
             and the re-inserted samples will be discarded as being duplicates of samples that are already there. In those cases the
             instance state will still not be changed back to ALIVE.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8857 / 00016738
          </td>
          <td>
            <b>Unable to start OpenSplice Tuner on Windows</b><br/>
            <i>
            An incomplete classpath in the manifest of ospltun.jar causes a ClassDefNotFound exception when starting the Tuner.
             <br/>
             <b>Solution: The issue was resolved by fixing the classpath handling in manifest file generation.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8872
          </td>
          <td>
            <b>DDSI transient local data retrieval may block indefinitely when packet loss occurs halfway through a fragmented
            sample if the writer is quiescent</b><br/>
            <i>
            When the DDSI service is retrieving historical transient-local data from a remote writer (this includes DDSI discovery data),
            it depends on a constant back-and-forth between requests for retransmits (NACKs) and heartbeats from the writer that allow
            it to send NACKs. The writer adapts its heartbeat rate based on the state of the readers it surmises from the ACKs/NACKs it
            receives. On receipt of an ACK it concludes the reader must have received all data (otherwise it should have NACKed the
            missing samples), which may result in no more heartbeats going out.</br>
            Requesting a retransmit of individual sample fragments is done using a message different from the standard NACK message.
            If a reader has received part of a fragmented sample, it will send such an alternative NACK for the missing fragments
            while sending an ACK for everything up to that point.</br>
            This ACK caused the writer to incorrectly conclude the reader had received everything. Writing a single new sample would
            cause the back-and-forth to restart and continue until all data had been transferred. Discovery of a new reader would
            temporarily trigger heartbeats as well, restarting the sequence, but in a state that further packet loss could cause it
            to stop again.
             <br/>
             <b>Solution: the writer requires that the ACK concerns a high enough sequence number before concluding it need not
             send any more heartbeats.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8847
          </td>
          <td>
            <b>When coherent transactions containing topics with strings as key are being aligned, the receiving durability service
            could crash</b><br/>
            <i>
            Durability services are used to align historical data. In case coherent transactions exist, special messages called
            End-Of_transactions (EOT) are aligned to indicate which parts of the transaction is completed. If topics are aligned
            that have a string as key and that belong to a transaction, then the receiving durability service could crash because
            it tries to access unallocated memory.
             <br/>
             <b>Solution: The code that was responsible for the illegal memory access is fixed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8803
          </td>
          <td>
            <b>Durability tracing logs do not mention the OpenSplice version number</b><br/>
            <i>
            The durability tracing log generated by the OpenSplice durability service (when tracing is enabled) does not
            mention the OpenSplice version number.
             <br/>
             <b>Solution: The OpenSplice version number is now added to the durability tracing log.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8819
          </td>
          <td>
            <b>Memory rise with transient local data using DDSI</b><br/>
            <i>
            When using transient local data in combination with DDSI, DDSI starts to allocate memory and never frees it.
             <br/>
             <b>Solution: When using transient local, DDSI did not request the receiver site to acknowledge the sent data,
             but still kept it in a list, allocation more and more memory for each message sent. DDSI has been changed to
             request acknowledgment when using transient-local data.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8771 / 00016706
          </td>
          <td>
            <b>Memory leak after deleting a transient reader</b><br/>
            <i>
            A reference counting error in determining whether a reader is entitled to transient data caused part of the data
            reader administration to be retained after deleting the data reader.
             <br/>
             <b>Solution: The stray reference is now released.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-7244
          </td>
          <td>
            <b>Tuner does not support writing samples containing multi-dimensional collections</b><br/>
            <i>
            The Tuner tool does not support editing multi-dimensional sequences (in IDL, sequence&lt;sequence&lt;some_type&gt;&gt;, or
            some_type[x][y] ). When editing such data fields in the Tuner writer table, the fields will be uneditable. This also affects
            editing Google Protocol Buffer samples that contain a field defined as repeated bytes, as that is represented as a sequence
            of a sequence of octets in the data model.
             <br/>
             <b>Solution: Support for editing certain multidimensional collections has been added, specifically editing of two dimensional
             unbounded sequences of primitives, such that the Google Protocol Buffer type &quot;repeated bytes&quot; is able to be edited. Other
             more complex types such as N-dimensional bounded/unbounded sequences or arrays of primitives or of structs are still not editable.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8804 / 00016720
          </td>
          <td>
            <b>Late joining DataReader receives 1 sample per DataReaderInstance per unfinished transaction at maximum</b><br/>
            <i>
            When a transaction on a transient topic contains more than 1 samples per instance, and the transaction is not yet
            finished at the time a late joining Volatile Reader requests historical data, then the reader will receive only the
            first sample for each ReaderInstance in that transaction, regardless of the Reader's history depth.
             <br/>
             <b>Solution:The Volatile Reader will now receive all samples of the unfinished transaction.
             </b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.6.3p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6961
          </td>
          <td>
            <b>Cause of an unregister not correctly tracked when a DDSI lease expires</b><br/>
            <i>
            There are two reasons why an "unregister" of an instance can take
            place: the first, most common one, is that a writer explicitly calls
            unregister_instance, or that it automatically happens when the
            writer is deleted; the second one is when a remote writer suddenly
            disappears and the other nodes have to synthesize one. The two are
            indistinguishable to applications, but the way the must be treated
            for transient data following a reconnect is different. DDSI did not
            properly distinguish between the two cases.
             <br/>
             <b>Solution: DDSI now correctly tags the "unregister" events.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8563
          </td>
          <td>
            <b>OS_INFO payloads from Node Info samples contain invalid XML
            character on Windows 10 64bit, Java 8</b><br/>
            <i>
            When publishing OS_INFO information using NodeMonitor on Windows 10
            in conjunction with Java 8, a form feed XML character is supplied by
            the underlying Sigar library, which cannot be processed by Vortex
            OpenSplice and throws the following exception:[error] o.o.a.c.t.xml
            - SAXException: Character reference "&#12" is an invalid XML
            character.
             <br/>
             <b>Solution:The NodeMonitor implementation now strips out the
             invalid XML form feed character for the OS_INFO description before
             publishing the data.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8585
          </td>
          <td>
            <b>Display the Topic Type definition wherever a GPB evolution can
            be selected</b><br/>
            <i>
            Tuner does not support selecting a protobuf type evolution yet when
            writing/reading.
             <br/>
             <b>Solution: Tuner's support for Google protocol buffers has been
             updated with the ability to directly view the type definition of
             the selected type evolution when choosing a type evolution to
             read/write data as.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8666 / 16651
          </td>
          <td>
            <b>Reading with QueryConditions could yield samples that shouldn't
            be read based on the masks provided.</b><br/>
            <i>
            For instance, when an QueryCondition has the mask
            NOT_READ_SAMPLE_STATE, only samples with that state should be
            returned when reading or taking with that condition. However, the
            query implementation did not include testing the masks for trigger
            values. This means that when using a triggered QueryCondition with
            the NOT_READ_SAMPLE_STATE, you'd be able to read samples with the
            READ_SAMPLE_STATE. This was also applicable for the view state and
            instance state.
             <br/>
             <b>Solution: Add masks checks for triggered QueryConditions.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8691
          </td>
          <td>
            <b>DDSI not properly retransmitting end-of-transaction messages.</b><br/>
            <i>
            When using coherent updates, an end-of-transaction message is used
            to notify the subscribers that the set is now complete and may be
            committed. These messages may get lost and hence may need to be
            retransmitted, just like ordinary samples. The retransmit path of
            DDSI however failed to handle these correctly. In consequence, the
            subscribing side would automatically reconstruct an
            end-of-transaction message at a later stage, one that is sufficient
            for guaranteeing topic-level coherence, but not for group-level
            coherence.
             <br/>
             <b>Solution: DDSI has been updated to properly retransmit
             end-of-transaction messages when required.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8708
          </td>
          <td>
            <b>Tuner cannot acquire key list for protobuf topics whose type
            contains a oneof.</b><br/>
            <i>
            If the user creates a reader or writer frame for a topic whose type
            is a protobuf type containing a oneof declaration, the reader or
            writer frame will show a warning in the status bar that the key list
            could not be obtained. Consequently, the user data table would not
            have any highlighting for key or protobuf required fields.
             <br/>
             <b>Solution: The underlying API that acquires protobuf specific
             properties for user data field (such as the required flag or
             default values) did not properly account for the switch field,
             which is an invented field for the mapping of oneof to union for
             representation in existing tooling data models. The tooling
             protobuf API was fixed to account for this switch field.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8710 / 16663
          </td>
          <td>
            <b>Issue with java waitset during termination of domain.</b><br/>
            <i>
            When a java application is blocking on a waitset _wait() call, while
            at the same time OpenSplice is stopping, the application may run
            into an ArrayIndexOutOfBoundsException. All conditions in the
            waitset are supposed to trigger because they are detached from the
            domain, but the java binding does not properly resize the
            ConditionSeqHolder array supplied by the application, to contain all
            conditions.
             <br/>
             <b>Solution: To resolve the issue a check on the array size was
             added, resizing if required.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8724
          </td>
          <td>
            <b>A crash could occur when the system terminates while there are
            pending events to process.</b><br/>
            <i>
            Events are handled asynchronously. In the exceptional case where the
            system terminates while there are pending events, these events must
            be cleaned up. A bug could cause that an unmanaged piece of memory
            is accessed, which potentially causes a crash.
             <br/>
             <b>Solution: When the system terminates while there are pending
             events, these events are now being cleaned up correctly.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8728
          </td>
          <td>
            <b>Unknown object kind error log when using isocpp2 listener.</b><br/>
            <i>
            When using the isocpp2 API with listeners an Unknown object kind
            error log shows up when listening for the DATA_AVAILABLE status
            event.
             <br/>
             <b>Solution: The isocpp2 listener mechanism is fixed and the error
             will not show anymore when using DATA_AVAILABLE status.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8736
          </td>
          <td>
            <b>Registering the same type twice in c sharp raises bad parameter.</b><br/>
            <i>
            There was a code path in the C# API in which registering a type that
            was already known did not set the correct return value, causing a
            successful call to raise an exception.
             <br/>
             <b>Solution: Added handling of the result code and return OK when
             all went well.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8737
          </td>
          <td>
            <b>Lag during replay by Record and Replay service.</b><br/>
            <i>
            TWhen a reasonable load is placed on the RnR service, i.e. by
            replaying a storage with many samples recorded at a high frequency
            from different topics and/or writers, the service may not be able to
            keep up with the exact original 'recorded' timing and a sample is
            replayed with a delay. The delays of individual samples add up,
            resulting in a noticeable delay sooner or later, as the replay
            carries on.
             <br/>
             <b>Solution: The replay timing has been improved to compensate for
             delays on individual samples, catching up lost time so the replay
             in general doesn't lag behind.
             </b>
            </i>
          </td>
        </tr>
         <tr>
          <td>
            OSPL-8738
          </td>
          <td>
            <b>Lease handling in the DDSI service is sensitive to time jumps.</b><br/>
            <i>
            The DDSI service's handling of the leases of remote participants was
            still based on the wall clock time, and therefore a forward jump of
            the wall clock by more than a few seconds could cause lease expiry,
            and hence disconnections. Similarly, a backward jump of the wall
            clock time could delay lease expiry.
             <br/>
             <b>Solution: The lease handling is now based on a monotonic clock
             that counts time elapsed since an arbitrary reference in the past.
             </b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.3p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-7245
          </td>
          <td>
            <b>Enable highlighting of required fields in GPB user data.</b><br/>
            <i>
            Google Protocol Buffer samples in Tuner do not highlight so-called
            required attributes. This makes it difficult to see what needs
            to be edited before samples can be written.
             <br/>
             <b>Solution: The Vortex OpenSplice Tuner UI has been updated to
             distinguish between key, required and optional fields for Google
             Protocol Buffer samples. Required user data fields in sample tables
             are highlighted with the cyan colour in the same fashion as in
             Vortex OpenSplice Tester.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-7392<br/>OSPL-8450
          </td>
          <td>
            <b>The durability service does not properly align open coherent transactions.</b><br/>
            <i>
            As soon as a durability service joins an existing system it will
            retrieve all non-volatile historical data that is available within
            the system by requesting this data from an available durability
            service (the master). The master durability service should provide
            the historical data to the newly joined durability service (this is
            called alignment). Unfortunately, the master durability service did
            not align the non-volatile historical data that belongs to a
            coherent transactions that is not committed (finished) yet. In
            particular, the following problems were identified:
            <ul>
              <li>historical data that is not committed yet was not aligned,
              only committed data was aligned</li>
              <li>end-of-transactions markers that are used to decide about
              completeness of a transaction were not aligned. As a consequence
              alignment of open transactions fails, and the late joining
              durability service may end up in an inconsistent state.</li>
             </ul>
             <br/>
             <b>Solution: Data that is part of a non-finished coherent
             transaction is now being aligned, just as the end-of-transaction
             markers. This allows the late joining durability service to
             reconstruct the same state as the master, and consequently
             completeness of transactions is handled correctly in various
             disconnect scenarios.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8102
          </td>
          <td>
            <b>Minimal configured database size required</b><br/>
            <i>
             If a database size is configured below the minimal database size
             Opensplice will not start and stop with out of memory problems.
             <br/>
             <b>Solution: If the configured database size is below the minimal
             required size, it will be increased to the minimal required size
             and a warning trace will be logged. Minimal database size for
             32-bit builds is 2 MB and for 64-bit builds it is 5 MB. The default
             memory threshold size for free memory will be 10% of the configured
             database size if the configured database size is less than the
             default 10MB.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8490 / 16602
          </td>
          <td>
            <b>Compatibility issues with dcpssacsAssembly.dll</b><br/>
            <i>
            When working with the dcpssacsAssembly.dll provided with the C# API
            there can be compatibility issues with Visual Studio versions. This
            is because the dcpssacsAssembly.dll is created with the use of the
            .NET framework 2.0.
             <br/>
             <b>Solution: The dcpssacsAssembly.dll is now generated with the
             corresponding Visual Studio .Net Framework compiler.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8528
          </td>
          <td>
            <b>Closing a DDS entity containing closed entities
            intermittently results in an AlreadyClosedException</b><br/>
            <i>
            When concurrently calling close methods on entities, the Java5 API
            may throw an AlreadyClosedException in case one of the children of
            the offending Entity already has been closed.
             <br/>
             <b>Solution: The implementation of the various close methods has
             been modified to be able to deal with concurrent closing of
             multiple entities by the application.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8544
          </td>
          <td>
            <b>Ordered_access for GROUP scope does not handle lifespan expiry
            correctly.</b><br/>
            <i>
            When a sample's lifespan has been expired in a Reader that belongs
            to a Subscriber with GROUP Presentation scope, it is not removed
            from the Reader cache at the start of a coherent_access block.
            Because of the fact that during a coherent_access block reader
            caches will be locked for any type of modification, the
            get_datareaders() function of the Subscriber and the read/take
            functions on individual Readers can still access samples that should
            have been expired.
             <br/>
             <b>Solution: The start of a coherent_accesss block on the
             Subscriber will first test the contents of its Reader caches for
             expired samples. All samples that have been expired will be purged
             prior to locking their Reader caches.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8566 / 16634
          </td>
          <td>
            <b>Deployment Guide vs Configuration Guide</b><br/>
            <i>
            In the set of documents delivered in the pdf directory of each OSPL
            version appears the Deployment Guide. This document refers about 56
            times to &quot;the separate Vortex OpenSplice Configuration Guide&quot;.
            The references should point to section 12 of the deployment guide
            itself instead
             <br/>
             <b>Solution: The references in the document have been updated.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8576
          </td>
          <td>
            <b>TypeEvolution chooser should be hidden when the Google Protocol
            Buffers feature is disabled for the platform.</b><br/>
            <i>
            In Vortex OpenSplice Tuner, when choosing to read or write data from
            a GPB-defined topic, the user is prompted with a dialog window to
            choose which type evolution to view the data as. If the Vortex
            OpenSplice installation does not have the Google Protocol Buffer
            feature enabled, then Tuner should not prompt the user for type
            evolution choice, since it wouldn't affect anything.
             <br/>
             <b>Solution: If the Google Protocol Buffer feature for Vortex
             OpenSplice is not included in the installation, then the type
             evolution choice in Vortex OpenSplice Tuner will not appear.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8583
          </td>
          <td>
            <b>Possible memory leak when reader is removed with uncommitted
            transactions</b><br/>
            <i>
            When a reader was removed that still had unread complete
            transactions it was possible that the transaction leaked.
             <br/>
             <b>Solution: Updated the internal transaction administration so
             that when a reader is removed so are its transactions.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8596
          </td>
          <td>
            <b>Wrong dispose in protobuf example</b><br/>
            <i>
            In the protobuf example, the 'Person' protobuf type has 2 key
            fields : name and worksFor.name. The publisher code first writes a
            sample with name="Jane Doe" and worksFor.name="Acme Corporation".
            Then it is supposed to dispose this instance but in fact disposes
            another instance as it only sets the name field to "Jane Doe" but
            does not set the worksFor.name field.
             <br/>
             <b>Solution: The worksFor.name is now also set for the dispose.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8609
          </td>
          <td>
            <b>When coherent group access is used a resource claim regarding the
            resource limits of the group is performed twice.</b><br/>
            <i>
            When coherent access is used then the resource counters maintained
            in the group regarding the resource limits set on the group are
            decremented twice. This may cause samples loss because of the
            incorrect resource counters in the group.
             <br/>
             <b>Solution: The group resource claim on the resource limit is
             performed only once for each coherent transaction.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8618
          </td>
          <td>
            <b>When the durability client receives an event to request
            historical data while the subscriber is being destroyed, the
            durability client could crash when it tries to handle the event.</b><br/>
            <i>
            When the durability client receives an event to request historical
            data from a reader, the durability client will asynchronously
            determine the partitions for which data must be requested from the
            reader's subscriber. In case the subscriber has just been destroyed
            then the durability client tries to determine the partitions from a
            non-existent subscriber. This inevitably leads to crash.
             <br/>
             <b>Solution: Before determining the partitions from the reader's
             subscriber a check is added to see if the subscriber exists. If the
             subscriber exists it is made sure using refcounting and locking
             that the partitions can be obtained, even if another thread is in
             the process of destroying the subscriber.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8640
          </td>
          <td>
            <b>C++ RMI applications may crash on Windows when compiled with VS 2015</b><br/>
            <i>
            C++ RMI applications may crash on Windows platforms when compiled
            with the Visual Studio 2015 compiler. The problem is that
            constructors of classes call the default constructor of its virtual
            parent class explicitly. However, that default constructor is not
            implemented.
             <br/>
             <b>Solution: These explicit calls to default constructors
             have been removed.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8643
          </td>
          <td>
            <b>A durability service that cannot act as aligner for a namespace
            does not merge when its fellow aligner re-appears within the expiry
            time</b><br/>
            <i>
            A durability service can be configured as non-aligner (alignee) for
            a namespace, in which case it is dependent on a fellow durability
            service (master) to provide historical data for this namespace. When
            the alignee looses its master (e.g., due to a disconnect) there is
            no durability service to provide historical data for this namespace,
            and the namespaces of the aligner and aligner may diverge. When a
            reconnect appears the master suddenly becomes available again, and
            the alignee should trigger a merge action to resolve the potentially
            diverged state. This was not happening in case a master appears with
            the expiry time. Consequently, the alignee may end up in a diverged
            state.
             <br/>
             <b>Solution: When a master of an alignee disconnects from the
             system, the namespace state is cleared and its master is reset. As
             soon as an aligner appears a master conflict is detected and a
             merge will be triggered.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-372 / 16541<br/>
            TSTTOOL-379 / 16578
          </td>
          <td>
            <b>Python Scripting Engine should support reading or writing to
            non-default partitions</b><br/>
            <i>
            The python scripting engine was connecting to on the default
            partition (the partition with an empty name) for all readers and
            writers.
             <br/>
             <b>Solution: With this fix, the scripting engine connects by
             default using the partition name pattern '*', which is consistent
             with Tester's behaviour. In addition, it is now possible to
             explicitly create subscribers and publishers, and to explicitly
             provide the desired partition name/pattern.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-373 / 16541<br/>
            TSTTOOL-378 / 16578
          </td>
          <td>
            <b>Python Scripting Engine should support setting QoS settings on
            entities</b><br/>
            <i>
            The Python scripting engine did not allow specification of Quality
            of Service (QoS) policies when creating entities
             <br/>
             <b>Solution: all readers and writers were configured with default
             policies. With the fix, it is now possible to specify QoS policies
             on the following entities: readers, writers, publishers and
             subscribers.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-391
          </td>
          <td>
            <b>Python Scripting Engine should support WaitSets</b><br/>
            <i>
            The python scripting engine did not permit creation of wait sets.
             <br/>
             <b>Solution: With this fix, a WaitSet class has been implemented,
             and it is possible to add Read Conditions to the wait set. Note
             that the WaitSet implementation does not yet support Status
             conditions or Query conditions.
             </b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.3p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-8547 / 16628
          </td>
          <td>
            <b>Topic definitions from KV-persistent store are not announced to other nodes </b><br/>
            <i>
            Topic definitions from the kv-store are not announced to other nodes. If no application
            on that other node has already registered this type definition the situation can
            occur that the durability service will not reach the operational state.
             <br/>
             <b>Solution: The kv store will announce the topic definitions found in the persistent
             store to all other nodes
             </b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.6.3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-7457
          </td>
          <td>
            <b>Interoperable transient-local support.</b><br/>
            <i>
            Transient-local was handled as transient data, but in combination
            with running DDSI as network protocol, data sometimes got delivered
            multiple times.
             <br/>
             <b>Solution: In cases where DDSI is the network protocol,
             historical transient-local data is delivered by the DDSI2(E)
             service and no longer by the durability service.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-7515 / 15645
          </td>
          <td>
            <b>When the merge policy for a namespace that is responsible for
            builtin topics is different from IGNORE or MERGE, the system state
            can become inconsistent.</b><br/>
            <i>
            In the configuration a user can specify merge policies for
            namespaces. These merge policies specify how to resolve state
            diverge due to disconnections. If the merge policy that is specified
            for the namespace that is responsible for the builtin topics is
            different from IGNORE or MERGE then the internal state of the system
            can become inconsistent. This is undesirable and should be
            considered as an invalid configuration.
             <br/>
             <b>Solution: The durability service now terminates when the merge
             policy that is specified for the namespace that is responsible for
             the builtin topics is different from IGNORE or MERGE. Termination
             will only occur when the durability service is responsible for
             builtin topic alignment.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8044
          </td>
          <td>
            <b>Undesired timeout on wait_for_historical_data without durability</b><br/>
            <i>
             When durability is not enabled, a wait_for_historical_data call
             would still block during the timeout-period, even though no
             historical data will be delivered to a reader without a durability
             service.
             <br/>
             <b>Solution: A check was added to determine if durability is used.
             If not, wait_for_historical_data will return precondition_not_met
             instead of blocking and returning timeout.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8306 / 16511<br/>
            OSPL-8096 / 16183<br/>
            OSPL-8313 / 16513<br/>
            OSPL-8314 / 16514<br/>
            OSPL-8315 / 16512<br/>
          </td>
          <td>
            <b>The networking service may crash when a reconnect occurs under
            high load conditions.</b><br/>
            <i>
             When the networking service determines that another has not reacted
             in time and is removed from the reliable protocol and shortly
             thereafter the just removed node is reconnected it may occur that
             some variables related to this node are not correctly initialized
             to their default values.
             <br/>
             <b>Solution: The state variables that the networking service
             maintains for the participating nodes are set to their default
             values when the node is removed from the reliable protocol. This
             ensures that when a node is considered alive again that the node
             related information has the correct state.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8310</td>
          <td>
            <b>Statistics not enabled when not explicitly setting enabled
            attribute to true.</b><br/>
            <i>
             When not explicitly configuring the enabled attribute of the
             //OpenSplice/Domain/Statistics/Category element and setting it to
             true, the statistics for that category are not enabled.
             <br/>
             <b>Solution: When not configuring the enabled attribute it is now
             interpreted as having the true value set.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8311</td>
          <td>
            <b>ThrottleLimit configuration does not accept human-readable sizes as input.</b><br/>
            <i>
             The //OpenSplice/NetworkService/Channels/Channel/Sending/ThrottleLimit
             provides the lower bound for throttling and is really a size in bytes.
             Until now this value would not accept human-readable sizes.
             <br/>
             <b>Solution: Human-readable sizes can now be used as input for
             ThrottleLimit</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8391 / 16538<br/>OSPL-8462 / 16573<br/>OSPL-8467 / 16579</td>
          <td>
            <b>The reception of a dispose message may cause a crash when the
            corresponding instance has been removed.</b><br/>
            <i>
             When a dispose message is received the corresponding datareader
             instance is lookup using the corresponding group instance. However
             in the mean time the datareader instance may have been removed. The
             dispose message will than trigger the creation of a new datareader
             instance but at that time the key information present in the
             dispose message is not available which may cause access to an
             invalid pointer value.
             <br/>
             <b>Solution: When the datareader receives a dispose message for an
             instance that is removed the dispose message will be dropped
             because it will not result in any state change.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8439 / 16570</td>
          <td>
            <b>Include dependency for constant declarations missing in idlpp.</b><br/>
            <i>
             When declaring a constant in an IDL file whose type is taken from
             an included IDL file, idlpp forgets to take the include dependency
             into account. This means that the generated code will refer to a
             type from another generated file, but the first file will not
             include the second file, resulting in compilation errors.
             <br/>
             <b>Solution: The missing dependency has now been added to idlpp.
             The type of a constant declaration will now be properly included if
             located in another file.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8477 / 16597</td>
          <td>
            <b>Registering a protobuf-modelled type multiple times leaks memory</b><br/>
            <i>
             When registering a type, its meta-data gets stored as well to allow
             tools to generically create datawriters and datareaders without the
             need for pre-compiling the type information into the tools. When
             registering a protobuf-modelled type with the same
             DomainParticipant multiple times, the definition leaks away though
             due to a missing deallocation.
             <br/>
             <b>Solution: the missing deallocation of the duplicate type in the
             type registration algorithm has been added.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8480 / 16599
          </td>
          <td>
            <b>Ordered_access does not always suppress invalid samples correctly.</b><br/>
            <i>
            When using ordered_access when reading/taking data, some additional
            (invalid) samples could appear that you would normally not see when
            reading from a similar Reader that is not using ordered_access. This
            is caused by the fact that normal DataReaders suppress invalid
            samples when they can mask those invalid samples by valid samples
            for the same instance. However, in case of ordered_access, this
            suppression mechanism was not being applied.
             <br/>
             <b>Solution: The suppression mechanism for invalid samples is now
             also applied to DataReaders that use ordered_access.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8488 / 16604
          </td>
          <td>
            <b>Coherent access in combination with resource limits can results
            in not receiving sample updates.</b><br/>
            <i>
            When using coherent access in combination with resource limits and
            max samples per instance is set to 1 it could be that samples are
            not being received by the datareader.
             <br/>
             <b>Solution: The defect in the datareader mechanism is resolved and
             samples are being properly received.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8497 / 16613
          </td>
          <td>
            <b>ArrayIndexOutOfBoundsException can occur during termination when
            using waitsets in the Java API.</b><br/>
            <i>
            When using the Java API in combination with waitsets an
            ArrayIndexOutOfBoundsException can occur during termination of the
            middleware.
             <br/>
             <b>Solution: The defect in the waitset mechanism is resolved and
             the exception will not occur anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-8508
          </td>
          <td>
            <b>DCPSPublication samples maintained longer than necessary.</b><br/>
            <i>
            In some cases, samples for the DCPSPublication topic were maintained
            longer than necessary by spliced. Even though eventually spliced
            would reclaim the memory, this appeared as if there was a memory
            leak.
             <br/>
             <b>Solution: The processing of the samples by spliced has been
             changed, so that cleanup happens as soon as possible instead of
             deferred.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-369 / 16350
          </td>
          <td>
            <b>Tester in Jython does not support float.Nan values in check method</b><br/>
            <i>
            In the provided Python Scripting Engine example, a Topic's float
            member with a NaN value could not be checked and the check function
            would return false.
             <br/>
             <b>Solution: The _checkPyobj(pyobj, checkValues, logFailures =
             False) function in
             $OSPL_HOME/tools/scripting/examples/tester_compat.py has been
             modified to handle this case.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-376 / 16517
          </td>
          <td>
            <b>Install Jython for ospltest scripts.</b><br/>
            <i>
            When installing Jython on Windows platforms following the
            instructions in the Tester User Guide, users would be unable to
            install the library due to spaces in the OSPL_HOME environment
            variable.
             <br/>
             <b>Solution: Updated the documentation to have quotes around the
             environment variables that could have spaces on Windows.</b>
            </i>
          </td>
        </tr>
       </table>
    <h2>6.6.2p4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-8139</td>
          <td>
            <b>Durability creates defaultNamespace when configuration is invalid.</b><br/>
            <i>
             The durability service can silently ignore an invalid
             configuration (i.e. no namespaces and/or policies are included, or
             are not consistent). In that case the service applies the default
             configuration, i.e. a defaultNamespace with initial alignment and
             alignee policy. Since namespaces are also exchanged with other
             durability services in a domain, the resulting behaviour of the
             mis-configured node, but also other nodes, may be far from what
             the user expects.
             <br/>
             <b>Solution: Instead of silently applying default configuration
             parameters, the service now refuses to start in case the
             configuration is missing or invalid, reporting relevant errors to
             the OpenSplice error log.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8221 / 16391</td>
          <td>
            <b>DataReader may crash when using ordered_access.</b><br/>
            <i>
             The administration of a DataReader that has a PresentationQos with
             ordered_access set to TRUE may become corrupted when all available
             samples are taken out of the DataReader. This corruption might in
             turn result in a crash of the publishing (when on the same
             federation as the corrupted reader) or subscribing application.
             <br/>
             <b>Solution: The ordering mechanism has been fixed so that taking
             the last sample out of a Reader with ordered_access set to TRUE can
             no longer corrupt the Reader's administration and therefore no
             longer crash your applications.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8234</td>
          <td>
            <b>Samples written without begin_coherent_changes broke ordering</b><br/>
            <i>
             Samples (including implicitly created samples like unregister on
             writer deletion) written with a coherent writer without calling
             begin_coherent_changes first could be delivered out of order and
             during a begin_access.
             <br/>
             <b>Solution: For a coherent writer every sample is now part of a
             transaction. A write without a previously called
             begin_coherent_changes now does this implicitly. This ensures in
             order delivery and samples are no longer delivered after calling
             begin_access.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8288 / 16504 <br/>
          OSPL-8289 / 16503</td>
          <td>
            <b>The use of the newly introduced detachAllDomain functionality
            occasionally resulted in processes blocking or crashing when
            detaching and terminating.</b><br/>
            <i>
             The problem was caused by two reasons:
             <ul><li>some internal participant threads that remained active
             where they should have stopped during the detachAllDomain call.</li>
            <li>when an application listener callback blocks in application code
            OpenSplice failed to detect that it was safe to terminate.</li></ul>
            As a result the detachAllDomain failed and depending on the
            parameter values it could either hang or fail. In case of a failure
            the detachAllDomain returned with OK and the application process
            continued to terminate which then caused a crash because the
            remaining thread where still accessing shared memory while being
            unmapped from process space.
             <br/>
             <b>Solution: The ordering mechanism has been fixed so that taking
             the last sample out of a Reader with ordered_access set to TRUE can
             no longer corrupt the Reader's administration and therefore no
             longer crash your applications.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8442<br/>OSPL-8435 / 16568<br/>OSPL-8427 / 16564</td>
          <td>
            <b>The method detach_all_domains may fail when a listener callback
            is blocking.</b><br/>
            <i>
             The listener callback is called from shared memory context which
             means that the thread that is executing the listener callback is
             registered as using shared memory resources. The detach_all_domain
             method will try to have all threads that are executing from shared
             memory context to either leave the shared memory context or to
             block which would allow the shared memory to be detached safely.
             Thus when the listener callback is blocking in the application
             context the detach_all_domain method will fail with a timeout
             because this thread is still listed as using shared memory
             resources.
             <br/>
             <b>Solution: When an application listener callback has to executed
             the thread that has to call this callback first releases the shared
             memory resources it is using and leaves the shared memory context
             before calling the callback.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8443</td>
          <td>
            <b>The function DDS_wait_for_historical_data_w_condition() does not
            handle the special value DDS_TIMESTAMP_INVALID correctly.</b><br/>
            <i>
             The function DDS_wait_for_historical_data_w_condition() allows an
             application to provide conditions on the historical data to
             request. In particular, timestamps to indicate the interval when
             data was produced can be specified. The special
             DDS_TIMESTAMP_INVALID can be used for both the lowerbound and
             upperbound of the interval and should be interpreted as don't
             care. Due to an error these special values where not handled
             correctly, causing these values to be interpreted as invalid
             parameters and causing the
             DDS_wait_for_historical_data_w_condition() to return with
             BAD_PARAMETER return code.
             <br/>
             <b>Solution: The error is fixed, so DDS_TIMESTAMP_INVALID is now
             considered a valid value. Note that this issue has currently only
             been fixed for the C language binding. Other language bindings
             will be fixed in a future release.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.2p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-8205 / 16386</td>
          <td>
            <b>Exception in thread ListenerEventThread causes NullPointerException.</b><br/>
            <i>
             When using the Java API with listeners a NullPointerException can
             occur when deleting an entity which has a listener attached.
             <br/>
             <b>Solution: The cause of the fault is still unclear but a
             workaround has been implemented to catch the exception so the
             listener will terminate correctly. When this happens a trace will
             also be added to the ospl-error.log</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8388</td>
          <td>
            <b>dbmsconnect example configuration fails to get open with configurator</b><br/>
            <i>
            The dbmsconnect example XML configuration contains a '&lt;&gt;' as
            part of an SQL filter expression, but this needs to be escaped in
            XML for the XML to be valid. Even though dbmsconnect is able to cope
            with this file, the configurator tool is not accepting this invalid
            XML.
             <br/>
             <b>Solution: The '&lt;&gt;' statement has been escaped in the XML
             file as '&amp;lt;&amp;gt;'.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.2p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6586</td>
          <td>
            <b>Coherent updates are not always delivered to DataReaders for a
            ContentFilteredTopic.</b><br/>
            <i>
            ContentFilteredTopic DataReaders filter out samples that do not
            match the filter. Samples that are filtered out purely on
            key-value do not even get offered to the DataReader as they are
            filtered out a bit earlier in the process. For coherent sets, the
            DataReader, even though filtering out the sample, still needs to
            receive the sample to determine completeness of a coherent set as
            it counts all samples and compares this count with the expected
            number provided by the publisher. Therefore not receiving all
            samples results in an incorrect count and the complete coherent set
            not being made available to the application.
             <br/>
             <b>Solution: For ContentFilteredTopic DataReaders, all samples
             are now delivered for counting purposes but immediately dropped
             afterwards so that no resources are consumed but completeness can
             be verified. Although samples are dropped, some additional
             administration is required to keep track of the data completeness
             and as a consequence some extra memory is kept during the time
             the coherent set is not completed yet.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7601</td>
          <td>
            <b>DDSI2 service lacks support for transient-local history settings
            of KEEP_LAST with depth &gt; 1.</b><br/>
            <i>
            The DDSI2 service maintains transient-local data in accordance with
            the DDSI specification, but only implemented support for KEEP_LAST
            with depth 1. For OpenSplice this is more-or-less a non-issue as the
            durability service handles the history correctly anyway, but when
            interoperating this could be a restriction.
             <br/>
             <b>Solution: The history maintained by DDSI now fully supports all
             history QoS settings.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7893</td>
          <td>
            <b>The durability service may select conflicting masters after a
            reconnect.</b><br/>
            <i>
            When a reconnect occurs the durability service has to determine a
            new master. When a reconnect occurs the master selection procedure
            will determine a new master in a number of rounds where information
            is exchanged between the durability fellows to reach conformity
            about the selected master for each namespace. However when the
            durability service is in the complete state the master selection
            procedure selects a master to early which causes that the durability
            fellows may select different masters causing the potential merge of
            historical data to fail.
             <br/>
             <b>Solution: When in the complete state and a reconnect occurs, the
             master selection now waits during each stage of the master
             selection until all information has been received from the other
             fellows about their master selection.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8114</td>
          <td>
            <b>When publishing a coherent set during durable data alignment,
            transactions may never become complete.</b><br/>
            <i>
            When publishing a transaction during alignment of durable data it is
            possible that a transaction never became complete as it could
            receive parts of the transaction in twofold, each received part of
            the transaction was counted and used to determine completeness. In
            this case the transaction never became complete as the expected
            count was lower then the actual count.
             <br/>
             <b>Solution: The transaction mechanism has been updated to handle
             duplicate messages. The internal group must be complete before the
             readers receive the EOT markers.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8126</td>
          <td>
            <b>When the durability service terminates while rejected samples
            still need to be delivered to the readers, then termination is
            stalled until the samples are delivered.</b><br/>
            <i>
            When the durability service receives alignment data it tries to
            deliver the data in the readers. When delivery is rejected (e.g.,
            because the reader has reached its resource limits) then the
            durability service will periodically retry to deliver the data. In
            case the durability service must terminate while periodically
            retrying to deliver rejected samples, then termination will
            only succeed after all samples have been delivered successfully.
            This negatively affects responsiveness of the durability service in
            case of a termination request. To improve responsiveness the
            durability service should not wait until all samples have been
            delivered when it is requested to terminate.
             <br/>
             <b>Solution: The durability service does not deliver rejected
             samples anymore when termination occurs.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8136</td>
          <td>
            <b>Possible missing of async RMI replies when server exits and starts again</b><br/>
            <i>
            A C++ RMI client may miss replies from a server when a server
            replies to an asynchronous request and exits, then it is re-started
            again to process other requests whereas the client remains active.
            The reply instance is then disposed and as the client takes the
            replies, it receives an invalid sample that disturbs the replies
            management and makes RMI miss the following valid replies.
             <br/>
             <b>Solution: Invalid samples for the RMI replies datareader have been
             disabled at the client side.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8152 / 16371</td>
          <td>
            <b>Crash in the shared memory monitor thread due stacksize
            limitation.</b><br/>
            <i>
            When using a datamodel with a lot of indirections and nesting and
            experiencing an application, the shared memory monitor crashes too,
            because it runs out of stack space during its attempt to clean up
            application resources.
             <br/>
             <b>Solution: It is now possible to configure the stack size for all
             splice daemon threads found in the configuration section
             //OpenSplice/Domain/Daemon. Each thread now has an additional
             element "StackSize" which can be set to the desired size. An
             example to set the shmMonitor stack to 1 Mb:
             &lt;shmMonitor&gt;&lt;StackSize&gt;1048576&lt;/StackSize&gt;&lt;/shmMonitor&gt;
             When this element is not specified the thread will use the default
             size of 512Kb.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8172</td>
          <td>
            <b>PublicationInfo for group coherent writer notification unsafe.</b><br/>
            <i>
            When the splice daemon receives a PublicationInfo message that
            describes a coherent writer, a notification is sent to the publisher
            in an unsafe way, which caused potential multithreaded manipulation
            of the internal cached writer administration in the coherency
            mechanism.
             <br/>
             <b>Solution: Made the notification threadsafe.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8185 / 16385</td>
          <td>
            <b>Potential crash during simultaneous creation and deletion of a
            shared DataReader/Subscriber</b><br/>
            <i>
            There is a race condition in the implementation of shared
            DataReaders/Subscribers, where if you simultaneously delete the last
            occurence of a shared Reader/Subscriber while at the same time
            creating a new reference to it, the system might end up crashing.
             <br/>
             <b>Solution: The race condition has been removed. It is now
             possible to safely remove the last occurence while at the same time
             creating a new reference to it.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8189</td>
          <td>
            <b>Group coherent reader can access subscriber after deletion.</b><br/>
            <i>
            After deletion of the subscriber a group coherent reader may still
            try to access the subscriber, which causes a crash.
             <br/>
             <b>Solution: The data reader has a back-reference to the subscriber
             , which is now removed in a threadsafe manner and checked
             before trying to access the subscriber.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8191</td>
          <td>
            <b>After calling begin_access, historical data could still be
            delivered to a group coherent reader.</b><br/>
            <i>
            Reader creation retrieves historical data with the begin_access
            (read)lock instead of with the access (write)lock. This can cause
            data to change after a begin access in a multithreaded environment.
             <br/>
             <b>Solution: Historical data is now retrieved within the access
             (write)lock.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8208</td>
          <td>
            <b>C# API Waitset.wait causes NullReferenceException.</b><br/>
            <i>
            When using the C# DDS API with waitsets a NullReferenceException may
            occur when using the Waitset.wait call.
             <br/>
             <b>Solution: The fault in the waitset handling for C# is fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8212 / 16390</td>
          <td>
            <b>RTI/TwinOaks interoperability issue with
            "autodispose_unregistered_instances" QoS</b><br/>
            <i>
            RTI and TwinOaks have a different interpretation of the
            "autodispose_unregister_instances" writer QoS, which could cause
            unexpected disposes of instances in OpenSplice when a RTI or
            TwinOaks writer disappears.
             <br/>
             <b>Solution: The DDSI2 service has been modified to distinguish
             between PrismTech and non-PrismTech peers when defaulting the QoS
             setting, eliminating these unexpected disposes.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8222</td>
          <td>
            <b>Multi-partition Publishers don't disconnect properly from all
            targeted partitions.</b><br/>
            <i>
            Publishers that are connected to multiple partitions can cause
            problems when they need to disconnect from one or more of their
            partitions (for example because the PublisherQos is changing one or
            more partitions or because the writer is deleted.) These problems
            manifest themselves by not properly sending unregister messages to
            all targeted partitions, which may cause the InstanceState not to go
            to NO_WRITERS when it should and which may cause v_registration
            objects to leak away in the shared memory.
             <br/>
             <b>Solution: Multi-partition Publishers now disconnect properly
             from all targeted partitions, which should result in the correct
             InstanceState for all partitions and which should remove the
             leaking of v_registration objects.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8224</td>
          <td>
            <b>Spinning waitset when an empty coherent set is received.</b><br/>
            <i>
            A data_available notification was sent to all readers part of a
            transaction even when it did not have data. A waitset with a
            following get_datareaders (which returned 0 readers) could then
            enter a spin loop when no read to reset the data_available flag was
            done.
             <br/>
             <b>Solution: data_available notifications are now only sent to
             readers which have data.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8233</td>
          <td>
            <b>Completing a pending group coherent transaction could cause a
            crash after reader deletion.</b><br/>
            <i>
            When a group coherent transaction becomes complete it is added to a
            pending list. This pending list is flushed to the readers upon call
            to begin_access. When a reader was deleted the begin_access still
            tried to flush the deleted readers part of transaction which could
            cause a crash.
             <br/>
             <b>Solution: Data for the removed reader is now removed from the
             pending list on deletion of that reader.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8267 / 16494</td>
          <td>
            <b>DDS protobuf compiler fails when including more than one
            depth-of-directory.</b><br/>
            <i>
            When proto files include others that are in different directories
            with more than one level difference, the DDS protobuf compiler
            plugin would fail to generate code as the output directory for the
            generated code was not created recursively, causing the directory
            creation to fail and therefore code generation would fail
            alltogether.
             <br/>
             <b>Solution: The code generator has been modified to create
             directories recursively in all situations.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8268</td>
          <td>
            <b>detach_all_domains operation doesn't return a failure when
            something went wrong.</b><br/>
            <i>
            The detach_all_domains function always return result OK. Even though
            no DDS call can be made afterwards, it may still prove useful to
            know if all was well.
             <br/>
             <b>Solution: The function now doesn't return OK anymore in case of
             a detectable failure. In case it took too long to wait for threads
             to leave shared memory, TIMEOUT will be returned.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8271</td>
          <td>
            <b>isocpp2 target for idlpp crashes on enum in outer scope.</b><br/>
            <i>
            When idlpp attempts to compile an IDL file that has an enum in the
            outer scope (i.e. not embedded in a module) to the isocpp2 target,
            it crashes with a segmentation violation.
             <br/>
             <b>Solution: The crash has been fixed in idlpp.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8302 / 16505</td>
          <td>
            <b>The detach_all_domain operation may cause a process to crash due
            to a race condition.</b><br/>
            <i>
            When detach_all_domains returns successfully it has detached the
            shared memory segments from the process. Before that, it will
            deny all threads access to the shared memory segments. However, a
            race condition exists, which may cause a thread still to be in a
            critical section after the unmapping the shared memory from the
            process. The aforementioned thread may try to access an object in
            shared memory, which causes a crash of the process.
             <br/>
             <b>Solution: The detach_all_domain operation now waits for all
             threads to have left this critical section before unmapping the
             shared memory segment(s) from the process.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.2p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7947-1</td>
          <td>
            <b>Transaction sample received twice, as historical data and as
            normal write, causing the transaction to never become complete</b><br/>
            <i>
            When reader was created it asynchronously connected and requested
            historical data making it possible to receive the same sample via
            historical data and the normal path, this sample was counted twice
            which caused the transaction to never become complete.
             <br/>
             <b>Solution: A reader now connects and requests historical data
             synchronously so that the sample is not received twice and the
             transaction now becomes complete.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7947-2</td>
          <td>
            <b>When discovering a transactionWriter implicitly is was possible
            that transactions with same writer never became complete</b><br/>
            <i>
            The group transaction mechanism has two ways of discovering writers
            one via builtin topics and an implicit discovery when data from that
            writer is received. In the latter situation it was possible that
            transaction which were depended on the same writer never became
            complete.
             <br/>
             <b>Solution: When a writer is implicitly discovered check all open
             transactions for dependency on the writer and mark the writer as
             discovered so that the transaction can become complete.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-6395 / 14540</td>
          <td>
            <b>Integration with Rhapsody 8.x</b><br/>
            <i>
            The name of the abstraction-layer header file ("os.h") was too
            generic and caused a collision with a file by the same name
            belonging to IBM Rhapsody. To prevent this, and other potential name
            clashes in the future, the file was renamed to "vortex_os.h"
             <br/>
             <b>Solution: In case the C language-binding is used, this header
             file is included in idlpp-generated code. This means code generated
             by a previous version of OpenSplice cannot be compiled using the
             new header file and should be regenerated first. Applications
             already compiled for a previous version are not affected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7562</td>
          <td>
            <b>Durability service does not handle default partition properly in
            name-space configurations</b><br/>
            <i>
            The OpenSplice durability services exchange name-space information
            before starting alignment of historical data. If name-space contents
            do not match, services refuse to align each other. The algorithm
            that does the comparison of the expressions has a problem dealing
            with the so-called default partition (this is an empty string). This
            leads to an incorrect interpretation of ".&lt;TOPIC&gt;" expressions
            when comparing name-space contents.
             <br/>
             <b>Solution: The flawed algorithm has been corrected to deal
             with the default partition properly too.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7601</td>
          <td>
            <b>DDSI2 lacks support for transient-local history settings of
            KEEP_LAST with depth &gt; 1</b><br/>
            <i>
             The DDSI2 service maintains transient-local data in accordance with
             the DDSI specification, but only implemented support for KEEP_LAST
             with depth 1. For OpenSplice this is more-or-less a non-issue as
             the durability service handles the history correctly anyway, but
             when interoperating this could be a restriction.
             <br/>
             <b>Solution: The history maintained by DDSI now fully supports all
             history QoS settings.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7687</td>
          <td>
            <b>bundled throughput-example doesn't work correctly on windows</b><br/>
            <i>
             The bundled throughput examples uses a flawed algorithm to determine
             how long an action takes.
             <br/>
             <b>Solution: The flawed algorithm has been corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8121 / 16218</td>
          <td>
            <b>Incorrect publication_handle in received SampleInfo</b><br/>
            <i>
             The publication_handle in the SampleInfo object of a recieved
             Sample was previously being set to the same value as the
             instance_handle, this was incorrect.
             <br/>
             <b>Solution: The SampleInfo::publication_handle() now returns the
             correct publication_handle as expected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8124</td>
          <td>
            <b>Improve group transaction flush mechanism.</b><br/>
            <i>
             The group transaction flush mechanism was inefficient as it used an
             unnecessary list
             <br/>
             <b>Solution:The unnecessary list has been removed</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8144</td>
          <td>
            <b>Java5 listener may trigger before entity is fully initialised.</b><br/>
            <i>
             Entities created throughout the Java5 API, which have a listener
             attached at creation time may cause problems as the listener may
             trigger before the entity is fully initialised.
             <br/>
             <b>Solution: Listener callbacks are now blocked until the entity is
             fully initialised</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8149</td>
          <td>
            <b>Server-part of client-side durability protocol must be enabled
            for all default shared memory configurations.</b><br/>
            <i>
             The durability service has a feature to react to requests from
             clients that have an interest in historical data but are not able
             (or willing) to run a durability service themselves. To have a
             durability service react to such requests a configuration option
             //Opensplice/DurabilityService/ClientDurability[@enabled] exists
             that must be explicitly set to TRUE. This is not very user-friendly
             and hampers a seemless out-of-the-box experience. For that reason
             the client durability feature will also be enabled now when the
             //Opensplice/DurabilityService/ClientDurability-element is present
             without the [@enabled]-attribute. Furthermore, for shared memory
             configurations that are part of the distribution this feature is
             enabled by default.
             <br/>
             <b>Solution: Durability services now will react to client requests
             when the //Opensplice/DurabilityService/ClientDurability-element is
             present without the [@enabled]-attribute. Furthermore, default
             shared memory configurations have been updated so that the client
             durability feature is enabled.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8151 / 16355 <br/>
              OSPL-8159 / 16376</td>
          <td>
            <b>JVM crash when creating a participant with an own set name for
            the thread creating the participant.</b><br/>
            <i>
             When using the Java API and the thread creating the participant has
             an own set name the JVM crashes.
             <br/>
             <b>Solution: The defect is solved and the JVM will not crash
             anymore on an own set thread name.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8157</td>
          <td>
            <b>Coherent updates do not get delivered when using a wildcard
            partition.</b><br/>
            <i>
             When a Publisher is publishing coherent updates in a wildcard
             partition (a partition that uses a wildcard like '*' or '?'), or a
             Subscriber is subscribing to a wildcard partition, then coherent
             updates are not correctly matched between Publisher and Subscriber
             and the contents of the coherent update will be lost.
             <br/>
             <b>Solution: Wildcard partition matching between Publisher and
             Subscribers has been improved: either the Publisher or the
             Subscriber can now use a wildcard partition without impacting the
             coherent update. However, if both Publisher and Subscriber use an
             unequal yet matching wildcard partition a mismatch may still take
             place. Fixing this scenario is left for a future update. The issue
             was already captured in the known issues list under OSPL-973</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8161 / 16377<br/>
          OSPL-8071 / 16171</td>
          <td>
            <b>NullPointerException in the ListenerThread when using Java</b><br/>
            <i>
             When using the Java API and a listener is used on an entity it can
             happen that the ListenerThread causes a NullPointerException when
             the entity is removed. The info logfile will then also show the
             following messages "timeout or spurious wake-up happened x times."
             <br/>
             <b>Solution: The defect is the listener mechanism is solved and the
             deletion of an entity will cause no more exceptions/info messages</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8169</td>
          <td>
            <b>In case of group coherent updates where more than one transaction
            is involved and simultaneously both become complete by actions on
            different readers a deadlock occurred.</b><br/>
            <i>
            A cross locking deadlock situation involving the reader lock and
            group lock caused the problem, when an action performed on a reader
            leads to a complete transaction it will lock the group to notify
            about the completeness, if the group itself then also become
            complete and notify all other readers, this last step takes the
            locks in reversed order and will lead to a deadlock when two
            transactions become complete by two simultaneous actions on
            different readers.
             <br/>
             <b>Solution: The group now releases the lock before notifying all
             other readers.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5028-1</td>
          <td>
            <b>Launcher - Choosing a file for OSPL_URI that is not an OpenSplice
            configuration file results in errors.</b><br/>
            <i>
             In the Vortex OpenSplice Launcher Settings, under the Environment
             tab, if one were to use the file chooser to set the URI or the
             Lice   nse field, and selected a file type that is not meant to go
             there, errors would be reported in the log and Launcher would
             otherwise not guard against any attempts to process the invalid
             files.
             <br/>
             <b>Solution: For the file chooser dialogs for the URI and the
             License fields, an extension filter has been added to only show
             only .xml and .lic files respectively.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5028-2</td>
          <td>
            <b>Launcher - NullPointerExceptions get printed to console every
            time a configuration is selected.</b><br/>
            <i>
             Whenever a OpenSplice config is selected from the configurations
             table, a NullPointerException and stacktrace would get printed to
             the console, due to a unchecked return from the examples table
             selection model.
             <br/>
             <b>Solution: The accessed to the examples selection model are
             properly checked for null returns.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7087 / 15090</td>
          <td>
            <b>Adding support to an unexpected stopping of the underlying DDS
            middleware to the java RMI library.</b><br/>
            <i>
             When the DDS middleware stops (normally or not) while a RMI server
             is waiting for or processing rmi requests, the
             RETCODE_ALREADY_DELETED error code should be handled properly.
             <br/>
             <b>Solution: The RMI implementation shutdowns the RMI server in
             case it receives a RETCODE_ALREADY_DELETED that signals a stopping
             of the DDS middleware. This will unblock the server application
             thread that was running the RMI runtime to wait for the requests.
             Note that the DDS middleware must not be stopped before the RMI
             application has stopped, whatever it is at the client or the server
             side. Even if the RMI library has been updated to handle an
             expected stopping of the DDS middleware, the user application
             cannot continue to work well.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7283 / 15405</td>
          <td>
            <b>GettingStartedGuide is missing information about how to install
            OpenSplice on UNIX ARM platform.</b><br/>
            <i>
             The procedure to install OpenSplice on a UNIX ARM platform is
             missing from the GettingStartedGuide.
             <br/>
             <b>Solution: The GettingStartedGuide is extended with a description
             of the OpenSplice installation on a UNIX ARM platform.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7317</td>
          <td>
            <b>Not possible to create reader for builtin topic DCPSType</b><br/>
            <i>
             Not possible to create reader for builtin topic DCPSType, because
             Typesupport for DCPSType was not mapped on the kernel
             representation as is required for builtin types, the typesupport
             was not registered and the DCPSType reader was not created for the
             Builtin Subscriber.
             <br/>
             <b>Solution: DCPSType typesupport is now generated correctly and
             registered and a DCPSType builtin reader is created.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7346</td>
          <td>
            <b>Truncation of max file size configuration values on 32-bit
            platforms</b><br/>
            <i>
             An issue in the configuration file processing caused truncation of
             the MaxFileSize RnR storage parameter on 32-bit platforms.
             <br/>
             <b>Solution:To resolve the issue, configuration processing was
             improved. In addition to resolving the truncation, it now supports
             floating point values and the following list of units: b, B (bytes)
             , KiB, kB, K, k (=1024 bytes), MiB, MB, M, m (=1024KiB), GiB, GB,
             G, g (=1024MiB), TiB, TB, T, t (=1024GiB)</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7618 / 15802</td>
          <td>
            <b>Readcondition and ContentFilterTopic could lead to a memory leak
            in isocpp2</b><br/>
            <i>
             When using a Readcondition or a ContentFilterTopic in isocpp2 it is
             possible that they are not properly cleaned up after removing them
             cauing a memory leak.
             <br/>
             <b>Solution: The leak has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7627 / 15740</td>
          <td>
            <b>Queries/Filters had problems with literals representing fields of
            type "unsigned long long" (uin64_t).</b><br/>
            <i>
             Queries/Filters were not able to parse literals of type
             "unsigned long long" whose value were between MAX_INT64 and
             MAX_UINT64 correctly.
             <br/>
             <b>Solution: The parser has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7686 / 15837</td>
          <td>
            <b>Deleting a domain (last participant from a domain) doesn't free
            all used resources correctly in single process mode.</b><br/>
            <i>
             Deleting a domain (last participant from a domain) doesn't free
            all used resources correctly in single process mode.
             <br/>
             <b>Solution: By setting the database size in single process mode
             the database will be allocated on heap and used by the memory
             manager in opensplice. Deleting the domain with this database
             results now in a correct cleanup off all used resources.
             Note: In previous versions the Size attribute for the Database
             element in the configuration had no meaning and was ignored. All
             available heap memory was available for the domain service. With
             the current implementation the database size is limited to the
             configured size and memory is managed by the ospl memory manager.
             Size 0 is default and will force the old unlimited
             behavior where the operating system memory manager is utilised.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7694 / 15847</td>
          <td>
            <b>Problem with string writing in the Tuner.</b><br/>
            <i>
             When using the Tuner and writing a string unbounded or bounded
             square brackets are always added to the input.
             <br/>
             <b>Solution: The writing mechanism is changed and no more square
             brackets are added.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7723 / 15852</td>
          <td>
            <b>Suppress default logs true provides null data to report plugin.</b><br/>
            <i>
             When using a report plugin with the option SuppressDefaultLogs set
             to TRUE and the element TypedReport set in stead of Report results
             in null data inside the specified report log.
             <br/>
             <b>Solution: The report plugin is adjusted so that in this scenario
             the reports are passed to the specific log file.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7743 / 15912</td>
          <td>
            <b>Possible missing of some async replies when their server exits.</b><br/>
            <i>
             The problem may occur when a server replies to an asynchronous
             request and exits, then it is re-started again to process other
             requests whereas the client remains active. The reply instance is
             then disposed and as the client takes the replies, it receives an
             invalid sample that disturbs the replies management and makes RMI
             miss the following valid replies.
             <br/>
             <b>Solution: Disabling the invalid samples for the replies
             datareader at the client side.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7762</td>
          <td>
            <b>DDSI declaring readers on slow machines "non-responsive".</b><br/>
            <i>
             DDSI2's flow-control was extremely sensitive to the configuration
             of the maximum allowed amount of unacknowledged data in a writer,
             the relative speeds of the machines and the networks and the socket
             receive buffers on the subscribing nodes.
             <br/>
             <b>Solution: DDSI2 now dynamically adapts the maximum amount of
             unacknowledged data, based on retransmit requests. The adaptation
             can be disabled for increased predictability if required.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7772</td>
          <td>
            <b>Missing samples after CATCHUP or REPLACE merge has occurred.</b><br/>
            <i>
             When nodes reconnect and a CATCHUP or REPLACE merge policy has been
             configured, then alignment takes place. It turns out that the
             alignment data contains the right amount of data, but after
             injection of the data in the system there are less samples than
             expected. This phenomenon was caused due to an incorrect
             administration of the last dispose time which prevented that data
             that has been produced before the last dispose time could be
             inserted. In case a foreign state conflict appears and there is no
             aligner, rediscovery of an aligner would not lead to a merge.
             <br/>
             <b>Solution: The last dispose time is not set when alignment for
             CATCHUP or REPLACE takes place, and rediscovery of aligners in case
             of foreign state conflicts is fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7777</td>
          <td>
            <b>Launcher should display the reason why tools cannot be
            started.</b><br/>
            <i>
             If Java is not installed on the host machine from which Launcher is
             started, the tools cannot be started even though the buttons are
             enabled.
             <br/>
             <b>Solution: In order to help troubleshoot incompatibilities
             between our tools and other Java implementations, Added another
             environment variable JAVA_HOME that would get picked up by Launcher
             from the user's environment (just like the other variables) and
             allow the user to specify/override their own JAVA_HOME path within
             the tool. Launcher will detected if a JRE and JDK are installed.
             When modifying the JAVA_HOME environment variable in Launcher or
             starting Launcher with a JAVA_HOME set to a non-officially
             supported Java implementation (OpenJDK, IBM Java, etc), a
             notification will come up indicating what Java implementation was
             picked up and that it is not officially supported for running the
             tools and building/running the examples. The tools and examples
             will remain enabled in this case. The tool buttons are disabled and
             a message is displayed that Java is not detected in JAVA_HOME. The
             buttons will re-enable once a valid Java install directory is
             specified in JAVA_HOME.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7814 / 15970</td>
          <td>
            <b>IllegalMonitorStateException when using the java API in
            combination with a listener</b><br/>
            <i>
             When using the Java API in combination with a listener an
             IllegalMonitorStateException could occur.
             <br/>
             <b>Solution: The cause of the exception is fixed in the listener
             handling.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7816 / 15972</td>
          <td>
            <b>Durability service doesn't notice disconnection.</b><br/>
            <i>
             When using durability in combination with RTNetworking it could
             happen that when the network connection is lost the durability
             service is not notified about this.
             <br/>
             <b>Solution: The defect in the disconnect mechanism is fixed and
             durability now gets the disconnect notification.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7823</td>
          <td>
            <b>As a user, I want to be able to preview all the environment
            variables.</b><br/>
            <i>
             Allow the user to preview the environment variables that are
             available to the user through Launcher.
             <br/>
             <b>Solution:  A new Preview Environment Variables dialog is
             available to the user through the Preview button in the settings
             environment tab. The new dialog allows the user to preview the
             configured environment variables and copy to the clipboard.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7839 / 15981</td>
          <td>
            <b>Generation error in C# backend of idlpp</b><br/>
            <i>
             When compiling an IDL file that specifies an array of
             enumerations, the C# backend of idlpp would generate a statement
             that contained a superfluous bracket, which caused a compilation
             error in the C# compiler.
             <br/>
             <b>Solution: The C# backend of idlpp has been fixed by removing the
             superfluous bracket.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7852 / 15991</td>
          <td>
            <b>Warning during compilation in isocpp2</b><br/>
            <i>
             When compiling an application that uses isocpp2 a warning can occur
             in State.hpp.
             <br/>
             <b>Solution: The warning has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7854 / 15990</td>
          <td>
            <b>Coherent Set transaction do not proper process dispose messages.</b><br/>
            <i>
             When a coherent set with a dispose message in it is used it can
             happen that the dispose is not proper handled causing late joiners
             to allign the disposed message with the status
             NOT_ALIVE_DISPOSE_INSTANCE_STATE where the message would be
             expected to be completly gone from the system.
             <br/>
             <b>Solution: The coherent set mechanism is updated to handle
             dispose messages correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7857 / 15988 <br/>OSPL-7697 / 15841</td>
          <td>
            <b>Memory could leak away when instances are recycled aggressively
            during an overflow of the networking queue.</b><br/>
            <i>
             When an instance is unregistered but quickly brought back to life
             (by writing/disposing a sample with the same key), there is a small
             chance that in case of an overflow of the network queue some memory
             is leaking away.
             <br/>
             <b>Solution: The memory leak has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7865</td>
          <td>
            <b>Enable java debug symbols</b><br/>
            <i>
             To improve readability of exception stack-traces and help with
             debugging java applications, symbols need to be included in the
             jars.
             <br/>
             <b>Solution: symbols are now included in all jar files. As a
             result, the average size of jar files has increased ~25%. It is
             possible to rebuild the Corba-Java language-binding jar using the
             custom-lib build process. To create a jar without symbols please
             change the custom-lib build script, replacing -g by -g:none.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7876</td>
          <td>
            <b>RT networking crash when using scoped discovery</b><br/>
            <i>
             RT networking configurations relying on roles and scopes to
             restrict discovery of nodes could crash on an invalid memory
             reference upon discovering a new node.
             <br/>
             <b>Solution: The issue has been corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7877</td>
          <td>
            <b>Durability merge policies can cause data loss</b><br/>
            <i>
             For some merge policies durability needs to dispose all (or a
             particular subset) of the instances in a group, but it must not use
             the internal disposeAll functionality, as that implements the
             "dispose-all" operation that has an effect into the future.
             Because of that effect, old data could be lost following a
             CATCHUP/REPLACE/DELETE merge operation, including the data being
             aligned.
             <br/>
             <b>Solution: The algorithm has been modified to no longer rely
             on this internal disposeAll functionality.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7889</td>
          <td>
            <b>idlpp may crash when generating for the isocpp2 target</b><br/>
            <i>
             idlpp crashes for the isocpp2 target when specifying an IDL enum
             in the outer scope (i.e. not in a module).
             <br/>
             <b>Solution: idlpp has been fixed to correctly handle enums in the
             outer scope.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7890</td>
          <td>
            <b>Group coherence: Avoid the partial alignment of historical group
            transactions</b><br/>
            <i>
             A late joining Subscriber for group transactions may receive a
             group transaction partly as historical data and partly as incoming
             messages, this currently leads to invalid delivery of partially
             complete group transactions.
             <br/>
             <b>Solution: Partly aligned group transactions now trigger full
             group alignment and remove all related transaction administration.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7895</td>
          <td>
            <b>Tooling entities are not cleaned up after disconnecting</b><br/>
            <i>
             Entities created by tools are no longer deleted when disconnecting
             the tool due to an incorrect reference count of entities in the
             C&M API. As a result entities remained available in the federation
             the tools ever connected to until the federation stops.
             <br/>
             <b>Solution: Corrected the reference count for entities in the
             C&M API.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7924</td>
          <td>
            <b>Add java5 and isoc++v2 DCPS examples to Launcher</b><br/>
            <i>
             The titular DCPS examples were added to Vortex OpenSplice, but the
             Launcher tool currently does not have the capability to detect them
             and add them to the list of available language options.
             <br/>
             <b>Solution: The Launcher tool has been updated to recognize the
             java5 and isocpp2 DCPS examples in a Vortex OpenSplice
             installation, and is able to execute build and run tasks for them.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7931</td>
          <td>
            <b>DDSI2 possible crash when lease expiry coincides with
            termination</b><br/>
            <i>
             During termination, DDSI2 would delete all proxy entities
             explicitly, without accounting for the possibility that leases
             could still expire. This could lead to freeing a proxy participant
             prematurely, resulting in a use-after-free when deleting the one
             remaining endpoint.
             <br/>
             <b>Solution: This issue has been solved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7932</td>
          <td>
            <b>Group coherence: detect missing EOT message and discard
            associated pending transaction.</b><br/>
            <i>
             As soon as a Subscriber receives a EOT message of a (group)
             transaction it knows for which writers it will receive EOTs,
             whenever an EOT or any data for other writer-transaction are
             missing but newer data for those writers is received then the
             Subscriber can conclude that the missing EOT and/or data is lost
             forever and subsequently discard the whole group transaction.
             <br/>
             <b>Solution: Received messages of group coherent updates that will
             not become complete are now discarded.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7933</td>
          <td>
            <b>Group coherence: Subscriber-side re-evaluation of completed
            transactions to assure data meets latest user expectation.</b><br/>
            <i>
             An initializing Subscriber can receive transactions that match the
             Subscribers readers before all readers are created, meaning that
             data belonging to the reader to be created is not considered and
             possibly not delivered.
             <br/>
             <b>Solution: Completeness evaluation of ongoing group transactions
             now consider creation of additional DataReaders that may affect
             completeness.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7938</td>
          <td>
            <b>In situations where the builtin topics do not have to be aligned,
            they where still being aligned in case the AutoBuiltinTopics
            namespace is generated</b><br/>
            <i>
             When no namespace for the builtin topics is configured, a namespace
             called 'AutoBuiltinTopics' is created automatically for the builtin
             topics. When no builtin topic aligned is needed (e.g., when DDSI
             takes care of the builtin topics) it could still happen that the
             builtin topics were being aligned, even though they shouldn't.
             <br/>
             <b>Solution: Alignment of builtin topics is prevented for the
             AutoBuiltinTopics namespace in case no alignment is needed for
             them.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7941</td>
          <td>
            <b>Resource leak in application after closing domain in shared
            memory mode.</b><br/>
            <i>
             If a domain is closed (last participant in an application for that
             domain is deleted) not all used resources were freed correctly in
             shared memory mode.
             <br/>
             <b>Solution: Used resources are freed now for the closed domain.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7946</td>
          <td>
            <b>Group coherence: ddsi crash during connection changed</b><br/>
            <i>
             A crash may occur in the DDSI2 service when a connection change
             happens during a coherent update by an application.
             <br/>
             <b>Solution: Crash during connection change has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7947</td>
          <td>
            <b>Group coherence: complete transactions sometimes do not get delivered</b><br/>
            <i>
             Depending on timing, transactions, although complete, would not
             get delivered to application readers.
             <br/>
             <b>Solution: Several bugs concerning race-conditions have been
             solved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7948 / 16142</td>
          <td>
            <b>Limitation on supported network interfaces.</b><br/>
            <i>
             When using DDSi with more than 32 network interfaces it is possible
             that the requested network interface is not found and DDSi will
             report an error.
             <br/>
             <b>Solution: The number of supported network interfaces is
             increased to 128.</b>
            </i>
          </td>
        </tr>
         <tr>
          <td>OSPL-7973 / 16144</td>
          <td>
            <b>Java 5 QosProvider error reporting improvement.</b><br/>
            <i>
             When using the QosProvider in Java 5 all errors with it resulted in
             a null pointer exception with no proper error message.
             <br/>
             <b>Solution: The error reporting in Java 5 for the QosProvider has
             been improved. Each error now results in the proper java exception
             with a use full error message.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7974 / 16145</td>
          <td>
            <b>Waitset events potentially leak away when received after the
            Waitset's timeout value.</b><br/>
            <i>
             If a WaitSet times out, and new events arrive before the unblocked
             Waitset thread gets the time to execute and report the timeout, the
             new events will be lost and their memory will leak away. The
             chances of this happening increase when a Waitset frequently times
             out while its thread has to compete for CPU time with one or more
             other threads that are generating events for that same Waitset.
             <br/>
             <b>Solution: A thread that is unblocked by a Waitset because of a
             timeout, will first check for pending events prior to reporting the
             timeout. If pending events exist, each event is processed
             accordingly and the timeout is is not reported. If no pending
             events exist, the waitset will report the fact that it timed out.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7978</td>
          <td>
            <b>DDSI connection fail-over when using Vortex Cloud routing
            service.</b><br/>
            <i>
             When DDSI is relying on the Vortex Cloud services, it should switch
             from one routing service instance to another in case of network or
             routing service failure. The Vortex Cloud discovery service
             provides its clients with new addresses to use in such situations,
             but the DDSI2 service would not actually switch to the new
             addresses.
             <br/>
             <b>Solution: The DDSI2 has been modified to always use the most
             recent addressing information provided by the discovery service.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8004</td>
          <td>
            <b>Shared memory leak in set_qos operations.</b><br/>
            <i>
             The OpenSplice kernel failed to release memory allocated in the
             process of changing the QoS of an entity, resulting in a memory
             leak each time a set_qos operation was used.
             <br/>
             <b>Solution: The memory is now freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8015</td>
          <td>
            <b>Tuner export data throws exception.</b><br/>
            <i>
             When attempting to export data using the Tuner, an exception is
             raised. This is caused by a change in the UI, where the QoS-ses to
             use are presented in a different manner.
             <br/>
             <b>Solution: Updated internal algorithm to cope with the change in
             the UI to obtain QoS settings.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8021-1</td>
          <td>
            <b>DDSI2 crash in nn_address_to_loc when accepting TCP connection.</b><br/>
            <i>
             The DDSI2 service could crash on accepting a TCP connection if
             requesting the peer address after accepting the connection failed,
             typically caused by the connection already having been closed.
             <br/>
             <b>Solution: DDSI now handles this error condition correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8021-2</td>
          <td>
            <b>Networking Bridge not enabling forwarding for a topic until new entities created/destroyed.</b><br/>
            <i>
             The networking bridge processes the built-in topics describing the
             existing entities to determine what to forward. There existed cases
             in which the networking bridge would have been able to enable
             forwarding of data but instead would pause its discovery process
             until the arrival of new built-in topics.
             <br/>
             <b>Solution: The networking bridge now always fully processes the
             available built-in topic samples before waiting for new ones to
             arrive.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8021-3</td>
          <td>
            <b>DDSI2 spamming the log with "malformed packet" warnings when
            interoperating with RTI Connext.</b><br/>
            <i>
             RTI Connext sends many packets that are not well-formed RTPS
             packets, and these lead to "malformed packet" warnings from DDSI2.
             It appears that recent changes to RTI Connext have caused a
             significant increase in the number of warnings, leading to huge
             log files with little value to users.
             <br/>
             <b>Solution: DDSI2 now no longer logs the "malformed packet"
             warnings for commonly encountered packets, except when run in
             "pedantic" mode.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8021-4</td>
          <td>
            <b>Phantom entities in OpenSplice with DDSI2 and the Networking
            Bridge when interoperating with other vendors' products</b><br/>
            <i>
             OpenSplice internally operates using globally unique identifiers
             that antedate the DDSI specification, and hence DDSI2 translates
             between the identifiers as used in the DDSI specification and those
             used in OpenSplice. In combination with the Networking Bridge, the
             interaction could lead to the creation of "phantom" readers and
             writers as a consequence of an incomplete filter on the translated
             identifiers.
             <br/>
             <b>Solution: The filtering now accounts for interoperating with
             other products.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8021-5</td>
          <td>
            <b>Networking Bridge discovery hang on topics used by other vendors'
            implementations</b><br/>
            <i>
             The Networking Bridge requires complete topic definitions to be
             available, but the DDSI specification does not define interoperable
             type definitions. Therefore, the bridge can encounter readers and
             writers for topics that do not exist in OpenSplice.
             <br/>
             <b>Solution: The networking bridge now skips endpoints that use
             undefined topics; if and when a topic becomes available, the
             endpoints are re-evaluated.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8029</td>
          <td>
            <b>When a client requests historical data using client-durability
            that matches multiple partition/topic combinations, multiple
            responses are generated.</b><br/>
            <i>
             One of the ways to obtain is historical data is using the
             client-durability feature. When a client requests data that matches
             multiple partition/topic combinations the server responded with
             multiple data sets (one for each partition/topic combination). The
             intended behavior is that a single response is generated that
             contains the aggregated data from all requested partition/topic
             combinations.
             <br/>
             <b>Solution: The problem is fixed and now a single response is
             generated that contains all requested data.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8032</td>
          <td>
            <b>Mmstat can report more memory available than configured</b><br/>
            <i>
             Mmstat determines the amount of available memory through a fairly
             involved calculation, in which an mistake was introduced, causing
             it to potentially report more available memory than was configured.
             <br/>
             <b>Solution: The calculation has been corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8034</td>
          <td>
            <b>DDSI memory leak caused by a race condition between discovery and
            termination</b><br/>
            <i>
             DDSI discovery runs in a separate thread that takes its input from
             the network. During termination, the network input was stopped, but
             the discovery thread could still be processing a participant
             discovery message. In this case, deleting all proxy participants
             could occur too soon, leaking a proxy participant.
             <br/>
             <b>Solution: Processing DDSI discovery data is now forced to
             complete in time.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8055</td>
          <td>
            <b>Group coherence: transaction does not become complete</b><br/>
            <i>
             In some scenarios transactions would not become complete and
             therefore not delivered due to an incorrect algorithm in the
             matching of writers and readers.
             <br/>
             <b>Solution: Several scenarios that resulted in incorrect dropping
             group coherent updates are now solved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8058</td>
          <td>
            <b>Group coherence: memory leakage</b><br/>
            <i>
             The introduction of the group coherency feature introduces
             memory leaks in some scenarios, also in some situations
             where group coherence is not even used by any application.
             <br/>
             <b>Solution: Several memory leaks concerning group coherent updates
             have been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8072</td>
          <td>
            <b>Maximum Domain ID value (230) is not enforced in the API or
            during startup of the domain.</b><br/>
            <i>Maximum Domain ID value (230) is not enforced in the API or
            during startup of the domain
             <br/>
             <b>Solution: The domain will refuse to start when the domain ID in the
             configuration file is out or range (larger then 230). Trying to
             pass an invalid domain ID in a function call will result in an
             error in the info log and the call to fail
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8091 / 16180</td>
          <td>
            <b>Coherent Set transaction leaks v_transactionPublisher object</b><br/>
            <i>
             Each time when a coherent set publisher is created a
             v_transactionPublisher object will leak.
             <br/>
             <b>Solution: The coherent set mechanism is updated and the object
             is now properly freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8095 / 16182</td>
          <td>
            <b>Durability crash when inserting out of order disposed messages</b><br/>
            <i>
             When durability is used to insert out of order disposed messages
             the service could crash.
             <br/>
             <b>Solution: The defect is solved and durability will not crash
             anymore on out of order disposed messages.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-8123</td>
          <td>
            <b>The persistent store retaining the wrong samples</b><br/>
            <i>
             When committing a transaction on a KEEP_LAST history where the
             transaction contained more samples for a single instance than the
             depth of the history, the (KV) persistent store would not always
             retain the N latest samples.
             <br/>
             <b>Solution: The persistent store has been fixed to retain the
             latest samples also in this case.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>TSTTOOL-265 / 15462</td>
          <td>
            <b>Implement built in script variables for current scenario/macro
            filename and path.</b><br/>
            <i>
            The Tester scripting engine has facilities for calling on built in
            variables that can be referenced from any script execution. Two new
            variables that hold the value for the currently executing script
            file name and and the file path are needed.
             <br/>
             <b>Solution:The new built in variables have been added to the
             Tester scripting engine as variable names "script_file" and
             "script_path". See Tester user guide section 6.1.2 for update.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>TSTTOOL-343</td>
          <td>
            <b>Tester's statistics browser can't display statistics
            information.</b><br/>
            <i>
            Navigating to the statistics tab and attempting to view statistics
            information for DDS entities does not currently work. The Tester log
            file reports that the entities are not available.
             <br/>
             <b>Solution: The CM objects that the statistics workers held to
             gather statistics from were being freed too early. The
             unintentional free has been fixed and the statistics view works
             again.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6050 / 14407</td>
          <td>
            <b>DDSI2 MaxMessageSize and FragmentSize are no longer considered
            Internal options</b><br/>
            <i>
             DDSI2 MaxMessageSize and FragmentSize are no longer considered
             Internal options and should therefore be moved from the Internal to
             the General section.
             <br/>
             <b>Solution: The DDSI2Service/Internal/MaxMessageSize and
             DDSI2Service/Internal/FragmentSize options have now been moved to
             DDSI2Service/General/MaxMessageSize and
             DDSI2Service/General/FragmentSize. The old setting is still
             supported as a deprecated setting and causes a warning when used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-6467</td>
          <td>
            <b>When a client sends a historical data request to the
            client-durability server, the server did not respect the timeout
            that was specified in the reques
            </b><br/>
            <i>
             When a client sends a historical data request to the
             client-durability server, the client can specify a timeout value to
             indicate the time that the server may take to answer the request.
             Up to now the server did not respect the timeout value and would
             always answer immediately. With this fix the server now respects
             the timeout value.
             <br/>
             <b>Solution: The server now queues requests and answers them based
             on their timeout value.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-6983</td>
          <td>
            <b>When a client sends a request for historical data to the
            client-durability server, the server must ensure that the client's
            historical data reader is discovered.
            </b><br/>
            <i>
             One way for a client to obtain historical data is by publishing a
             request for historical data, and waiting for the response from the
             client-durability server. In case the server uses ddsi as the
             networking service, ddsi first must match readers and writers
             before communication can take place. If the server sends a response
             to a request, but ddsi has not yet discovered the reader to deliver
             the response to, then ddsi will drop the request. To prevent this
             from happening the server must either ensure that the reader has
             been discovered before sending the response, or send back a error
             indicating that no reader was discovered in time.
             <br/>
             <b>Solution: The server know contains functionality to detect
             whether the reader of a client has been discovered.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7348</td>
          <td>
            <b>userClock is missing from the configurator.</b><br/>
            <i>
             userClockService is not a service but an option for a domain in
             the configuration. The tag userClockService is confusing.
             <br/>
             <b>Solution: The configuration tag userClockService is now changed
             to userClock. The old tag userClockService is still supported for
             backwards compatibility, but will result in a warning in the info
             log that a deprecated tag is used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7716 / 15848 <br/>OSPL-7708 / 15846</td>
          <td>
            <b>Sometimes a RETCODE_ERROR was reported during the processing of a
            dispose_all_data request.</b><br/>
            <i>
             When the durability service contained a sample with a newer
             timestamp than the timestamp for the dispose_all_data request, it
             would decide to exclude the newer sample from the dispose request.
             Its return status would clearly communicate that decision, but this
             was wrongfully interpreted as an unspecified error.
             <br/>
             <b>Solution: The interpretation of the return status of the
             durability service has been corrected, and will no longer consider
             the exclusion of samples newer than the dispose_all_data request as
             an unspecified error.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7733</td>
          <td>
            <b>Premature deletion of a writer may cause that DDSI drops data
            when client-durability is used.</b><br/>
            <i>
             When a client sends a historical data request to a server, the
             server is expected to deliver the response in the partition
             specified in the request. For that reason the server creates a
             writer, publishes the data, and destroys the writer again. If ddsi
             has not yet taken the data before the writer is destroyed then the
             data will not be delivered. To prevent this situation premature
             deletion of the writer must be prevented.
             <br/>
             <b>Solution: The writer is cached for some time before it is being
             deleted. This gives ddsi sufficient time to take the data.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7775</td>
          <td>
            <b>ISOCPPv2 union generation from idl can fail.</b><br/>
            <i>
             The generation of an IsoCpp2 union from an idl will fail if
             <br/>a) the union has char as switch type
             <br/>b) it has a default case and
             <br/>c) it is build on a platform on which char is unsigned by
             default and signed-char compiler flag is not used.
             <br/><br/>The result can be a wrongly initialized union class, an
             ncompilable generated header file or an idlpp crash. All these
             problems were caused by the fact that idlpp expected the minimum
             and maximum values of a char to be -128 and 127 respectively. This
             is untrue when the char is unsigned.
             <br/>
             <b>Solution: Maximum and minimum values of a char are now dependent
             on its default sign-ness.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7782</td>
          <td>
            <b>Memory leak using Java5 DDS API DataReader.Selector.</b><br/>
            <i>
             Whenever a DataState or Content filter was applied to a Selector,
             an underlying ReadCondition got created. The ReadCondition had
             native resources associated with it and would only be freed when
             the DataReader associated with the Selector got deleted. This could
             lead to a serious increase of memory usage and eventually result in
             an OutOfMemoryError in case new Selectors were allocated frequently
             i.c.w. setting a DataState or Content filter.
             <br/>
             <b>Solution: In case of just a DataState, no ReadCondition is
             allocated. This also improves performance of the Selector.
             Additionally, the finalizer of the Selector frees native resources
             associated with a ReadCondition now (only applicable if a Content
             filter is applied). Finally, the Selector implementation has been
             made thread-safe as well by returning a new Selector object every
             time a setter method is called.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7785 / 15959</td>
          <td>
            <b>JVM crash during deletion of data reader or data writer.</b><br/>
            <i>
             A mistake in the reference counting of basic types in the code
             constructing samples of the built-in topics for the user_data,
             group_data, topic_data and partition name settings could eventually
             cause the freeing of a basic type that is supposed to remain in
             existence during the operation of the domain. Use of this type in
             subsequent operations could then cause a crash. The issue could
             only occur when these QoS are set to a non-empty value.
             <br/>
             <b>Solution: The reference counting for these cases has been
             corrected. Note that the issue is not limited to Java and that the
             crash can also occur in situations other than deleting a data
             reader or writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7794</td>
          <td>
            <b>ISOCPP2 dds::core::Time::operator &gt; and &lt; are broken</b><br/>
            <i>
             The &gt; and &lt; operators on dds::core::Time in the ISOCPP2 API
             are not working properly due to an incorrect algorithm for
             comparing the nanoseconds part.
             <br/>
             <b>Solution: The internal algorithm has been fixed to make the
             operators work properly again.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7808</td>
          <td>
            <b>ISOCPPv2 domain_id function returned default_id</b><br/>
            <i>
             The isocpp2 'dds::domain::DomainParticipant::domain_id()' function
             returned 'org::opensplice::domain::default_id()' when a the
             DomainParticipant was created with default_id. The
             DomainParticipant delagate kept a copy of the domain_id which was
             used during creation iso requesting the actual domain_id from the
             underlying core.
             <br/>
             <b>Solution: The 'dds::domain::DomainParticipant::domain_id()'
             function now gets the actual domain_id from the underlying core.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7822</td>
          <td>
            <b>Possible crash due to double free in listener.</b><br/>
            <i>
             The listener has a thread that waits for events and dispatches
             them. After dispatching, the events are freed. Events are also
             freed before dispatching when the source entity was destroyed in
             the meantime. It was possible that these events of the destroyed
             entity were still be handled and thus freed a second time.
             <br/>
             <b>Solution: Fixed the wait for events loop.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.0p4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7761 / 15836</td>
          <td>
            <b>Illegal time is reported repeatedly when the native networking service is used.</b><br/>
            <i>
             On some Windows platforms the native networking service does not
             serialize the source timestamp of the messages correctly, which
             causes the receiver to read an incorrect timestamp and will report
             the illegal time error. This is caused by a code construction
             i.c.w. an optimalisation made by a compiler in the order of parsing
             the second and nanosecond part of a messaged. This causes the
             seconds and nanoseconds to be sent on the wire in the wrong order.
             <br/>
             <b>Solution: The code that serializes the source timestamp is
             changed to ensure that they are parsed in the correct order.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.0p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6481</td>
          <td>
            <b>Launcher can not load User defined Files.</b><br/>
            <i>
             When a user creates their own deployment xml file
             (e.g %OSPL_HOME%\etc\config\My_ospl.xml), Launcher can not load it
             and can therefore not apply it. Even if you stop the Launcher and
             you restart it, it does not see the new xml file.
             <br/>
             <b>Solution: Updates to the user-specified configurations field in
             the directory setting panel, to the OSPL_URI field in the
             environment settings panel will trigger a refresh of the
             configurations list in teh Configurations page. Added a new Refresh
             button on the Configurations page. If the Refresh button is
             pressed, then the configurations list is rebuilt using OSPL_URI,
             user specified configurations directory,
             OSPL_HOME\etc\config directory. Duplicates are removed if they
             exist in more than one of these locations.Notifications added to
             warn users if a configuration no longer exists when trying to edit
             it or set it as default configuration.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7664</td>
          <td>
            <b>Memory leakage when using group coherence with volatile topics.</b><br/>
            <i>
             Every group coherent transaction leaked memory as the EOT message,
             as opposed to other transaction message, created a new group owned
             transactionGroup which never became complete and was therefor never
             removed.
             <br/>
             <b>Solution: Prevent EOT messages from creating group owned
             transactionGroups for volatile topics.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7700 / OSPL-7714 / 15845</td>
          <td>
            <b>Group coherence data possibly wrong during discovery.</b><br/>
            <i>
             When group coherent data was written and not all writers had been
             discovered yet via builtinTopics it was possible data wrong data
             was flushed or data was never flushed as the mechanism for
             determining the completeness of a group was flawed.
             <br/>
             <b>Solution: The mechanism for calculating completeness has been
             reworked so correct completeness can be determined.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7728 / 15854</td>
          <td>
            <b>Memory leakage on Waitset time-out.</b><br/>
            <i>When monitoring shared memory using mmstat, it could be seen
            that v_proxy objects were leaking away every time a WaitSet timed
            out.
             <br/>
             <b>Solution: The leak has been solved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7737 / 15858</td>
          <td>
            <b>XML parser does not allow reference to DTD.</b><br/>
            <i>The new XML parser introduced in V6.6.0p1 did not allow references or
            attributes that started with a '!' so that references to the DTD
            like &lt;!DOCTYPE ...&gt; would result in a validation error.
             <br/>
             <b>Solution: Tag names and attribute names that start with '!' will
             no longer automatically result in validation errors.</b>
            </i>
          </td>
        </tr>
      </table>
      <h2>6.6.0p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5834</td>
          <td>
            <b>Host side binaries included in target RTS installers</b><br/>
            <i>
             Host side RLM binaries, rlm, rlmutil and pristmech were being
             included in target RTS installers unnecessarily.
             <br/>
             <b>Solution: RLM binaries, rlm, rlmutil and prismtech are no
             longer included in target RTS installers.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7082 / 14835</td>
          <td>
            <b>Tuner tool hangs when reading large sequences or arrays.</b><br/>
            <i>
             The tuner tool can take a a few minutes when reading samples that
             contain large sequences or arrays. It is not responsive during that
             time.
             <br/>
             <b>Solution: The performance of C&M (which the Tuner tool uses) is
             improved considerably by using StringBuilder instead of Strings
             where keys an values are concatenated and by adding happy paths
             to searches.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7641</td>
          <td>
            <b>Group coherence not working when used during discovery.</b><br/>
            <i>
             When a group coherent writer starts publishing data before/during
             creation of the group coherent reader it was possible that no group
             coherent data was received during the livecycle of the reader. The
             transaction mechanism had an invalid (too high) writer count,
             because the discovery based on PublicationInfo and EOT message both
             increased the count for the same writer, the count is used
             determine if a transaction is complete and can be delivered to the
             reader. Because the count was too high the transaction never became
             complete and thus the transaction was never delivered to the reader.
             <br/>
             <b>Solution: Updated the internal administration so that when a
             writer is discovered via PublicationInfo it removes it from a list
             so that the EOT message cannot add the same writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7646 / 15812</td>
          <td>
            <b>Issue with register_service in Java RMI with duplicate services.</b><br/>
            <i>
             The register_service call of the Java binding of RMI did not
             properly check for duplicate services. When a particular
             service-name is already registered, the call would return true
             without considering the instance-id and class parameters.
             <br/>
             <b>Solution: The behaviour was fixed by returning false when a
             service is registered with a name that already exists, but a
             different instance-id or class.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7661 / 15813</td>
          <td>
            <b>Unregistering of report plugin not done sufficiently.</b><br/>
            <i>
             Participant creation failed after 10 registered report plugins due
             to incomplete cleanup on unregistrations and a hard limit of 10
             plugins.
             <br/>
             <b>Solution: Limits on number of report plugins are removed and
             unregistering report plugin has been improved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7673 / 15816</td>
          <td>
            <b>Compiler warnings caused by idlpp-generated cpp code.</b><br/>
            <i>
             C++ code generated by idlpp leads to warnings when
             compiling the code.
             <br/>
             <b>Solution: Improvements to idlpp templates and code-generation
             (to remove unnecessary casts, among others) can result in compiler
             warnings regarding signedness of comparison operands. The
             signedness of length variables in generated code for
             sequence-of-sequences was changed to an unsigned type so the
             warning does not occur.</b>
            </i>
          </td>
        </tr>
      </table>
      <h2>6.6.0p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7610</td>
          <td>
            <b>Durability crash when fellow running pre-V6.6 version is present</b><br/>
            <i>
             The durability service contains a mechanism to determine compatibility with
             other durability services, which could be other versions with a different set of features.
             A flaw in this mechanism causes the durability service to crash when it
             receives a sample-request from a durability service with a pre-V6.6.0 version.
             <br/>
             <b>Solution: The mechanism was improved to be more robust</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7557</td>
          <td>
            <b>SOAP service not allowing connections in SP-mode</b><br/>
            <i>
             Connecting any tool to a SOAP service, that is
             running as part of a single-process DDS application, fails with
             the error report that a participant could be created. This is due
             to a change in the SOAP service where it passed on an empty
             domain URI internally to prevent an extra unnecessary
             parse of the configuration file.
              <br/>
              <b>Solution: The SOAP service now always passes on the full
              URI and domain id in all cases.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7564</td>
          <td>
            <b>Change default installdir/windows start menu for Vortex V2</b><br/>
            <i>
             For Vortex_v2 the directory/start menu structure must change. All
             Vortex products should now install into the same structure. The
             version number is the version of that product and has no leading
             "v". For OpenSplice the structure must be:
             PrismTech/Vortex_v2/Device/VortexOpenSplice/&lt;version&gt;
              <br/>
              <b>Solution: The new structure has been applied.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7565</td>
          <td>
            <b>Durability crash when more than 2 roles present</b><br/>
            <i>
             For each name space, durability maintains information about the
             various roles it has merged with. The way this set is represented
             could cause a crash when more than 2 roles were used in the system.
              <br/>
              <b>Solution: The representation has been fixed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7616</td>
          <td>
            <b>Interoperability problem with RTI Connext 5.2.0</b><br/>
            <i>
            DDSI2 is quite strict in its checking of the values it receives in
            the discovery messages, which can from time-to-time result in
            interoperability problems. With Connext 5.2.0, RTI appears to have
            appropriated part of OMG-reserved namespace for a new extension in
            the discovery data, DDSI2 flags it as invalid and discovery fails
            completely.
            <br/>
            <b>Solution: DDSI2 now accepts unrecognised values unless the
            "pedantic" mode for StandardsCoformance has been selected.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7630</td>
          <td>
            <b>Coherent transaction shared memory leakage</b><br/>
            <i>
            For every coherent transaction a
            kernelModuleI::v_transactionPublisher and child objects leaked in
            shared memory.
            <br/>
            <b>Solution: Memory is now freed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7631</td>
          <td>
            <b>Possible deadlock when using coherent transaction</b><br/>
            <i>
            When an end-of-transaction (EOT) message was the only message in the
            resend list the 'end_coherent_changes' function could deadlock. The
            resending of an EOT message did cause 'end_coherent_changes' to
            re-evaluate its conditions.
            <br/>
            <b>Solution: Resending EOT messages now causes
            'end_coherent_changes' to reevaluate it's conditions.</b>
            </i>
        </tr>
    </table>
    <h2>6.6.0</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
        <tr>
          <td>OSPL-139</td>
          <td>
            <b>Retention-period for purging unregistered instances is too long and non-configurable</b><br/>
            <i>
             Currently theres a fixed 5 seconds 'retention period' after which
             unregistered instances are actually deleted/memory-freed. The
             'artefact' that this retention period prevents is the unwanted
             resurrection of unregistered-instances in case of out-of-order
             reception of network-traffic. The current value of 5 seconds is so
             long that its not hard at all to run out of the default 10 MB
             shared-memory segment if you create/delete instances in a rapid
             pace.
              <br/>
              <b>Solution: A new option RetentionPeriod is added to the domain
              configuration (OpenSplice/Domain/RetentionPeriod). This option
              specifies how long the administration for unregistered instances
              is retained in both readers and the durability service before it
              is definitively removed. The default value is 500 ms</b>
            </i>
        </tr>
        <tr>
          <td>
            OSPL-5395 / 13742
          </td>
          <td>
            <b>The dispose_all_data() (resp. on_all_data_disposed()) method is not supported under ISOCPP</b><br/>
            <i>
              The dispose_all_data() (resp. on_all_data_disposed()) method is
              not supported in the ISOCPP, but must be supported in its
              successor ISOCPP2.
             <br/>
             <b>Solution: The ISOCPP2 API now supports this feature.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-6338</td>
          <td>
            <b>ISOCPP streams API may drops samples silently when flush of stream fails.</b><br/>
            <i>
             The flush operation of the isocpp streams API will perform a write
             operation of the underlying datawriter. The write operation on this
             datawriter may return a timeout because of expiry of the
             reliability.max_blocking_time associated with this datawriter,
             which is default set to 100 ms. For example this may occur because
             of network congestion. In that case the sample are silently dropped.
              <br/>
              <b>Solution: When the flush operation on the stream fails, because
              the underlying datawriter write operation returns a timeout then a
              timeout exception will be thrown. Also the append operation will
              throw a timeout exception when the append results in a flush
              operation that times out. The samples will not be dropped and will
              be sent when the application retries the flush operation.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6436 / 14446</td>
          <td>
            <b>Errors in generated Modeller code.</b><br/>
            <i>
              C++ code generated for IDL where the same module scope was
              repeatedly opened and closed would be ordered such that all of
              the code for each module would be grouped together. This could
              lead to the generated code being invalid due to datatypes being
              used before they are defined. IDL generated by the Modeller
              application could be sensitive to this issue.
              <br/>
              <b>Solution: C++ code is generated with a module scope structure
              corresponding to the IDL source.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6808</td>
          <td>
            <b>Java5 DDS API lacks support for proprietary DomainParticipant operations.</b><br/>
            <i>
             The Java 5 API was missing support for the proprietary
             delete_historical_data() and create_persistent_snapshot()
             operations on a DomainParticipant for the Java 5 DDS API.
              <br/>
              <b>Solution: The 2 missing operations have been added to the
              Java5 API.</b>
            </i>
        </tr>
        <tr>
          <td>
            OSPL-6943
          </td>
          <td>
            <b>1st and 3rd implementation of dds::domain::discover function potentially clash.s</b><br/>
            <i>
              When invoking the 1st implementation of dds::domain::discover with
              2 parameters (so time-out becomes default parameter), the function
              clashes with its 3rd overloaded implementation, which also has 2
              parameters and which is preferred by the compiler.
             <br/>
             <b>Solution: 3rd implementation of dds::domain::discover has been
             renamed to dds::domain::discover_all.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-6954 / 14900</td>
          <td>
            <b>The concurrent handling of a terminate signal and cleanup
            performed by the application may cause a crash.</b><br/>
            <i>
             When a termination signal is handled, which tries to detach the
             application entities from shared memory, and concurrently the
             application is performing a cleanup of the DDS entities a crash may
             occur when the signal handler tries to access memory that is
             already freed.
              <br/>
              <b>Solution: A reference count is added to the objects that can be
              freed concurrently when the application is detaching from shared
              memory.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7035 / 15067</td>
          <td>
            <b>NullPointerException when using Java RMI API during shutdown.</b><br/>
            <i>
             When RMI is used and a request is done during shutdown of the
             application it is possible that a NullPointerException can occur
             in the handleRequest call.
              <br/>
              <b>Solution: The handleRequest function is adjusted so the
              NullPointerException will not occur anymore.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7095</td>
          <td>
            <b>Reader snapshot in Tuner returned incomplete history and provided
            unreliable sample info.</b><br/>
            <i>
             The reader snapshot feature from the tuner did not present reliable
             sample info, in particular the sample, view and instance states
             could not be trusted. Moreover, the snapshot only showed a single
             sample per instance, rather than the full history in case of
             KEEP_ALL or KEEP_LAST n with n > 1.
              <br/>
              <b>Solution: These issues have been addressed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7101</td>
          <td>
            <b>Handling of leading/trailing white-space in configuration file.</b><br/>
            <i>
             In the OpenSplice XML configuration-file, leading and trailing
             whitespace in configuration directives was not handled
             consistently. The raw value was stored, leaving the possibility for
             different services to interpret it in different ways.
              <br/>
              <b>Solution: Whitespace is now trimmed from all configuration
              elements (not attributes) to get a consistent behavior, even
              though it doesn't strictly adhere to the XML specification, it is
              what most users expect.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7109</td>
          <td>
            <b>The default value of the durability StoreSleepTime is 2.0, it
            should be 0.0.</b><br/>
            <i>
             In case data needs to be persisted, the StoreSleepTime and
             StoreSessionTime control the rate at which data is stored. In
             particular, the StoreSleepTime is intended to prevent that the
             thread that is responsible for persisting data eats up too much
             CPU time. The default used to be 2.0 seconds. This may potentially
             cause unnecessary delays. Since in many use cases persisting data
             will not cause a resourcing problem, it makes more sense to use a
             default of 0.0 seconds instead 2.0 seconds. Only if it turns out
             that the persistency thread takes up too much CPU time a non-zero
             value should be used.
             <br/>
              <b>Solution: The default value for StoreSleepTime is set to
              0.0 seconds.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7130 / 15176</td>
          <td>
            <b>Problem with listener notification at startup in ISOCPP API</b><br/>
            <i>
             When creating an entity with a listener and events that should be
             notified to the listener occur immediately, they may not when
             using the ISOCPP DDS API.
             <br/>
              <b>Solution: The ISOCPP DDS API is now deprecated and has been
              replaced by the ISOCPPv2 DDS API that does not suffer from the
              problem. Users who suffer from the issue should migrate to the
              new ISOCPPv2 DDS API. Check out the
              <a href="./isocpp_migration.html">ISOCPP migration guide</a> to
              find out how.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7198 / 15368</td>
          <td>
            <b>Enum c_collKind elements are named too generic.</b><br/>
            <i>
             The elements of c_collKind are named quit generic (f.i. C_LIST).
             This can conflict with customers' variable naming or their 3rd
             party libraries.
              <br/>
              <b>Solution: The elements have been renamed by prefixing them
              with the OSPL_ prefix.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7202 / 15365</td>
          <td>
            <b>If a write with timestamp of a new sample is done after an
            unregister of the instance but with a timestamp before the
            unregister, this new sample is not received the reader</b><br/>
            <i>
             Due to the unregister the communication path between the writer and
             reader(s) was destroyed, causing the write of the new sample being
             discarded.
              <br/>
              <b>Solution: In case a write with timestamp is done after an
              unregister with a timestamp before the unregister, bypass the
              normal communication path and write directly to the reader(s).</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7210 / 15373</td>
          <td>
            <b>Defining multiple protobuf messages in a single proto file fails
            for isocpp2</b><br/>
            <i>
             For each protobuf message in a proto file that is annotated to be
             used in DDS, the proto compiler back-end generates the required
             underlying trait when using the isocpp2 API. In case of multiple
             'DDS-enabled' message structures, the generated traits end up in
             the same file, which is ok, but the surrounding ifdef's are the
             same for each traits. This leads to exclusion of all generated
             traits except the first, which on its turn leads to an invalid
             argument error when trying to use the trait at run-time.
              <br/>
              <b>Solution: In case of multiple traits the ifdefs are only done
              once and surrounds the complete set of traits.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7228 / 15377</td>
          <td>
            <b>Creating Topics using ISOCPP2 with non-topic types results in cryptic error reports</b><br/>
            <i>
              With the ISOCPP2 DDS API, templates and traits are used to deal
              with type-specific structures. This requires a pre-processing step
              that generates traits for data-structures that need to be
              published/subscribed in DDS. Whenever an attempt is made to
              create a Topic for a type without a trait, the code still compiles
              due to the fact a template exists. At run-time though, the
              creation fails due to the missing trait and this results in an
              exception being thrown (Note: the missing
              trait can also be caused by not including the correct header
              file in your application. The "&lt;type&gt;_DCPS.hpp" is the
              correct one to include). The fact that an exception is thrown is
              correct, but the corresponding error message is very cryptic
              and needs to be improved.
              <br/>
              <b>Solution: The error message has been modified to "Topic type
              without traits detected. This can happen by using a non-topic type
              or including the wrong header file."</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7234 / 15376 <br>
              OSPL-7336 / 15432</td>
          <td>
            <b>The durability service may crash when not all namespace from a
            fellow are received and another fellow disconnect.</b><br/>
            <i>
             The role of a fellow is set when all namespaces of that fellow are
             received. When another fellow is disconnected the namespace
             administration of all fellows is checked to see if the disconnected
             fellow was not an aligner. A crash occurs when the administration
             contains a fellow for which the role is not set.
             <br/>
              <b>Solution: When the first namespace of fellow is received the
              role of that fellow is also set.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7250</td>
          <td>
            <b>Unable to terminate StreamDataReader applications.</b><br/>
            <i>
             The get() method on a StreamDataReader can cause applications to
             block for a significant amount of time if a large timeout is
             supplied and no data is being delivered to the reader.
             <br/>
              <b>Solution:A new method named interrupt() has been added to the
              StreamDataReader. Calling this method causes any threads blocking
              on get() to unblock immediately and return control to the
              application. For an example of using this method in a termination
              handler, please see the Streams Throughput example.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7256</td>
          <td>
            <b>Waitsets on Java API (classic and Java 5) would sometimes not unblock when the middleware is shutdown.</b><br/>
            <i>
             Due to an error in handling the list of conditions, the check done
             to detach from a single domain might come up empty, causing the
             waitset to remain blocked. This was particularly likely on 64-bit
             platforms.
             <br/>
              <b>Solution: The list of conditions is now passed correctly,
              causing the unblock to properly work.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7263</td>
          <td>
            <b>Isocpp2 can generate garbled information in exceptions.</b><br/>
            <i>
              A few calls to isocpp2 would produce somewhat unclear reports
              and/or exceptions when they fail.
              <br/>
              <b>Solution: The creation of exceptions and report stacks has
              been improved.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7284 / 15404</td>
          <td>
            <b>Unexpected and incomplete native log report.</b><br/>
            <i>
             Some logs are written in the OSPL native report system in spite of
             the configuration SuppressDefaultLogs=True. Also, the information
             in the native log and report plugin are incomplete.
             <br/>
              <b>Solution: The tracing functionality is improved regarding
              checking of SuppressDefaultLogs and regarding traces copying
              when SuppressDefaultLogs=True.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7288 / 15399</td>
          <td>
            <b>String &lt;NULL&gt; given as value to report plugins</b><br/>
            <i>
             When the reporting functionality fails, it can generate
             "&lt;NULL&gt;" for certain information within the report. Reports
             are provided to report plugins by means of XML. This will clash,
             because the value "&lt;NULL&gt;" can now be seen as an XML tag.
             <br/>
              <b>Solution:Generate "NULL" instead of "&lt;NULL&gt;" in a report
              functionality error situation.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7293</td>
          <td>
            <b>Crash in RnR service during remove-replay command containing transformations</b><br/>
            <i>
             While processing a remove-replay command containing
             transformations, the RnR service could potentially crash due to
             memory corruption. This does not occur when replay is stopped by
             other means, i.e. by stopping a scenario or reaching end-of-storage.
             <br/>
              <b>Solution: The issue was caused by a double free and fixed by
              improving the transformation cleanup.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7303</td>
          <td>
            <b>The secure networking service cannot find the security element in the configuration file.</b><br/>
            <i>
             The secure networking service reads the security settings from the
             Security element in the configuration file. The XPath expression
             used to find the Security settings is incorrect. It tries to find
             the Security element under the NetworkService element instead of
             the SNetworkService element.
             <br/>
              <b>Solution: First try to find the Security element under the
              SNetworkService element. When the Security element cannot be found
              under the SNetworkService element then the Security element is
              searched under the NetworkService element in order to be backward
              compatilble, in that case a deprecated warning is logged.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7313 / 15414</td>
          <td>
            <b>DDSI2 ExternalNetworkAddress error in a multicast configuration.</b><br/>
            <i>
             The presence of the DDSI2 ExternalNetworkAddress option in a
             multicast configuration was deemed an error and DDSI2 would
             terminate during startup.
             <br/>
              <b>Solution: Reduce the presence of DDSI2 ExternalNetworkAddress
              in a multicast configuration to a warning and ignore its value.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7318 / 15416</td>
          <td>
            <b>The durability service configuration parameter maxWaitCount is parsed incorrectly.</b><br/>
            <i>
             The durability service configuration parameter maxWaitCount is not
             correctly interpreted. This may cause that the check performed by
             the durability service for the attachment of the networking
             services to a particular group may timeout to early. When that
             occurs the group involved is ignored. When this particular
             durability service has been selected as master for that group it
             will not align this group to the other nodes.
             <br/>
              <b>Solution: Calculate the correct timeout value from the
              configured maxWaitCount.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7337 / 15433</td>
          <td>
            <b>DDSI configuration with SSM allowed and ASM disallowed causes attempts at sending to 0.0.0.0</b><br/>
            <i>
             A DDSI configuration that enables SSM but disabled ASM should
             (initially) send participant discovery (SPDP) packets to the
             explicitly configured peers only. Internally this is realised by
             setting the SPDP multicast address to the unspecified address
             (:: or 0.0.0.0), but enabling any form of multicasting caused the
             unspecified SPDP address to no longer be recognised as such, and
             hence to be considered a required destination for SPDP packets.
             This in turn caused the log to fill up with error messages, but was
             otherwise harmless.
             <br/>
              <b>Solution: The error in the processing of the addresses has been
              fixed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7338</td>
          <td>
            <b>Report 'no append' feature improvements.</b><br/>
            <i>
             When using the &lt;Report append="false"/&gt; configuration in
             shared memory mode on Linux, only the traces of the last started
             process will be present in the log files. Also, the log files
             remained untouched until a process opened the log files. This means
             that the error log was sometimes not in sync with the info log
             file. Lastly, the OSPL_LOGAPPEND environment variable was not
             handled correctly.
              <br/>
              <b>Solution: Delete stale log files as soon as
              OSPL_LOGAPPEND=false or &lt;Report append="false"/&gt; is
              detected. Only the first process (spliced or single process) is
              allowed to do the deletion.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7341</td>
          <td>
            <b>A durability configuration with aligner set to false may miss data on reconnect.</b><br/>
            <i>
             A federation with a durability service which has the configuration aliger set to
             false (alignee) didn't receive data published during a disconnect after a reconnect
             when the master durability service does not detect a master conflict. The alignee
             detected a master conflict and assumed the master would raise its state while the
             master saw no conflict and has no reason to raise its state.
              <br/>
              <b>Solution: The alignee now requests the latest state from the master when it recovers
              from a disconnect during which it had no master.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7368 / 15476</td>
          <td>
            <b>For the ISO C++ mapping the idlpp compiler generates incorrect code for a typedef of an array.</b><br/>
            <i>
             When the idl specification contains a typedef of an array then the
             code generated by the idlpp compiler for the ISO C++ mapping is
             incorrect. In that case it partly generates typedefs for the array
             alias which are usually generated for the C++98 mapping.
              <br/>
              <b>Solution: A condition is added to the idlpp compiler when
              generating code for a typedef of an array which checks if the code
              has to be generated for the ISO C++ or the C++98 mapping.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7376 / 15469</td>
          <td>
            <b>Idlpp generates sequence alloc functions twice when compiling
            different idl modules which contain a definition of a sequence of a
            type specified in another module.</b><br/>
            <i>
             When different idl modules specify a typedef of sequence of a type
             which is defined in another module or the specify a typedef of a
             sequence of a basic idl type then the corresponding alloc and
             allocbuf functions are generated twice by the idlpp preprocessor.
             For example when two modules each specify
             "typedef sequence&lt;long&gt; LongSeq" then the corresponding
             DDS_sequence_DDS_long__alloc functions are generated twice.
              <br/>
              <b>Solution: For sequence definitions the generate sequence alloc
              and allocbuf are prefixed with the scope name of the module in
              which they are defined.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7445</td>
          <td>
            <b>Cannot build ISOCPP custom lib on Windows</b><br/>
            <i>
             On Windows the custom_lib for isocpp would fail to build due to an
             import/export conflict with the classic C++ API on which it was
             built.
             <br/>
              <b>Solution: That conflict has now been resolved.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7469</td>
          <td>
            <b>Java5 DCPS API TopicQos.withPolicy() methods don't copy policies from original TopicQos</b><br/>
            <i>
             The witPolicy() methods on the TopicQos are meant to return a copy
             of the original TopicQos with only the policies supplied as
             arguments to be overridden compared to the original TopicQos. The
             implementation returned the default TopicQos with the supplied
             policies overridden per supplied argument(s), but not using the
             original TopicQos as source meaning that non-overridden policies
             would be default instead of the value in the original TopicQos.
             <br/>
              <b>Solution: The algorithm has been modified to use the original
              TopicQos as source instead of the default TopicQos.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7480 / 15640</td>
          <td>
            <b>ISOCPP C++11 should use defaulted functions.</b><br/>
            <i>
             Since C++11, defaulted functions are introduced. But older Visual
             Studio versions did not support it completely. Constructors and
             assignment operators are generated by idlpp for these compilers.
             However, the compiler detection regarding this issue was not
             correct, resulting in all non-VS C++11 compilers using the
             generated functions.
             <br/>
              <b>Solution: Compiler detection is improved so that all C++11
              compilers that support defaulted functions actually use them.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7527</td>
          <td>
            <b>Invalid handle error in isocpp2 when getting an status from an entity</b><br/>
            <i>
             When requesting an status from an entity which needs an instance
             handle and the status has not yet occurred results in an invalid
             handle error on isocpp2. Invocations of the
             offered/requested_deadline_missed() functions on Writer and Reader
             in the ISOCPP2 API would crash when no such event ever occurred
             before the invocation.
             <br/>
              <b>Solution: That crash has now been fixed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7528 / 15653</td>
          <td>
            <b>TimeoutException in ISOCPP2 should not be recorded in ospl-error.log</b><br/>
            <i>
             The TimeoutException in ISOCPP2 should bot be recorded in the
             ospl-error.log, since it might result in massive amounts of
             undesired log messages in normal scenario's.
             <br/>
              <b>Solution: When a
              TimeoutError occured, it would show up in the ospl-error.log file
              causing potentially lots of undesired messages in there since a
              timeout scenario can be totally valid and is by no means an
              inherent error. For that reason any TimeoutError is no longer
              logged into the ospl-error.log file.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7533</td>
          <td>
            <b>Alignment of durability service with aligner=false configuration
            by late joining master with persistent data intermittently fails</b><br/>
            <i>
             When a late joining durability service with aligner=true
             configuration (master) joins a system with running durability
             services with aligner=false configuration (alignee) it could happen
             that the alignee nodes did not get alignment data from the master
             node. The alignee node detected a master conflict when the master
             node arrived and when it tried to resolve the conflict this
             intermittently failed because the master wasn't always ready to
             align.
             <br/>
              <b>Solution: The alignee node now triggers a master conflict when
              the master is ready to align.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-164</td>
          <td>
            <b>Builtin topic filters do not take into account all CM* topics.</b><br/>
            <i>
              If the user preference Hide Builtin (DCPS*) is set to true, the
              following topics would still be unfiltered: CMPublisher, CMSubscriber,
              CMDataWriter, CMDataReader.
              <br/>
              <b>Solution: The filter pattern has been adjusted to filter out all
              DCPS* topics and all CM* topics. The preference page label has also
              been updated to reflect that.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-207</td>
          <td>
            <b>Partition combo boxes in Tester's AddReader Dialog does not always
            show all existing partitions</b><br/>
            <i>
              If user starts Tester with the "ospltest" command (instructing Tester
              to automatically connect on startup to ospl target using JNI), it is
              possible for some existing partitions to be missing from the partition
              comboboxes in AddReader, AddReaders and AddTopic Dialogs. This behavior
              is more prominent, the more partitions there are.
              <br/>
              <b>Solution: Tester's Partition Manager is now created before any of
              its dependent components to ensure partitions are properly managed and
              available right at connection time.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-332</td>
          <td>
            <b>Mismatch in handling of unbounded character sequences between script
            send and script check.</b><br/>
            <i>
              In a scenario script, given a topic that has an unbounded sequence of
              characters in its type "cseq", the following script  code would fail the
              check:
              <pre><code>
send test_SimpleCollectionTopic (
    cseq[0] => a,
    cseq[1] => b,
    cseq[2] => c
);

check test_SimpleCollectionTopic (
    cseq[0] => a,
    cseq[1] => b,
    cseq[2] => c
);
              </code></pre>
              Passing in indexed parameters for unbounded character sequences is accepted
              for send, but not for check.<br/>
              <b>Solution: The check instruction can now accept indexed parameters for
              unbounded character sequences</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-336</td>
          <td>
            <b>New example Tester scripts needed to show how to manipulate the Record
            and Replay service.</b><br/>
            <i>
              <b>Solution: Composed some new example scripts that define record and
              replay scenarios, configures a storage, then allows to start and stop
              the scenarios on demand. They are now a part of the suite of example
              scripts.</b>
            </i>
        </tr>
      </table>
    <h2>6.5.2p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7425</td>
          <td>
            <b>Support for armv7-marvell-linux-gnueabi-hard</b><br/>
            <i>Port for Ubuntu 14.04 64 bit host to Ubuntu 14.04 for custom ARM V7 board using Marvell Armada 385 processor and armv7-marvell-linux-gnueabi-hard_i686_64K_Dev_20131002 compiler
              <br/>
              <b>
              </b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.5.2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
          <td>OSPL-7134 / 15140</td>
          <td>
            <b>Idlpp (cppgen) created ambiguous enum mutators.</b><br/>
            <i>
             Idlpp (cppgen) generated both a value mutator and an r-value
             reference mutator for enum types. These mutators conflict when
             compiling.
              <br/>
              <b>Solution: Idlpp (cppgen) will not generate the r-value
              reference mutator for enum type anymore.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7155</td>
          <td>
            <b>Failing group coherence on reconnects.</b><br/>
            <i>
             During reconnect tests we discovered failures in the discovery of
             matching writers causing updates never to become complete.
              <br/>
              <b>Solution: Improved discovery mechanism which solves several
              use cases of failing group coherent updates during discovery
              phase of communication end points.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7160 / 15268</td>
          <td>
            <b>Conditions cannot handle const functors.</b><br/>
            <i>
             The different kinds of isocpp2 Conditions only supported non-const
             functors. This particularly caused problems when using lambda
             functions.
              <br/>
              <b>Solution: Added support for const functors.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7227 / 15378</td>
          <td>
            <b>Multiple definitions of copy-routines in code generated by protoc gen-dds back-end</b><br/>
            <i>
             The protoc-gen-dds back-end generates code to allow .proto messages
             to be transparently published/subscribed. When using multiple
             messages originating from different .proto files in a single
             application, some code gets duplicated causing symbols to be
             defined multiple times and linking of compiled code to fail.
              <br/>
              <b>Solution: The offending code has been moved to a separate file,
              which is included by all files that use the construction instead
              of duplicating it in each file.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6802</td>
          <td>
            <b>A new merge policy called CATCHUP is available. This merge policy is similar to
            the already existing REPLACE merge policy, but the resulting instance states may
            differ.</b><br/>
            <i>
              When nodes get disconnected their historical data sets may diverge. To recover
              from divergent states once the nodes get reconnected again, the durability
              service has defined various merge policies. One of these merge policies is the
              REPLACE merge policy. The REPLACE merge policy dispose and replace all historical
              data by the data set that is available on the remote node. Because all data
              is disposed first, a side effect is that instances whose state did not change
              will still be marked as NEW after the merge.<br/>
              For some use cases it is undesirable that instances which have not diverged
              are still marked as NEW. For that reason a new merge policy called CATCHUP
              is available now.
              <br/>
              <b>Solution: The CATCHUP merge policy updates the historical data to match the
              historical data on the remote node by disposing those instances that are not
              available any more, and adding and updating all other instances. The resulting
              data set is the same as that for the REPLACE merge policy, but without the
              side effects. In particular, the instance state of instances that are both
              present on the local node and remote node and for which no updates have been
              done will remain unchanged.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7106</td>
          <td>
            <b>When durability terminates while persisting samples, a sample can be lost.</b><br/>
            <i>
              When persistency is enabled, the durability service is responsible for
              writing samples to the persistent store. If durability terminates while
              writing samples to the persistent store, it may occur that a sample that
              has recently been extracted from the persistent queue but not stored yet,
              gets lost.
              <br/>
              <b>Solution: The check to decide whether durability should terminate can
              now only occur before taking a sample from the persistent queue and after
              it has been written to the store, but not in between. This ensures that a
              sample that has been taken from the persistent queue will always be
              persisted.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7093</td>
          <td>
            <b>When a merge policy is applied, private groups are accidentally aligned.</b><br/>
            <i>
              When a merge policy is applied, merge data must be requested from the master.
              It turns out that also data for private groups is requested when a merge is
              about to take place. This is not needed, as private groups are by definition
              local groups, and data for these groups should never be merged.
              <br/>
              <b>Solution: When a group is marked as a private group, no data for that group
              will be requested anymore.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7136</td>
          <td>
            <b>Improved handling of nested GPB messages during proto files compilation.</b><br/>
            <i>
              The compilation of nested GPB messages could cause compiler crashes when handling
              multiple proto files and/or proto files without the package keyword.
              <br/>
              <b>Solution: The parsing part of the GPB compiler has been improved and made more
              robust.</b>
            </i>
        </tr>
        </tr>
        <tr>
          <td>OSPL-7153</td>
          <td>
            <b>When durability terminates while a merge conflict is pending to be resolved,
            memory may be leaked.</b><br/>
            <i>
              When a merge conflict is detected, the durability service creates a conflict
              object and stores it in a queue. A conflict resolver asynchronously takes
              conflicts from the queue and tries to resolve them. In case a conflict object
              is pending in the queue and the durability service terminates, pending conflicts
              in the queue are NOT destroyed. This may lead to leakage of the conflict object.<br/>
              <b>Solution: When durability terminates any pending conflicts in the queue are
                cleaned up.</b>
            </td>
          </tr>
        <tr>
          <td>OSPL-7189 / 15316</td>
          <td>
            <b>Networking receive thread can spin at 100% following a disconnect/reconnect</b><br/>
            <i>
              The RT networking service keeps track of a list of ACKs waiting to be sent. The code to cleanup this list following a disconnect contained an error that could corrupt the list if packet loss had occurred just before the disconnect.
              <br/>
              <b>Solution: the cleanup is now robust against packet loss.</b>
            </i>
          </tr>
        <tr>
        <tr>
          <td>OSPL-3817</td>
          <td>
            <b>The idlpp preprocessor crashed when when the idl specification
            contains a sequence of a typedef of sequences.</b><br/>
            <i>
              The idl preprocessor handles a sequence of a typedef of sequences
              incorrect which results in a crash of the preprocessor when it
              tries to parse such a construction. This caused by a missing
              condition in the preprocessor when it tries to pass this
              construction.<br/>
              <b>Solution: The missing condition is added to the idlpp
              preprocessor to handle the sequence of a typedef of sequences
              correctly.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-4403</td>
          <td>
            <b>Corrupt RnR storage when using a storage type that doesn't match
            contents of existing file</b><br/>
            <i>
              Two RnR storage backends are available: XML and CDR. When
              configuring a storage, either by publishing RnR config commands or
              in the OpenSplice configuration file, it's possible the data files
              already exist on the filesystem and contain data recorded during a
              previous session. When the recorded data is in XML format, it's
              possible to configure a new storage to use CDR format, or
              vice-versa, and append new samples to the storage in a different
              format than those recorded earlier. This corrupts the storage and
              it will not be usable for replay.<br/>
              <b>Solution: The issue was resolved by adding verification of
              attributes when existing files are found. Changing a storage in a
              way that conflicts with existing files is no longer allowed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7060 / 15088</td>
          <td>
            <b>When OpenSplice is stopped an application may remain blocked on
            the WaitSet.wait operation.</b><br/>
            <i>
              When an application is waiting in the WaitSet.wait operation and
              at that moment OpenSplice is stopped with the 'ospl stop' command
              or the spliced daemon is killed, then the WaitSet.wait operation
              will never return. This may occur even when the wait is performed
              with a timeout value.
              <br/>
              <b>Solution: When the application is attached to the shared memory
              segment a callback is registered. When the spliced daemon which
              controls the shared memory segment is terminated the callback will
              be triggered. The callback detaches the application from the
              shared memory segment and triggers the WaitSet.wait to return
              OK together with the list of conditions that were detached from
              shared memory. When performing an operation on one of the returned
              conditions, ALREADY_DELETED will be returned.</b>
            </i>
          </tr>
        <tr>
        <tr>
          <td>OSPL-7090</td>
          <td>
            <b>Publishing multiple RnR commands at once may result in some
            commands being rejected by the service</b><br/>
            <i>
              An issue in the ordering of commands received at the same time,
              may cause commands to be rejected if they depend on each other.
              For example a config-command that defines a storage must be
              processed before an add-record command that uses this storage, or
              the add-record command would be rejected because the storage is
              undefined.<br/>
              <b>Solution: The issue was resolved by processing the commands in
              the same order as they are delivered to the service.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7126 / 15106</td>
          <td>
            <b>Crash of watchdog thread on VxWorks RTP shared memory deployments</b><br/>
            <i>
             The VxWorks support for shared mutexes and condition variables does not match the POSIX pthreads behaviour required by OpenSplice core, with the OpenSplice abstraction layer translating between the two. Because of an issue in the abstraction layer, reuse of a shared memory address for a mutex could result in different processes using different VxWorks kernel mutexes. This in turn caused race conditions and, under some circumstances, crashes.
              <br/>
              <b>Solution: the abstraction layer has been updated to not dissociate the mutex names from the kernel mutexes, thereby ensuring that all processes always agree on the kernel mutex to use.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7141 / 15185</td>
          <td>
            <b>Crash when getting IsoCpp null listener.</b><br/>
            <i>
              Calling the listener getter function in IsoCpp when no listener
              was set, will cause a crash because it tries to use a null pointer.
              This null pointer is of an internal object that is created when a
              listener is attached.<br/>
              <b>Solution: A check is added to the getter functions that will
              return null when the internal data object is null, meaning that no
              listener was set.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7142 / 15187</td>
          <td>
            <b>First few samples written after discovery of a remote node not
            delivered</b><br/>
            <i>
              Data is forwarded from the local writers (i.e., those attached to
              the same shared memory as the networking service in a shared
              memory deployment) only when the RT networking service has
              detected remote nodes, with some additional safeguards for data
              written while the system is starting up. When the first remote
              node was discovered, services (and applications) on the local node
              were informed of this event before the forwarding was enabled,
              which could cause data to be lost. A typical symptom is that the
              durability service fails to make progress.
              <br/>
              <b>Solution: forwarding is now enabled before the existence of the
              remote node is announced.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7151</td>
          <td>
            <b>Error registering a non-scoped type in Java</b><br/>
            <i>
              Attempting to register a type without a scope (so module-less)
              using the Java API no longer worked due to internal algorithm that
              simply assumed that each type was scoped.
              <br/>
              <b>Solution: The internal algorithm has been modified to check
              whether the type that is registered has a scope.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7189 / 15316</td>
          <td>
            <b>Networking receive thread can spin at 100% following a disconnect/reconnect</b><br/>
            <i>
              The RT networking service keeps track of a list of ACKs waiting to be sent. The code to cleanup this list following a disconnect contained an error that could corrupt the list if packet loss had occurred just before the disconnect.
              <br/>
              <b>Solution: the cleanup is now robust against packet loss.</b>
            </i>
          </tr>
        <tr>
        <tr>
          <td>TSTTOOL-296</td>
          <td>
            <b>Sample Display view and Check script instruction incorrect when
            char sequence field followed by field with same initial prefix.</b><br/>
            <i>
              If a topic data type contains a string field (or a character
              sequence or array) followed by another string field whose field
              name starts with the field name of the previous field, then the
              internal data model would mistakenly concatenate both fields into
              one field.
              <br/>
              <b>Solution: The string field concatenation involving similarly
              named fields has been fixed.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-314 / 15087</td>
          <td>
            <b>Tester bounded sequence fields always send the max length of the
            sequence.</b><br/>
            <i>
              When Tester writes out a sample containing a bounded sequence, it
              always populates the full sequence length with values.
              <br/>
              <b>Solution: The tool has been modified to only allocate sequence
              elements for elements that are actually defined, instead of
              allocating the full length with defaults first. NOTE: sequence
              elements must be defined in index order in a script.
              eg. send aTopic(seq[0] => 1, seq[1] => 1); is valid, while send
              aTopic(seq[1] => 1, seq[0] => 1); is not.</b>
            </i>
        </tr>
    </table>
    <h2>6.5.1p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6616 / 14697</td>
          <td>
            <b>Lack of reliability for first few packets on a channel can lead to unexpected behaviour</b><br/>
            <i>
              Loss of the first few packets on a channel was not always detected and corrected. In case
              these packets belonged to the durability protocol for example, this could lead to unexpected
              behaviour.<br/>
              <b>Solution: The reliable protocol is extended to support (re)transmitting packets to nodes
                that have not yet responded. Information about these nodes is shared across channels, so
                that when a single channel discovers reliable communication with a node on a NetworkPartition,
                all channels will. Furthermore, receiving nodes wait until they received the oldest packet
                they should receive before starting delivery of data.
              </b>
            </i>
          </td>
        </tr>
      </table>
      <h2>6.5.1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4153</td>
          <td>
            <b>Durability to support compression for KV-persistence</b><br/>
            <i>
               The durability service currently has a KV-persistency
               implementation that allows persisting data to disk in either
               SQLite or LevelDB. Testing performance indicates that the disk is
               the bottleneck when trying to achieve a high throughput in some
               use cases. Therefore it should be possible to compress data
               when persisting.
              <br/>
              <b>Solution: Samples can now be compressed by durability before
              persisting them (and obviously uncompressed before re-publishing
              them in DDS). Durability supports compression as a configurable
              option for KV persistence. For more details on how to configure
              it, please check section 4.3.3.9.3 of the Deployment Guide</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6519</td>
          <td>
            <b>Durability service may not detect disconnecting federation</b><br/>
            <i>
              The durability service could in some situation miss a dispose of
              DCPSHeartbeat, causing it to think that a remote federation is
              still running. That on its turn may prevent correct alignment of
              historical data.
              <br/>
              <b>Solution: The algorithm that checks whether the DCPSHeartbeat
              is disposed has been corrected.</b>
            </i>
        </tr>
        <tr>
          <td>
            OSPL-6950
          </td>
          <td>
            <b>DDSI2 not utilizing latency budget shorter than 1s</b><br/>
            <i>
              The interface between the OpenSplice kernel and the networking
              services implements the latency budget, but requires the
              networking service to make a trade-off between idle wake-ups and
              efficient handling of short latency budgets. The DDSI2 service
              opted to have few idle wake-ups, but at the cost of essentially
              treating a latency budget setting < 1s as if set to 0s, which
              differs significantly from the RT networking service that puts the
              cutoff at 10ms by default (The RT networking service has useful
              work to perform in the idle wake-ups, hence the different
              trade-off). It should be noted that a high-rate writer typically
              manages to have some data in the network queue at all times, in
              which case it is not the latency budget that drives packing, but
              the sheer amount of data.
             <br/>
             <b>Solution: DDSI2E now allows configuring this using the
             Internal/Channels/Channel[@name]/Resolution setting (similar to the
             RT networking setting). The default is unchanged.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6984
          </td>
          <td>
            <b>DDSI2 possible use-after-free following end_coherent_updates</b><br/>
            <i>
              DDSI2 may find itself forced to grow a message buffer when
              sending an end-of-transaction commit message, which may result
              in the message header being relocated in memory. This could lead
              to the setting of a flag in the message header in freed memory.
             <br/>
             <b>Solution: It now recomputes the address of the message header.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6985
          </td>
          <td>
            <b>Crash due to refcounting issue in large group-transactions</b><br/>
            <i>
              The publishing side in group transactions larger than 50 writers
              may crash because of a refcounting issue in enlarging the group
              transaction administration.
             <br/>
             <b>Solution: The refcounting issue has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-7013
          </td>
          <td>
            <b>DDSI2 limits max deployments on a node that are discoverable via unicast</b><br/>
            <i>
              When DDSI2 tried to allocate a participant index when
              Discovery/ParticipantIndex = "auto", it limited itself to indices
              0 .. 9. In practice, this meant that running more than 10
              (single-process) deployments on a machine required multicast
              discovery and the ParticipantIndex option set to "none". The
              amount of unicast discovery pings sent out periodically is
              directly related to the maximum, which argues in favour of a small
              limit. However, it should provide a way of configuring a higher
              limit in cases where this is required.
             <br/>
             <b>Solution: A setting has been added,
             Discovery/MaxAutoParticipantIndex, which configures the highest
             index to be tried automatically.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-7032
          </td>
          <td>
            <b>C&M API does not allow creation of Publisher with one ore more
            non-default immutable policies</b><br/>
            <i>
              The C&M API is internally creating a publisher as enabled and
              tries to apply the QoS afterwards. If any of the immutable
              policies are immutable, this fails. This prevents tools from
              creating publishers with non-default values for immutable QoS
              settings. The implementation should create the entity disabled,
              set the qos and then enable the entity.
             <br/>
             <b>Solution: The C&M API now creates the publisher in disabled
             mode, sets the QoS and enables the publisher.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-298
          </td>
          <td>
            <b>Protobuf feature for Tester does not account key and filterable
            fields with name overrides.</b><br/>
            <i>
              If a .proto data definition contains FieldOptions with "name"
              defined to override the name of the member in DDS, then Tester
              doesn't recognize it, and subsequently fails to read those fields.
             <br/>
             <b>Solution: The feature now finds all the DDS specific
             FieldOptions defined in the protobuf metadata to find the real
             names of the key and filterable fields, and properly translates
             them between sample data view in tool, and sample reading/writing
             at middleware.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7027 / 14930</td>
          <td>
            <b>100% CPU Usage network service when resending locally rejected samples</b><br/>
            <i>
              When a local datareader uses resource limits and runs into these
              resource limits, the networking service may go to 100% load when
              attempting to re-deliver a sample that is rejected due to the
              fact that the maximum resource limits have been reached.
              <br/>
              <b>Solution: The networking service now reschedules the delivery
              of a sample to a datareader for the next resolution tick instead
              of attempting the re-deliver in a busy-wait loop until it is
              delivered.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7052</td>
          <td>
            <b>Unmatching Qos messages in ospl log and remote group detection
            for client durability.</b><br/>
            <i>
              The durability service uses various topic definitions for the
              implementation of the client-durability feature. The qos values
              for the max_blocking_time and lease_duration for these topics
              should be same as the defaults in the DDS specification, but due
              to a bug this was not the case. When a client generates a topic
              that does use the default qos values, a warning would appear in
              the ospl log file. Furthermore, the client durability server did
              not detect the creation of non-volatile writers on remote nodes.
              Consequently, the client durability server would not collect and
              align the data written by these writers.
              <br/>
              <b>Solution: The durability service has changed the qos values for
              the topics related to the client-durability feature, so that the
              max_blocking_time and lease_duration qos values now conform to the
              DDS specification. Also, the durability service now detects the
              creation of non-volatile writers on remote nodes so that the
              server is able to collect and align the data written by these
              writers.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7089 / 15092</td>
          <td>
            <b>DURATION_ZERO_SEC and DURATION_ZERO_NSEC notation not recognized
            by QoSProvider.</b><br/>
            <i>
              The QoSProvider does not recognize the DURATION_ZERO_SEC and
              DURATION_ZERO_NSEC time values defined in the XML syntax.
              <br/>
              <b>Solution: The missing time values are now correctly parsed.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-301</td>
          <td>
            <b>Writing octal character codes in c_char fields in script
            scenarios fails validation.</b><br/>
            <i>
              As of OpenSplice V6.5.0p7, a feature was introduced in Tester
              where values punched in to sample field parameters in a scenario
              script send or dispose command underwent a validation on execute
              to ensure that the typed in value fits in the known IDL type for
              that topic. The validation failed to consider the case for IDL
              char fields where octal character codes of the form was used as
              input.
              <br/>
              <b>Solution: Script validation of topic fields of IDL type "char"
              now allows for values of the regex form \\[0-3][0-7][0-7]  eg. a
              valid send instruction: "send aTopic(aCharField => '\000');"</b>
            </i>
        </tr>
    </table>
    <h2>6.5.0p13</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5473</td>
          <td>
            <b>Shared memory leak during alignment</b><br/>
            <i>
              Implicit disposes weren't delivered for builtin topics, causing
              leakage in shared memory. There is a small chance other topics
              could potentially leak as well, but it was never observed.
              <br/>
              <b>Solution: The implicit (internal) flag is now part of the
              equation to find duplicate messages.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6284 / 14504</td>
          <td>
            <b>Durability service fails to update service lease</b><br/>
            <i>
              An issue in the durability service could result in a spinning
              thread during alignment of data from other durability nodes. The
              spinning thread triggers internal safety mechanisms, eventually
              preventing the service from updating it's service-lease which
              causes the spliced process to consider the service crashed or
              deadlocked. Depending on the configured failure-action, spliced
              may decide to terminate or restart all OpenSplice services.
              <br/>
              <b>Solution: The cause of the spinning thread has been resolved.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6519 / 14561</td>
          <td>
            <b>Incorrect processing of DCPSHeartbeat by spliced and durability</b><br/>
            <i>
               The durability service could in some situation miss a dispose of
               DCPSHeartbeat, causing it to think that a remote federation is
               still running. That on its turn may prevent correct alignment of
               historical data.<br/><br/>
               The spliced thread responsible for processing the DCPSHeartbeat
               topic and thus monitoring the liveliness of other nodes in a
               domain, could get stuck reading old data. Depending on
               configuration (i.e. realtime thread priorities) this could result
               in unreasonably high CPU usage preventing other threads from
               running. This means the liveliness monitoring of other nodes is
               not reliable and removal of a node can possibly go unnoticed by
               the spliced processes on other nodes in the domain.
              <br/>
              <b>Solution: The algorithm that checks whether the DCPSHeartbeat
              is disposed has been corrected in durability and spliced is now
              skipping data that has already been processed.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6683 / TSTTOOL-208</td>
          <td>
            <b>Tooling version compatibility constraint.</b><br/>
            <i>
               Vortex OpenSplice Tester and Tuner need to connect to and read
               the necessary information present in C&amp;M API minimum version
               of 6.5.0.
              <br/>
              <b>Solution: When using C&M API to connect to a remote Vortex
              OpenSplice DDS system, a minimum version compatibility of 6.5.0 is
              now enforced in order to connect between local and remote.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-6992</td>
          <td>
            <b>Some cross development installers are not built for the correct
            architecture.</b><br/>
            <i>
               In some cases where the host machine is 64 bit in a
               cross-development environment, the installer would be generated
               for a 32 bit host. On a subset of machines without 32 bit
               compatibility libraries the installer would not run.
              <br/>
              <b>Solution: The installer generation logic has been updated to
              build for the correct architecture.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7010 / 14926</td>
          <td>
            <b>Deadlock while terminating during initial alignment of durability.</b><br/>
            <i>
               Durability did not notice a fellow being removed during
               initialization causing the initial alignment loop to be infinite.
              <br/>
              <b>Solution: The durability service now checks if a fellow is
              still alive before deciding to wait for the communication state to
              change.</b>
            </i>
        </tr>
        <tr>
          <td>OSPL-7015 / 14928</td>
          <td>
            <b>C# API sample marshalling issues</b><br/>
            <i>
               The C# API was having problems marshaling samples that had an
               attribute that was a sequence of a sequence and of demarshaling a
               sample that had an attribute that was a sequence of strings.
              <br/>
              <b>Solution: Both issues have now been fixed.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-261</td>
          <td>
            <b>As a user I would want to see reason for failure of QOS compatibility in Tester</b><br/>
            <i>
               Tester does not how reasons for failure of QoS compatibility.
              <br/>
              <b>Solution: Display reasons of incompatibility from the tooltip
              when highlighted as incompatible reader.</b>
            </i>
        </tr>
        <tr>
          <td>TSTTOOL-272 / 14761</td>
          <td>
            <b>Slow sending of large strings from Tester</b><br/>
            <i>
               Tester executed non-linear code when sending strings, char
               sequences and char arrays. A 'send' instruction that contained a
               string of about 10K characters would take several minutes to
               complete. Larger strings took exponentially longer.
              <br/>
              <b>Solution: Code was reworked so that send behaviour is now no
              worse than linear with the string size. Casual observations
              suggests that a send is close to constant time. Typical send times
              for strings up to 64K characters where observed to be in the
              30-60 ms time frame.</b>
            </i>
        </tr>
    </table>
    <h2>6.5.0p12</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5473</td>
          <td>
            <b>Small memory leak in DCPS api listener dispatcher for C and C++.</b><br/>
            <i>
              The list of observables monitored by the listener dispatcher leaked.
              <br/>
              <b>Solution: Create the list object at creation instead of on the fly.</b>
            </i>
        </tr>
        <tr>
          <td>
            OSPL-6867
          </td>
          <td>
            <b>Historical data that was requested by a client using the
            client-durability feature could only be delivered to a single,
            fixed partition instead of the partition preferred by the client.</b><br/>
            <i>
              When the durability service receives a request for a client for
              historical data using the client-durability feature, the
              historical data could only be delivered to a single,
              preconfigured partition. Some use cases require the data to be
              delivered to different partitions. For this reason it is now
              possible to deliver the historical data to the partition requested
              by the client. This feature only applies to client-durability, the
              behaviour between durability services is not affected.
             <br/>
             <b>Solution: Clients for client-durability can now specify the
             partition to receive historical data on a per-request basis.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6947
          </td>
          <td>
            <b>Tools and SOAP service may crash.</b><br/>
            <i>
              A piece of memory may be freed to soon when deleting proxy
              entities, like our tools do (potentially through a SOAP
              connection). This leads to free memory leads that can cause the
              tools or SOAP service to crash.
             <br/>
             <b>Solution: The memory corruption has been solved.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.5.0p11</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6900 / 14883
          </td>
          <td>
            <b>Leak of global references in Classic Java PSMs.</b><br/>
            <i>
              A global reference was created for some Java classes related to DDS entities such as DomainParticipantImpl, TopicImpl etc. These references were never deleted so a class instance would leak every time an entity is deleted though the corresponding DDS calls like factory.delete_participant or participant.delete_topic.
             <br/>
             <b>Solution: The issue was resolved by deleting the global reference when an entity is deleted.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6896 / 14878
          </td>
          <td>
            <b>Durability crash during alignment.</b><br/>
            <i>
              When durability is used and during alignment of a fellow, the fellow disconnects durability could crash.
             <br/>
             <b>Solution: The alignment functionality is adjusted so that this crash cannot happen anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6797
          </td>
          <td>
            <b>DDSI2 needs to support multicast on platforms that incorrectly declare multicast unsupported.</b><br/>
            <i>
              DDSI2 relies on the network interface capabilities it reads from the operating system kernel to determine whether or not multicasting is supported on the selected interface, but some platforms do not mark the interface as supporting multicasts even though it does in reality. The workaround of setting the flag manually on the interface requires elevated privileges, which is not always acceptable.
             <br/>
             <b>Solution: An option Internal/AssumeMulticastCapable is now available in DDSI2, which can be set to a comma-separated list of interface name patterns (i.e., including ? and * wildcards) that are assumed to be multicast capable.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6889 / 14875
          </td>
          <td>
            <b>Crash of spliced during termination while application(s) still running.</b><br/>
            <i>
              Normally when the spliced process is terminated, application participants have already detached from the shared database so the database can be removed and OS resources reclaimed during spliced termination. However it is also possible that applications are still running when spliced is terminated. In that case the database will be detached and OS resources can only be reclaimed after the last application participant has also detached from the database. An issue could lead to a crash during spliced termination in the latter case. Because the database is detached but not removed, another spliced thread would try to access it resulting in undefined behaviour.
             <br/>
             <b>Solution: The issue was resolved by preventing the offending spliced thread from accessing administration data during termination stage.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6912 / 14885
          </td>
          <td>
            <b>DDSI2 can mismatch sockets and participants in "many sockets" compatibility mode.</b><br/>
            <i>
              DDSI2 can operate in a compatibility mode in which each participant gets its own socket, in which case there is no requirement to include an explicit destination address in messages intended for just one participant, but the mapping from socket to participant could be done incorrectly. That results in the wrong participant being addressed, which typically results in dropping a message, but could also lead to data being considered acknowledeged when it fact it hadn't been. The problem is only known to occur when interoperating with TwinOaks CoreDX product.
             <br/>
             <b>Solution: The mapping has been corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6909 / 14886
          </td>
          <td>
            <b>Deadlock during durability exit.</b><br/>
            <i>
              When durability is terminating it could end up in a deadlock when terminating its internal listeners.
             <br/>
             <b>Solution: The listener termination mechanism within durability is adjusted and the deadlock cannot occur anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6894 / 14876
          </td>
          <td>
            <b>The u_domain object is not freed causing a memory leak</b><br/>
            <i>
              The u_domain object was not freed even though no other object had a reference to it.
             <br/>
             <b>Solution: When the last participant is freed and thus all the references to the u_domain object are release the u_domain object is freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6419
          </td>
          <td>
            <b>Possible crash in C++ when calling find_topic and deleting participant in other thread</b><br/>
            <i>
              When having a deplicated C++ participant and one thread blocks in a find_topic while the other deletes the participant a crash could occur because the domain was unaware of the deletion of the kernel participant object.
             <br/>
             <b>Solution: The domain is now aware that the deletion of the kernel participant object.</b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.5.0p10</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6781 / 14838
          </td>
          <td>
            <b>Possible deadlock during durability clean up.</b><br/>
            <i>
              When delete_contained_entitites is called it is possible that durability could end up in a deadlock.
             <br/>
             <b>Solution: The clean up mechanism of durability is adjusted so this deadlock cannot occur anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6707
          </td>
          <td>
            <b>Perc JVM error when using waitsets.</b><br/>
            <i>
              When using the Perc JVM in combination with waitsets an error occurs: java.lang.NoSuchMethodError: get_trigger_value.
             <br/>
             <b>Solution: The error is fixed and will not occur anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6555 / 14568
          </td>
          <td>
            <b>OpenSplice installer could fail when installing OpenSplice as service.</b><br/>
            <i>
              When installing OpenSplice as service and the Microsoft Visual C++ Runtime redistributable is not installed on the system the installation could fail.
             <br/>
             <b>Solution: The installer is adjusted to install the Microsoft Visual C++ Runtime redistributable before installing the service.</b>
            </i>
          </td>
        </tr>
       <tr>
          <td>
            OSPL-4466 / 12862
          </td>
          <td>
            <b>C# examples give warnings.</b><br/>
            <i>
              When opening the OpenSplice C# example project files warnings with the text 'Load of property 'ReferencePath' failed' occur.
             <br/>
             <b>Solution: The project files are adjusted so this warning will not occur anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6759 / 14832
          </td>
          <td>
            <b>Typographical error in ospl traces.</b><br/>
            <i>
              The durability service contained a typographical error in the traces.
             <br/>
             <b>Solution: The typographical error is corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6839 / 14865
          </td>
          <td>
            <b>Warning during termination of cmsoap service.</b><br/>
            <i>
              When the cmsoap service is exiting a warning message "Received termination request, will detach user-layer from domain."
in the info log can occur. The cmsoap service wrongly registers 2 exit handlers.
Both of them are executed when the service is requested to terminate and eventually both do the same thing under the hood.
             <br/>
             <b>The exit handler registration is adjusted and now only one handler is registered for the cmsoap service which also removes this warning message.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6519 / 14561
          </td>
          <td>
            <b>Incorrect processing of DCPSHeartbeat by spliced.</b><br/>
            <i>
              The thread responsible for processing the DCPSHeartbeat topic and thus monitoring the liveliness of other nodes in a domain, could get stuck reading old data. Depending on configuration (i.e. realtime thread priorities) this could result in unreasonably high CPU usage preventing other threads from running. This means the liveliness monitoring of other nodes is not reliable and removal of a node can possibly go unnoticed by the spliced processes on other nodes in the domain.
             <br/>
             <b>Solution: The issues were resolved by skipping data that has already been processed.</b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.5.0p9</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6541
          </td>
          <td>
            <b>Tuner doesn't show entity-relations in the partition view</b><br/>
            <i>
              The Tuner tool no longer shows entity-relations when selecting
              the 'partition' view.
             <br/>
             <b>Solution: An internal algorithm to find dependant entities has
             been modified to support partitions again.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6591 / 14606
          </td>
          <td>
            <b>OSPL waits 10 seconds before exiting with a wrong report plugin</b><br/>
            <i>
              When OSPL is started and a configured reportplugin is missing OSPL
              always waits 10 seconds before exiting.
             <br/>
             <b>Solution: OSPL now directly exits and does not wait 10 seconds
             anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6628 / 14751
          </td>
          <td>
            <b>SIGKILL on an application with listeners terminates OpenSplice in an unexpected way.</b><br/>
            <i>
              When SIGKILL is used on an application which uses listeners
              OpenSplice terminates with a message that it could not properly
              clean up its resources.
             <br/>
             <b>Solution: There was a problem in the listener cleanup mechanism.
             This is now fixed and all resources are now properly cleaned up.</b>
            </i>
          </td>
        </tr>
       <tr>
          <td>
            OSPL-6682 / 14763
          </td>
          <td>
            <b>The ospl tool help for return code 16 is incorrect.</b><br/>
            <i>
              The ospl tool help says "not available" instead of "non existent"
              for code 16.
             <br/>
             <b>Solution: Non-existent is now also reported in the ospl help for
             code 16.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6743
          </td>
          <td>
            <b>The calculation of a key hash for a key type that is 8-bytes could lead to a crash.</b><br/>
            <i>
              The client-durability feature calculates a md5 key hash for topic
              keys that do not fit in 16 bytes. The hash calculation for an
              8-bytes type key contained an error, leading an invalid key.
              Access to such key could lead to a crash.
             <br/>
             <b>Solution: The hash calculation is changed so that the correct
             hash is calculated.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6748 / 14821 <br/>
            OSPL-6760 / 14833
          </td>
          <td>
            <b>Old sample erratically rejected causing weird resending behaviour.</b><br/>
            <i>
              Due to an issue with handling the case that an old sample is not
              stored in the reader because there is already newer data available
              (KEEP_LAST), a writer (or service) would begin to retry delivering
              that sample. As long as the state of the reader/instance doesn't
              change, this will continue to behave wrongfully.
             <br/>
             <b>Solution: The proper return-code is now returned, causing the
             sample to be accepted instead of rejected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6690
          </td>
          <td>
            <b>Incorrect use of the '#' character in the RTNetworking protocol for topic-/group-coherency</b><br/>
            <i>
              Due to a bug in the handling of transaction-markers in the RTNetworking
              protocol, partition-names starting with a '#' could cause problems.
             <br/>
             <b>Solution:  The processing of transaction markers in the RTNetworking
             protocol has been fixed. If coherency with access_scope V_PRESENTATION_TOPIC
             is used on a build since V6.5.0p5, this will not interoperate with this fix included
             and an upgrade must be performed to get the bug resolved.</b>
            </i>
          </td>
        </tr>
      </table>
    <h2>6.5.0p8</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6687
          </td>
          <td>
            <b>When a sample request is received from an unknown fellow the durability service can crash</b><br/>
            <i>
              When a sample request is received from an unknown fellow, the response that is
              generated contains flawed data which could cause the durability service to crash.
             <br/>
             <b>Solution: The response now contains valid data so that the receiving durability service
              does not crash anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6685
          </td>
          <td>
            <b>DDSI2 may &quot;hang&quot; reading from a socket</b><br/>
            <i>
              When DDSI2 was configured to use only a single unicast
              port, it would attempt to read two packets from the
              socket corresponding to the unicast port when signalled
              that data was available. Usually, a packet would arrive
              in short order, but if it didn't happen DDSI2 would
              appear to hang (stop processing messages, fail to
              terminate properly).
             <br/>
             <b>Solution: It now only reads when data is known to be available.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6588 / 14609
          </td>
          <td>
            <b>ExitRequest handler interferes with Java shutdown hook</b><br/>
            <i>
              The exit request handler that OpenSplice installs runs before the
              JVM runs the shutdown hooks. This causes a problem for an application
              that tries to do some clean up when the JVM shuts down due to failing
              DDS operations in Java with ALREADY_DELETED as result.
             <br/>
             <b>Solution: For Java OpenSplice now does nothing for the posix signals
             SIGHUP, SIGINT and SIGTERM and windows signals CTRL_C_EVENT and
             CTRL_BREAK_EVENT before the JVM runs the shutdown hooks. This way an
             application can still do some clean up. After the application clean
             up the JVM will then trigger the OpenSplice clean up.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6622 / 14699
          </td>
          <td>
            <b>Possible NULL pointer dereference could cause durability service
            to crash on segmentation fault</b><br/>
            <i>
              Removal of a fellow assigned NULL to the request member of a chain
              object. This could cause the durability service to dereference a
              NULL pointer, causing it to crash with signal 11.
             <br/>
             <b>Solution: Upon removal of a fellow the request member of a chain
             object is now copied. This fix ensures the request member is
             properly freed if the reference count of a chain object reaches zero.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6662
          </td>
          <td>
            <b>API call create_persistent_snapshot execution time</b><br/>
            <i>
              When API call create_persistent_snapshot is called it will take
              always at lease one StoreSessionTime to execute. This could lead to
              extra or less data in the snapshot than would be expected.
             <br/>
             <b>Solution: The snapshot will now be executed as soon as possible
             and will not wait at least one StoreSessionTime.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-180
          </td>
          <td>
            <b>Difficult to send amended topic instances in OpenSplice Tester</b><br/>
            <i>
              Tester scenarios in which a topic instance is sent, and then repeatedly amended and resent
              were tedious, and error prone to write, as each send instruction had to repeat all sample fields,
              even if only a few had changed.
             <br/>
             <b>Solution: A new 'update parameter' (composed of the topic name followed by _update) is now accepted
             by the send instruction. If the update parameter is present, the sample data sent will be preserved by
             the topic reader. At most one sample per topic instance is preserved. If the update parameter's value
             is 'true', then the sample to be sent is initialized from the previously preserved sample for that topic instance,
             provided the topic reader has retained it.
             In all other cases, the sample to be sent continues to initialized from topic defaults.
             If a send instruction without the update parameter sends a sample, then the topic reader clears
             the saved sample data for the topic instance sent.
             Disposing the reader clears all retained sample data for that topic.</b>
            </i>
          </td>
        </tr>
      </table>

    <h2>6.5.0p7</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-3824
          </td>
          <td>
            <b>C&M API cannot handle array of arrays, array of sequences,
            sequence of arrays and sequence of sequences</b><br/>
            <i>
              The serializer contained bugs that caused the proper elements not to be found.
             <br/>
             <b>Solution: Serializer now uses proper name generation functions where
             applicable and simplified retrieval of user data elements.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-5245
          </td>
          <td>
            <b>Write of c_char field would fail on writing certain characters to ospl log files</b><br/>
            <i>
              The log contained error messages when writing certain special characters,
              even though the write would actually succeed for some of them.
             <br/>
             <b>Solution: Fixed a case in the serializer where memory would be
             released twice, stop write on validation errors, and fixed scanning for
             octal sequences</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6141 / 14430
          </td>
          <td>
            <b>Deadlock in signal handler on trapping synchronous signal in signal handler</b><br/>
            <i>
              The signal handler would deadlock if a synchronous signal was caught
              by the signal handler thread itself.
             <br/>
             <b>Solution: The signal handler will not try to gracefully handle a
             synchrounous signal trapped by the signal handler itself.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6156
          </td>
          <td>
            <b>In order to retrieve historical data on a node a durability service
            must be configured. For nodes with limited resources running that
            still want to acquire historical data a full-fledged durability
            service may not be desirable.</b><br/>
            <i>
              Until now, if a late joiner wants to receive historical data, the
              late joiner must run a fully-fledged durability service. If the late
              joining device runs on a platform with limited resources, running
              a fully-fledged durability service may not be desirable. Instead of
              running a full-fledged durability service, an alternative way to
              acquire historical data whilst being on a platform with limited
              resources has been implemented. This feature is called client-durability.
              Using client-durability, a client can send out a request for historical
              data to a durability service that implements the client durability
              protocol. The server will then provide the requested data to the
              client. To enable the client-durability protocol in a durability
              service the Opensplice/DurabilityService/ClientDurability[@enabled]
              must be set to true. More information is available in the deployment
              manual. The client-durability feature is currently an internal
              feature of OpenSplice and not available for applications.
             <br/>
             <b>Solution: An alternative approach to acquire historical data has
             been implemented that does not require running a fully-fledged
             durability service on the client node.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6626
          </td>
          <td>
            <b>Heap and SHM leakage when using the tester in combination with the Soap service</b><br/>
            <i>
              When using the Tester in combination with the Soap service topic related heap and SHM leakage occurs.
             <br/>
             <b>Solution: The topic leakage has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6490
          </td>
          <td>
            <b>Crash of RnR service when storage path doesn't exist</b><br/>
            <i>
              The RnR service would not allow creation of a storage in a non-existing
              path. This caused the service to publish a storage-status update to
              indicate the storage is in an error state, but a bug caused the
              service to crash shortly after updating its state.
             <br/>
             <b>Solution: The behaviour was changed and the service will now create
             the required path if it doesn't exist. If the creation fails, i.e. due
             to permissions or a corrupt path name, the service updates the
             storage-status topic accordingly without crashing. A config command
             can then be issued to correct the path name and make the storage
             accessible to record and/or replay commands.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-260
          </td>
          <td>
            <b>No reliable way to recheck the last sample of topic instance in OpenSplice Tester</b><br/>
            <i>
              Tester instructions such as check_last and check_any would not reliably find the most recent
              sample for a topic instance.
             <br/>
             <b>Solution: A new instruction, 'recheck_last' was introduced. It's syntax is identical to check_last, but it's
              behaviour is different: recheck_last will always check the most recent sample received, where as check_last fails
              if that topic has already been checked by a previous instruction.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-180
          </td>
          <td>
            <b>No way to invoke a JavaScript without invoking another instruction in OpenSplice Tester</b><br/>
            <i>
              The scenario syntax did not allow for the direct invocation of a JavaScript (or other script language).
              Instead, users needed to include the script as part of another instruction, such as the log instruction.
              For scenarios making heavy use of scripts, this was both inconvenient, and resulted in unnecessarily
              large log files.
             <br/>
             <b>Solution: Scripts are now allowed in scenarios at the same level of as other instructions. The
              script must be enclosed in back quotes (`). The script invocation block (script enclosed in its back quotes)
              must be terminated by a semi-colon.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-179
          </td>
          <td>
            <b>OpenSplice Tester provides no feedback on data conversion errors</b><br/>
            <i>
              Sending a sample which included invalid data (e.g. sending a floating point value to an integer field)
              were silently ignored. No entries were made in the Tester log.
             <br/>
             <b>Solution: Tester now checks for invalid values, and creates appropriate log entries.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            TSTTOOL-194
          </td>
          <td>
            <b>OpenSplice Tester fails reading scenario files in some cases</b><br/>
            <i>
              Tester would fail to start if it did not have permissions on all the folders containing the specified script and/or macro paths.
              Tester could open the wrong script/macro file if a backup copy with a tilde (~) appended to the full name was present.
             <br/>
             <b>Solution: Tester correctly opens starts and opens scenario and macro files for which it has at least read access.</b>
            </i>
          </td>
        </tr>
     </table>


    <h2>6.5.0p5</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-5888
          </td>
          <td>
            <b>Listener scheduling QoS settings were not being honoured</b><br/>
            <i>
              Listener scheduling QoS (changes) where not applied to the listener
              dispatcher thread.
             <br/>
             <b>Solution: The QoS operations involved in setting and updating
             listener scheduling changes have been reworked and the listener
             dispatcher code was altered to be event based.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6156
          </td>
          <td>
            <b>For nodes with limited resources running that still want to
            acquire historical data a full-fledged durability service may not be
            desirable.</b><br/>
            <i>
            Until now, if a late joiner wants to receive historical data, the
            late joiner must run a full-fledged durability service. If the late
            joining device runs on a platform with limited resources, running a
            full-fledged durability service may not be desirable. Instead of
            running a full-fledged durability service, an alternative way to
            acquire historical data whilst being on a platform with limited
            resources has been implemented. This feature is called
            client-durability. Using client-durability, a client can send out a
            request for historical data to a durability service that implements
            the client durability protocol. The server will then provide the
            requested data to the client. To enable the client-durability
            protocol in a durability service the
            Opensplice/DurabilityService/ClientDurability[@enabled] must be set
            to true. More information is available in the deployment manual.
            The client-durability feature is currently an internal feature of
            OpenSplice and not available for applications.
             <br/>
             <b>Solution: An alternative approach to acquire historical data has
             been implemented that does not require running a full-fledged
             durability service on the client node.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6194
          </td>
          <td>
            <b>When there are many pending sample requests the durability service
            becomes very slow.</b><br/>
            <i>
              The durability service internally keeps a list of pending sample
              requests. Each time a new request is received the durability service
              will traverse the list to see if there exists a pending sample
              request for the same partition and topic. If so, these requests
              will be combined. The algorithm to find if a request is already
              pending used an inefficient implementation, causing high CPU load and
              unnecessary delays in situations where the list of pending sample
              requests is very long.
             <br/>
             <b>Solution: The implementation to see if a request is already
             pending now uses an optimized algorithm.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6258
          </td>
          <td>
            <b>Liveliness of a remote node not always consistent among services, built-in topics</b><br/>
            <i>
              A number of independent heartbeat mechanisms were used in parallel,
              resulting in different parts of a single federation having short-term
              inconsistencies between their views of the set of live remote nodes
              as well as introducing dependencies between the configuration of
              services to ensure dependencies between the services (e.g.,
              networking and durability) in removing a node were handled correctly.
             <br/>
             <b>Solution: The mechanisms have now been consolidated.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6272
          </td>
          <td>
            <b>Networking bridge log verbosity</b><br/>
            <i>
              The configurator contains a non-existent value 'finer' for the networking
              bridge tracing level verbosity.
             <br/>
             <b>Solution: The value finer is removed from the configurator.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6320
          </td>
          <td>
            <b>Bind error when reconnecting after a network adapter does down and then up.</b><br/>
            <i>
              When trying to reconnect after an ethernet adapter went from down to up
              a 'bind returned errno 22 (Invalid argument)' message could occur in the
              ospl-error log.
             <br/>
             <b>Solution: The reconnect mechanism is adjusted and the error will
             not occur anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6332/<br>14527
          </td>
          <td>
            <b>ISOCPP listeners might reference invalided entities</b><br/>
            <i>
              ISOCPP listeners used a raw pointer to reference DDS entities on
              stack that might have been invalidated.
             <br/>
             <b>Solution: ISOCPP listeners now use a dds::core::WeakReference to
             keep a reference to DDS entities.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6385<br>
          </td>
          <td>
            <b>Durability service internal algorithm to read protocol samples
             should be optimised</b><br/>
            <i>
              The durability service creates datareaders to receive information
              from fellow durability services to align historical data. The
              internal algorithm to read this data, takes one sample at the time
              where taking all samples at once would be more efficient processing-wise.
             <br/>
             <b>Solution: The durability service has been modified to take all
             available samples at once.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6408<br>
          </td>
          <td>
            <b>Invalid default max_blocking_time for DataWriter with BEST_EFFORT
               reliability in ISOC++ API</b><br/>
            <i>
              The ISOC++ API set the default Reliability.max_blocking_time to
              zero instead of 100 ms for DataWriters with BEST_EFFORT reliability.
             <br/>
             <b>Solution: DataWriters now always have 100ms as default
             max_blocking_time no matter their reliability.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6437<br>
          </td>
          <td>
            <b>Topic-access feature behaves unexpectedly</b><br/>
            <i>
              When using the Topic-access feature unexpected errors can occur in OpenSplice
              because the builtin topics are also slaved to this setting.
             <br/>
             <b>Solution: The builtin topics ignore the topic-access setting so
             OpenSplice will keep working.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6454<br>
          </td>
          <td>
            <b>Installer crashes in text mode</b><br/>
            <i>
              A fault with the third party installer creation program causes
              OpenSplice to crash when text mode is used
             <br/>
             <b>Solution: The install cration program for OpenSplice was fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6477<br>
          </td>
          <td>
            <b>Default Secure Networking configuration files contain an error</b><br/>
            <i>
              The default secure networking configuration files contained a fault
              which could lead to an error when loaded into the configurator.
             <br/>
             <b>Solution: The fault is fixed and the configurator can load the
             file without errors.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6488/<br>14510
          </td>
          <td>
            <b>Tuner problem when writing a bounded character array</b><br/>
            <i>
              There was a fault in the tuner when trying to write a bounded character array.
              When the array is written the data was ignored.
             <br/>
             <b>Solution: The array is not ignored anymore and the data will be written.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6491<br>
          </td>
          <td>
            <b>Remove dependency on CORBA::string_dup in RMI</b><br/>
            <i>
              Vortex Lite shares some of the RMI codebase, but does not support
              CORBA co-habitation. rmipp generates code that includes CORBA::string_dup
              that Vortex Lite does not support.
             <br/>
             <b>Solution: rmipp changes to generated DDS::String_dup instead that is
             supported by both Lite and OpenSplice.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6496<br>
          </td>
          <td>
            <b>Adding RMI support to operate without a durability service</b><br/>
            <i>
              RMI services registration and discovery relies on Transient
              DurabilityQosPolicy as provided by a durability service in
              OpenSplice. In some specific deployment conditions, that service is
              not available. RMI should be adapted to support that use case.
             <br/>
             <b>Solution:A new command line option called
             "--RMIDurability = yes | no" was added to indicate the availability
             of the durability service. Please refer to the RMI documentation
             for more details.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6531/<br>14566
          </td>
          <td>
            <b>DDS-RMI OSGi bundle not exporting all required packages</b><br/>
            <i>
              An issue in the manifest for the OSGi bundle of DDS-RMI, caused the
              bundle to be incompatible with application bundles that contain
              code generated by rmipp.
             <br/>
             <b>Solution: The DDS_RMI.Impl packages were added to the
             Export-Package manifest attribute</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6640/<br>14547
          </td>
          <td>
            <b>Problem with license files</b><br/>
            <i>
              The information to get a license is not accurate for the Windows
              platform in the getting started guide.
             <br/>
             <b>Solution: The getting started guide has been updated to reflect
             the proper command on Windows.</b>
            </i>
          </td>
        </tr>
     </table>


    <h2>6.5.0p4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6464/<br>14412
          </td>
          <td>
            <b>Semaphore handle leak on Windows</b><br/>
            <i>
              An issue in the OS abstraction layer of the product caused a leak
              of semaphore handles. When a condition variable, used in many areas
              of the product i.e. when notifying a WaitSet or acknowledging a
              synchronous write, is triggered while no other threads are blocking
              on the condition, a shortcut is possible. However this shortcut
              contains a bug and leaks a semaphore handle.
             <br/>
             <b>Solution: The issue was resolved by making sure the handle is
             closed in all circumstances.</b>
            </i>
          </td>
        </tr>
     </table>

    <h2>6.5.0p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-2967
          </td>
          <td>
            <b>Deadlock in Java listeners</b><br/>
            <i>
              Terminate from within a listener callback required a new Thread from
              within the callback and call System.exit() in that thread.
             <br/>
             <b>Solution: The additional thread is no longer required.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5684
          </td>
          <td>
            <b>Reference guide update for SchedulingQosPolicy</b>
            <br/>
            <i>
             The reference guides did not mark SchedulingQosPolicy as PrismTech
             proprietary property.
             <br/>
             <b>Solution: Minor updates to guides only.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-3936
          </td>
          <td>
            <b>DDSI2 memory leak when retransmitting part of a fragmented sample
            because of retransmit queue limiting</b>
            <br/>
            <i>
             When DDSI2 attempts to retransmit a fragmented sample, but reaches the
             retransmit queue limit at the first fragment, it can leak one HeartbeatFrag message.
             <br/>
             <b>Solution: Memory leak fixed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-4471
          </td>
          <td>
            <b>Configurator does not support fields other than string values to
            contain environment variables</b>
            <br/>
            <i>
             The configurator does not support fields other than string values to
             contain environment variables i.e. ${DOMAIN_ID}
             <br/>
             <b>Solution: The configurator does now support environment values for
             every input type except enum types and booleans. However Enum
             types/boolean environment variables are supported by all services
             so by editing the configuration file manually it still can be used.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5557
          </td>
          <td>
            <b>Starting Tester in an environment where the data types for
            OsplArrayTopic and OsplSequenceTopic were already registered,
            caused errors to be printed in the ospl error log.</b>
            <br/>
            <i>
             Tester tries to register its topic types on startup. However, due to
             a slight change in the way the meta type is defined internally, if
             the topic types were registered previously, then trying to register
             them again caused a meta type mismatch error.
             <br/>
             <b>Solution: The XML meta type definitions for OsplArrayTopic and
             OsplSequenceTopic have been updated. The error no longer appears.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5575
          </td>
          <td>
            <b>Begin/End Coherent Changes API calls did not return the correct
            return code in case of a failure.</b>
            <br/>
            <i>
             When using the Begin/End Coherent Changes API calls it was possible
             that an incorrect return code was returned.
             <br/>
             <b>Solution: The calls now return a correct return code as per spec.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5739
          </td>
          <td>
            <b>When there are other users running OpenSplice then the command "ospl stop -a" may fail.</b>
            <br/>
            <i>
             The ospl stop -a command may fail when there are other users running
             OpenSplice. The ospl stop -a command tries to open all key files
             associated with the running OpenSplice instances. When it cannot open
             a key file because it belongs to another user the ospl stop -a command
             stops and reports an error.
             <br/>
             <b>Solution: When the ospl stop -a command walks over the key files
             and finds a file it cannot open it should not stop and instead continue
             parsing the next key file.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6030
          </td>
          <td>
            <b>Java OSGI Support</b>
            <br/>
            <i>
             Only the standalone Java language binding had an OSGi compliant jar
             file. It was a separate jar file called dcpssaj-osgi-bundle.jar.
             <br/>
             <b>Solution: The following jar files are now OSGi compliant: dcpscj.jar,
             dcpscj5.jar, dcpssaj.jar, dcpssaj5.jar, ddsrmi.jar and rlm.jar.
             These OSGi jar files can still be used as 'normal' jar files. No
             extra separate OSGi jar files needed. Only dcpssaj-osgi-bundle.jar
             is maintained for backwards compatibility.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6192
          </td>
          <td>
            <b>Incomplete error messages</b>
            <br/>
            <i>
             A review of error log output across the OSPL APIs has been completed
             and a number of reports have been improved.
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6285/<br>14506
          </td>
          <td>
            <b>Non terminated AsyncReplyWaiter threads in case of concurrent
            asynchronous methods invocations</b>
            <br/>
            <i>
             When the application invokes multiple asynchronous requests concurrently,
             multiple threads called AsyncReplyWaiter are created and not
             terminated by the runtime stop operation. The AsyncReplyWaiter thread
             is the thread that waits for asynchronous replies and dispatches
             them to the user asynchronous reply handlers. There should be only
             one AsyncReplyWaiter thread per process that is terminated at
             runtime stop.
             <br/>
             <b>Solution: The AsyncReplyWaiter thread creation has been properly
             synchronized to avoid creating multiple AsynchReplyWaiter threads.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6286/<br>14507
          </td>
          <td>
            <b>RMI asynchronous reply handler may be called back for an invalid reply</b>
            <br/>
            <i>
             This may happen when the rmi server stops running for some reason. If
             a client issues an asynchronous request after that, it may receive
             an invalid reply including random values.
             <br/>
             <b>Solution: The RMI asynchronous reply has been updated to
             process only valid reply topics samples.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6323
          </td>
          <td>
            <b>Durability can deadlock on discovering an unclean shutdown of a
            remote fellow</b>
            <br/>
            <i>
             There are various ways in which the durability service can discover
             that a remote fellow no longer exists, and under most circumstances
             will detect this by the absence of a heartbeat. There are however
             some other ways that are only rarely used, and one of these would
             result in a deadlock in durability, and in such a way that it would
             have significant consequences for the rest of the federation.
             <br/>
             <b>Solution: This deadlock has been fixed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6351/<br>14533
          </td>
          <td>
            <b>Shared memory monitor sometimes couldn't find specifyied domain name</b>
            <br/>
            <i>
             Shared memory monitor couldn't properly compare domain names because
             of trailing whitespace.
             <br/>
             <b>Solution: Updated keyfile parser to trim lines before analyzing them.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6352/<br>14532
          </td>
          <td>
            <b>Warnings when compiling c++ header files with-Wignored-qualifiers using gcc 4.8.2</b>
            <br/>
            <i>
             When compiling code with OSPL c++ include headers files with the
             -Wignored-qualifiers compiler flag warnings show up.
             <br/>
             <b>Solution: The c++ include files now compile without warnings
             when -Wignored-qualifiers is set.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6371
          </td>
          <td>
            <b>Unable to use Foo type name in classic C++ PSMs</b>
            <br/>
            <i>
             Compilation errors occur when building code generated by idlpp, if
             the application idl specifies a type named Foo. This type is also
             used internally, leading to name clashes with existing classes and
             methods.
             <br/>
             <b>Solution: The issue was resolved by using fully scoped names in
             idlpp templates. This ensures the classes from the DDS::OpenSplice
             namespace are used instead of the application namespace.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6387/<br>14539
          </td>
          <td>
            <b>Attaching conditions to a waitset during a wait could cause a crash</b>
            <br/>
            <i>
             Due to an error in the administration of the waitset, a crash could
             occur if the internal administration of the waiset grew to contain
             over 32 conditions while waiting on the waitset.
             <br/>
             <b>Solution: The error in the administration has been resolved.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6402/<br>14542
          </td>
          <td>
            <b>Wrong directory path in windows PATH variable</b>
            <br/>
            <i>
             When installing OpenSplice on Windows and choosing to let the
             installer set the environment variables the installer adds a non
             existing directory path (%OSPL_HOME%/host/lib) to the PATH variable.
             <br/>
             <b>Solution: The installer does not add the non existing directory
             path anymore.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-203
          </td>
          <td>
            <b>Tester unable to handle bounded and unbounded arrarys and
            sequences of characters.</b>
            <br/>
            <i>
             <b>Solution: Tester now properly reads in and write to the DDS,
             field data of bounded and unbounded arrays and sequences of characters.
             This feature is fixed in both the sample editor dialog (in the UI)
             and the scripting engine.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6431
          </td>
          <td>
            <b>An API_INFO report message is reported in the ospl-error.log but
            should be reported in the ospl-info.log.</b>
            <br/>
            <i>
             When an API_INFO report message is reported in the context of an API
             call then the API_INFO report is mistakenly reported in the
             ospl-error.log instead of the ospl-info.log.
             <br/>
             <b>Solution: When an API_INFO message is reported the ospl-info.log
             file is selected instead of the default ospl-error.log.</b>
             </i>
           </td>
        </tr>
      </table>

    <h2>6.5.0p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5859 / 14325
          </td>
          <td>
            <b>Durability alignment may require larger amount of shared memory than expected</b>
            <br/>
            <i>
             During the alignment process, the durability service may collect
             historical data from multiple sources and these sources may
             (partly) have overlapping sets of historical data. When these
             data-sets are received, durability needs to filter out the
             duplicates and once all data from all sources has been received,
             republish the set locally. Due to an issue in a durability
             algorithm, the duplicates where only filtered out at republishing
             time and not on reception time, causing an temporary, but
             unnecessary, need for a larger amount of shared memory.
             <br/>
             <b>Solution: The flaw in the durability algorithm that filters out
             duplicate samples on reception has been repaired.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5930 / 14351
          </td>
          <td>
            <b>Invalid configuration of networking service not detected at startup and causing a crash at termination.</b>
            <br/>
            <i>
             In case the networking service is started with a missing or invalid
             configuration, the service would still run but a connection with
             other nodes is never established. At termination the service could
             crash on freeing internal administration related to configuration
             parameters.
             <br/>
             <b>Solution: The service will now refuse to start when mandatory
             elements of the configuration, such as channels, are missing or
             invalid. An appropriate report is logged to the error log.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6083
          </td>
          <td>
            <b>iShapes example readers should be created with INFINITE latency-budget QoS</b>
            <br/>
            <i>
             To allow demonstrating (the effects of changing) LATENCY_BUDGET,
             the readers should be created with an infinite latency-budget as
             this is a RxO Qos.
             <br/>
             <b>Solution: readers are now created with INFINITE latency-budget.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6176
          </td>
          <td>
            <b>Durability may report error if no persistent data exists on disk</b>
            <br/>
            <i>
             The durability service may report the error 'Unable to resolve
             persistent data version.' during start up when there is a topic
             definition on disk already, but no data yet.
             <br/>
             <b>Solution: The error is now no longer reported under these
             particular circumstances that are deemed valid.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6279 / 14503
          </td>
          <td>
            <b>All XML documents received by registered report plugins start with an error element</b>
            <br/>
            <i>
             All XML documents received by registered report plugins started
             with an error element instead of using a string representation of
             the report type passed when generating the report
             <br/>
             <b>Solution: The string representation of the report type is now
             properly used to start and end the XML document</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6280
          </td>
          <td>
            <b>A non-OpenSplice reliable writer does not to deliver data to an
            OpenSplice best-effort reader using DDSI when on the OpenSplice side
            RnR is enabled.</b>
            <br/>
            <i>
             As RnR was using a reader with private QoS setting named QoS
             matching relaxing to receive both best-effort and reliable data.
             DDSI implementations that didn't understood this setting saw the
             reader as a best-effort reader. When a sample was lost, the RnR
             reader would request a retransmit, as this is required for the
             reliable data. However, as the reader was seen as a best-effort
             reader by the writer, this retransmit request was ignored, causing
             the data to be never delivered.
             <br/>
             <b>Solution: RnR will now create separate readers for each
             non-matching QoS setting (reliable/best effort and
             shared/exclusive ownership)</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6282
          </td>
          <td>
            <b>iShapes GUI badly formatted</b>
            <br/>
            <i>
             The publisher buttons of the iShapes demo application are badly
             formatted compared to the rest of the interface.
             <br/>
             <b>Solution: Fixed the formatting.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6288
          </td>
          <td>
            <b>User data QoS empty in participants discovered via DDSI2</b>
            <br/>
            <i>
            An issue in the processing of the participant QoS caused the
            UserDataQoS to be empty for all remote participants discovery via
            DDSI discovery. This issue did not affect systems using RTnetworking.
             <br/>
             <b>Solution: UserDataQos is now exchanged again during discovery.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6297 / 14516
          </td>
          <td>
            <b>Find topic timing behaviour incorrect</b>
            <br/>
            <i>
             The find topic method on a DomainParticipant has a timeout
             parameter that specifies the maximum blocking time to wait for the
             topic to appear. In certain cases the method would block for 100 ms
             too long, which can have a noticeable impact on the application
             when called often on topics that don't yet exist.
             <br/>
             <b>Solution: The implementation was changed to never exceed the maximum timeout.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6295
          </td>
          <td>
            <b>Windows start menu entry needs to be moved</b>
            <br/>
            <i>
             The location in the start menu on Windows does not match other
             Vortex products and needs to move to 'PrismTech/Vortex OpenSplice'
             <br/>
             <b>Solution: Moved start menu entry to requested location.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-167
          </td>
          <td>
            <b>Tester hangs and won't close if soap connection fails</b>
            <br/>
            <i>
             Ospl tester hangs when the soap connection fails due to an issue
             in an internal algorithm.
             <br/>
             <b>Solution: Tester does not hang anymore when soap connection fails.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-182 / 14349
          </td>
          <td>
            <b>Ospl tester appears to lock up</b>
            <br/>
            <i>
             Ospl tester appears to lock up due to the fact in runs out of
             memory when consuming large amount of samples.
             <br/>
             <b>Solution: Added new preference to limit number of samples kept
             per reader under File&gt;Preferences&gt;Settings&gt;Max Samples Per Reader
             The preference only takes  only integer values. Default value is
             &quot;0&quot; which means infinite number of samples kept.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-184 / 14348
          </td>
          <td>
            <b>New instance not automatically displayed in tester</b>
            <br/>
            <i>
             In Tester, if an application data writer has a QoS of
             autodispose_unregistered_instances set to false and then
             unregister_instance is called on a data writer for some instance,
             a sample reaches matching data readers with the no_writers state
             and is ignored.
             <br/>
             <b>Solution: A new boolean option has been added in the Preferences
             menu under the Settings tab, called "Ignore not_alive_no_writers samples"
             and is set to true by default. If the option is set to false,
             then these specific samples will be displayed in Tester.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-190
          </td>
          <td>
            <b>Tester does not show Vortex Lite ishapes instances in its
            topology unless user disconnects and reconnects</b>
            <br/>
            <i>
             Tester's Reader/Writer Tables (in the Browser Section) does not
             show all partitions for readers/writers with more than 2 partitions.
             <br/>
             <b>Solution: Updated Reader Info and Writer Info to retrieve all
             the available partition names from the
             user data and display them properly in the Browser tab.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-191 / 14350
          </td>
          <td>
            <b>No convenient way to select an existing partition</b>
            <br/>
            <i>
             Ospltest tool does not provide a convenient way to select an
             existing partition when creating a reader.
             <br/>
             <b>Solution: User can now select from a list of existing partitions
             when trying to create a new reader.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-192 / 14352
          </td>
          <td>
            <b>Ospltest gives errors on startup</b>
            <br/>
            <i>
             The script directories are not included in the RTS installers.
             <br/>
             <b>Solution: Updated scripts and install directories to fix these
             errors.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-194 / 14443
          </td>
          <td>
            <b>Tester Script and Macro paths</b>
            <br/>
            <i>
             The Tester does not have permissions for every file/folder within
             and below the path specified for its script/macro area it will not
             start.
             <br/>
             <b>Solution: Tester is now updated to read files with extensions
             .sd, .md and .bd.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-195 / 14447
          </td>
          <td>
            <b>Ospl tester and marks being treated as failures</b>
            <br/>
            <i>
             If Tester is receiving a very large amount of samples, and a mark
             command is run, the execution of the command wants to iterate
             through the sample list to mark all the samples it needs to ignore.
             However, if this iteration does not complete before the next sample
             comes in from the data reader, then it can throw this exception.
             <br/>
             <b>Solution: Fixed tester to add marks properly.</b>
             </i>
           </td>
        </tr>
    </table>
    <h2>6.5.0p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6250
          </td>
          <td>
            <b>Java5 Protobuf example Build.bat script is not working</b>
            <br/>
            <i>
             The Build.bat file that is delivered to compile the Java5 Protobuf
             example contains a copy-paste error from the one derived from Linux.
             <br/>
             <b>Solution: Bat file has been modified to allow compilation of
             the example on Windows</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6250 / OSPL-6263 / OSPL-6270
          </td>
          <td>
            <b>Java5, ISO C++ and C# applications may crash on termination</b>
            <br/>
            <i>
             Applications using the Java5 DDS API may crash at termination time
             due to an issue in the automatic clean up procedure.
             <br/>
             <b>Solution: The issue in the automatic clean up procedure has been
             fixed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6253
          </td>
          <td>
            <b>ISOC++ listeners may be deleted to soon</b>
            <br/>
            <i>
             Applications using the ISO C++ DDS API may experience that entities
             have already been deleted during listener call backs.
             <br/>
             <b>Solution: The entity deletion procedure now waits for potential
             listener callbacks to finish before deleting the entity.</b>
             </i>
           </td>
        </tr>
    </table>
    <h2>6.5.0</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1722
          </td>
          <td>
            <b>Synchronous reliability not supported when using DDSI2</b>
            <br/>
            <i>
             OpenSplice DDS' synchronous reliability feature was not supported
             using DDSI2 and resulted in timeouts on the writer side because one
             of the built-in endpoints needed for processing the acknowledgements
             could not be discovered by DDSI2.
             <br/>
             <b>Solution: The built-in endpoint is now discovered by DDSI2 and
             synchronous reliability is now supported.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-3837
          </td>
          <td>
            <b>Tuner generates error when starting with the -uri argument</b>
            <br/>
            <i>
             When the Tuner is started with the -uri argument, an error is
             generated in the error log file.
             <br/>
             <b>Solution: The error is not generated anymore.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-4841 /<br> 12996
          </td>
          <td>
            <b>Crash in WaitSet_wait for Java application</b>
            <br/>
            <i>
             In some situations a java application could crash in a Waitset_wait.
             <br/>
             <b>Solution: The java api language binding has been rewritten. With
             this new implementation the code causing this crash is not present anymore.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-4383
          </td>
          <td>
            <b>Serializer serializes wrong double/float values when LC_NUMERIC
            is not equal to the default "C" locale.</b>
            <br/>
            <i>
             The LC_NUMERIC of the "C" locale is '.'. OpenSplice assumes doubles
             and floats to always have a '.' as LC_NUMERIC. But a few locales use
             a different LC_NUMERIC (like fr_FR and nl_NL use ','). The
             (de)serializing of doubles and floats is locale dependent and thus
             this LC_NUMERIC difference will cause problems when a different one
             is set on the system.
             <br/>
             <b>Solution: Make double/float to/from string conversions locale
             LC_NUMERIC independent.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5095/<br>13095
          </td>
          <td>
            <b>No convenient RMI API for non default DDS domain</b>
            <br/>
            <i>
             Most of the RMI features are available via the
             org.opensplice.DDS_RMI.DDS_Service class, but this class works on
             the default DDS domain only.
             <br/>
             <b>Solution: The Java and C++ RMI API has been extended so that the
             RMI applications can use any DDS domain id in a convenient way. As
             the CRuntime object is tied to the used DDS domain, a new method has
             been added to that class with the following signature : <br><br>
             DDS_ServiceInterface getDDS_ServiceInterface()<br><br>
             The DDS_ServiceInterface provides all the convenient methods to
             register/unregister services, run/shutdown them, and getting service
             proxies on the runtime-specific domain id, as does the DDS_Service
             class for the default domain. You can find a detailed description of
             that interface in the RMI API reference documentation.
             So, to use a non default DDS domain for RMI invocations, the
             developer should, first, get a CRuntime object for the wanted domain
             id as follows (in java):<br><br>
             CRuntime runtime = CRuntime.getRuntime(domain_id);<br>
             DDS_ServiceInterface dds_Service = defaultRuntime.getDDS_ServiceInterface();<br><br>
             then, dds_Service can be used to make the usual actions for client
             and server RMI applications.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5265
          </td>
          <td>
            <b>idlpp -j package prefix not applied to module-less types</b>
            <br/>
            <i>
             idlpp package prefixes specified using the -j option were not applied
             to module-less types.
             <br/>
             <b>Solution: The behaviour of idlpp changed in version 6.5 and
             prefixes are now properly applied.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5329
          </td>
          <td>
            <b>Windows installation path</b>
            <br/>
            <i>
             On 64bit windows version a 64bit version is installed in C:\Program Files (x86)
             <br/>
             <b>Solution: The 64 bit installation directory is changed to C:\Program Files\</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5424
          </td>
          <td>
            <b>Windows Registry Keys not cleared on uninstall</b>
            <br/>
            <i>
             Registry key entries in HKEY_CURRENT_USER under Software -&gt PrismTech
             -&gt OpenSpliceDDS -&gt Version were not cleared when uninstalling.
             This was due to a bug in the uninstaller that prevented it completing.
             This could result in a number of entries in the HKEY_CURRENT_USER
             if multiple versions of OpenSplice were installed.
             <br/>
             <b>Solution:  The bug in the uninstaller has been fixed and uninstall
             now completes correctly. The Registry key in HKEY_CURRENT_USER is
             now cleared for any new installations but it will not clear historical
             entries.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5605 / <br>14105
          </td>
          <td>
            <b>Unable to attach shared memory in Java8 applications on 64-bit Windows</b>
            <br/>
            <i>
             The OpenSplice shared-memory database needs to be mapped to the same
             address in all applications. On 32-bit and 64-bit Windows, the same
             default address is used. This caused problems with recent Java8 on
             64-bit platforms, where the default address is already occupied by the JVM.
             <br/>
             <b>Solution: The default address was changed to 0x100000000, to make
             the product work out-of-the-box on 64-bit Windows platforms that use
             a recent Java8 JVM. Note the address can be changed through the
             Domain/Database/Address configuration parameter when the default is
             unsuitable.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5678
          </td>
          <td>
            <b>Networking service crash when compiled with VS2012</b>
            <br/>
            <i>
             The networking service encountered a VS2012 C Runtime bug that let it crash:
             https://connect.microsoft.com/VisualStudio/feedback/details/782889/
             <br/>
             <b>Solution: Don't call strftime() with "%Z" when compiled with VS2012.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5697
          </td>
          <td>
            <b>Declaring multiple RMI operations having the same name in different
            interfaces lead to compilation errors of C++ generated code</b>
            <br/>
            <i>
             Declaring two or more RMI operations with the same name in different
             IDL interfaces makes the compilation of the generated C++ proxy
             classes fails because of a duplicate typedef declaration.
             <br/>
             <b>Solution: The RMI C++ code generator has been updated to declare
             the typedef in the scope of the related RMI interface proxy class
             instead of the global scope.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5789 /<br>14287
          </td>
          <td>
            <b>Wrong error code returned by ospl</b>
            <br/>
            <i>
             The error codes, returned by ospl, did not reflect the information
             within the usage help text of ospl. Also, extended error codes and
             a small behaviour change were required as well.
             <br/>
             <b>Solution: The normal ospl error codes have been improved. Also,
             the extended error codes will be returned when using the -e option.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5793 /<br>14284
          </td>
          <td>
            <b> Error missing field initializers in the OpenSpliceRMI header file
            dds_service_sdk.h</b>
            <br/>
            <i>
             gcc compilation could warn or fails (with -Werror=missing-field-initializers
             option) about missing filed initializers for member
             "DDS::ReliabilityQosPolicy::synchronous" in the RMI header file
             "include/rmi/dds_service_sdk.h"
             <br/>
             <b>Solution: The missing field initialization has been added.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5832 /<br> 14298
          </td>
          <td>
            <b>Termination issues while listener callback in progress.</b>
            <br/>
            <i>
             The listener callback implementation contains in concurrency issues
             which were difficult to resolve due to the internal design limitations.
             <br/>
             <b>Solution: The mechanism has been redesigned, moving control of
             the listener thread into the PSMs.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5844
          </td>
          <td>
            <b>DCPS Java 5 PSM does not use correct default domain id</b>
            <br/>
            <i>
             The internal default domainId is 0x7fffffff. In this case this
             domain id is used, the OSPL_URI environment variable is inspected
             and the domain id in there is used. The Java 5 PSM always uses 0 as
             default domainId. As a result setting OSPL_URI i.c.w. a non-zero
             domainId did not allow applications to create a DomainParticipant.
             <br/>
             <b>Solution:  In case no domainId is provided on creation of the
             DomainParticipant, OpenSplice checks the domainId in the OSPL_URI
             and uses the domainId specified there. This is not always 0 as
             specified in the Java5 PSM as default id. As our default config
             files that we ship always use 0, this is deemed acceptable
             behaviour.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5848 /<br> 14321
          </td>
          <td>
            <b>The durability service can crash i.c.w. dispose_all functionality</b>
            <br/>
            <i>
             The durability service could crash due to a null-pointer dereference
             in an internal algorithm related to the dispose_all functionality.
             <br/>
             <b>Solution: The internal algorithm has been modified to deal with
             dispose all properly as well.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5847 /<br> 14320
          </td>
          <td>
            <b>Unable to read sample that complies to readcondition</b>
            <br/>
            <i>
             Due to a locking problem in the product, a read call could return
             NO_DATA while actually there is data available
             <br/>
             <b>Solution: The issue has been resolved by adding a lock while
             freeing internal data</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5849
          </td>
          <td>
            <b>The Java5 DCPS implementation does not handle infinite duration
            correctly in Waitset.waitForConditions calls.</b>
            <br/>
            <i>
             The Java5 DCPS implementation internally converts durations to a
             different representation. In the Waitset.waitForConditions()
             operations the conversion algorithm was incorrect.
             <br/>
             <b>Solution: Fixed the conversion routine to deal with infinite
             durations correctly.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5880 /<br> 14330
          </td>
          <td>
            <b>Crash of RTSM tool</b>
            <br/>
            <i>
             The RTSM tool fails to attach to shared memory and crashes on a
             segmentation violation due to a mismatch in the layout of internal
             data-structures.
             <br/>
             <b>Solution: The issue was resolved and the code has been updated
             to prevent similar issues from occurring in the future</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5893 /<br> 14333
          </td>
          <td>
            <b>The durability service could crash when running out of shared memory</b>
            <br/>
            <i>
             The durability service did not check in all cases whether the
             allocation of a sample in shared memory succeeded. As a result it
             could dereference a null-pointer and crash.
             <br/>
             <b>Solution: The durability service did not check in all cases
             whether the allocation of a sample in shared memory succeeded. As a
             result it could dereference a null-pointer and crash.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5905
          </td>
          <td>
            <b>Selecting a network interface for DDSI2 using the network address
            fails on Linux</b>
            <br/>
            <i>
             The comparison of the specified address and the network interface
             address failed to take the network mask into account.
             <br/>
             <b>Solution: Code fix applied.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5910 /<br> 14339
          </td>
          <td>
            <b>A valgrind memcheck analysis of certain functionality reports:
            Invalid read of size 4</b>
            <br/>
            <i>
             The copy subscriber qos out function from the gapi subscriber is
             creating a normal string for the name in the qos. This should be a
             gapi object string, as it is also freed as an gapi object.
             <br/>
             <b>Solution: The copy out function is now creating the correct gapi
             object for the string.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5913
          </td>
          <td>
            <b>Transport priority in built-in topics for remote writers always 0
             when using DDSI</b>
            <br/>
            <i>
             The DDSI2 service generates built-in topics for the remote entities
             it discovers, but failed to correctly set the transport priority QoS
             in the CMDataWriter topic, leaving it at the default value instead.
             <br/>
             <b>Solution: DDSI2 now sets the transport priority.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5999
          </td>
          <td>
            <b>Tuner corrupts OSPL_URI when used with argument -uri=</b>
            <br/>
            <i>
             When starting the Tuner with the argument -uri=$OSPL_URI the OSPL_URI
             would be overwritten by the tuner property file.
             <br/>
             <b>Solution: The OSPL_URI is no longer overwritten.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6053/<br>14398
          </td>
          <td>
            <b>Errors reported about lack of multicast capability by networking
            service, when no multicast communication is configured</b>
            <br/>
            <i>
             The networking service checks multicast support when a partition is
             added to it's internal administration. Even when the
             network-partitions are configured to use broadcast or unicast, the
             service still performs this check, leading to error reports when it
             determines multicast is not supported.
             <br/>
             <b>Solution: The check was moved so it is only performed after
             determining the address is in fact a multicast address.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6069 / 6079<br>14409 / 14412
          </td>
          <td>
            <b>Missing RMI asynchronous replies in some cases and their memory cleanup</b>
            <br/>
            <i>
             In case of an OpenSpliceRMI Java application that invokes multiple
             asynchronous calls intensively, some of the asynchronous replies
             could be lost. Furthermore, every asynchronous request is uselessly
             kept in memory, which increases dramatically the memory footprint
             of the RMI server.
             <br/>
             <b>Solution:  A bug in the asynchronous replies management of Java
             OpenSplice RMI has been fixed. The Java RMI generated code has also
             been updated consequently. So, existing java RMI applications should
             re-generate their code using rmipp.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6119
          </td>
          <td>
            <b>DDSI can spontaneously change best-effort max_blocking_time</b>
            <br/>
            <i>
             DDSI tries to minimise the discovery traffic by leaving out all QoS's
             that are set to the default (this can be overridden using
             Compatibility/ExplicityPublishQosSetToDefault), relying on the peers'
             filling in the defaults again. The decision whether the reliability
             QoS was at the default value or not was not correctly accounting for
             the max_blocking_time part of it.
             <br/>
             <b>Solution: Reliability QoS value now set correctly.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6134
          </td>
          <td>
            <b>Documentation links in RTS don't work</b>
            <br/>
            <i>
             Documentation links in the RTS installer don't work as the target files
             are not available.
             <br/>
             <b>Solution: The RTS should not include the documentation, so it has
             been removed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6145
          </td>
          <td>
            <b>RnR replay crash</b>
            <br/>
            <i>
             Whenever RnR replay had to wait for the resources to become available
             or for the networking services to be ready to accept data for a
             partition/topic combination introduced by the replay, RnR could crash.
             <br/>
             <b>Solution: The issue has been fixed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6160
          </td>
          <td>
            <b>RnR replay fails when topic definitions are provided by recording</b>
            <br/>
            <i>
             RnR does not store/inject topics automatically, instead requiring the
             user to record and replay the DCPSTopic topic. Replaying a topic
             definition means that the topic becomes available once spliced has
             processed the sample, but this may be too late for the writer
             creation in RnR.
             <br/>
             <b>Solution: RnR now creates the topics synchronously.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6207
          </td>
          <td>
            <b>Report plugin robust against null pointer exception</b>
            <br/>
            <i>
             The report plugin could crash if it experienced a null pointer
             exception.
             <br/>
             <b>Solution: The plugin is now robust against the exception.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5524
          </td>
          <td>
            <b>Java Throughput example exception when running a 2nd publisher</b>
            <br/>
            <i>
             Due to an issue in the internal administration of the java
             subscriber-part of the throughput example, the subscriber would
             terminate with an exception in case the publisher is stopped and
             started for the second time.
             <br/>
             <b>Solution: The algorithm to update internal administration has
             been fixed.</b>
             </i>
           </td>
        </tr>
    </table>

    <h3>6.4.3p6</h3>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6075 / 14410
          </td>
          <td>
            <b>Concurrency issue while freeing signal-handler administration</b>
            <br/>
            <i>
             When a process detaches from the user-layer, which occurs
             automatically when a process terminates, it deinits the signal
             handler administration. Since it is still possible the process
             receives a signal at the same time, the signal handler thread may
             still be running and depending on the administration that is freed
             by the exit handler. A crash or mutex deadlock is often the result.
             <br/>
             <b>Solution: The issue has been resolved by removing the
             possibility of the administration being freed while still in use.
             </b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6184 / 14467
          </td>
          <td>
            <b>Missing event on data reader view query.</b>
            <br/>
            <i>
             Queries on data reader views could miss a trigger causing data not
             to be read.
             <br/>
             <b>Solution: The trigger mechanism is corrected and the data reader
             views are now always correctly updated.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6297 / 14516
          </td>
          <td>
            <b>Find topic timing behaviour incorrect.</b>
            <br/>
            <i>
             The find topic method on a DomainParticipant has a timeout
             parameter that specifies the maximum blocking time to wait for the
             topic to appear. In certain cases the method would block for 100 ms
             too long, which can have a noticeable impact on the application
             when called often on topics that don't yet exist.
             <br/>
             <b>Solution: The implementation was changed to never exceed the
             maximum timeout.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6298 / 14515
          </td>
          <td>
            <b>Crash when closing OpenSplice rtsm with ctrl+c</b>
            <br/>
            <i>
            The RTSM tool accesses the internal database to get information and
            statistics. When terminating RTSM with ctrl+c during such a period,
            then it'll corrupt the database and make the domain stop.
             <br/>
             <b>Solution: The signal is now caught and the handler detaches the
             tool properly from the database before quiting when needed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6350 / 14531
          </td>
          <td>
            <b>Deletion of entities while other threads are accessing causes a lot of exceptions</b>
            <br/>
            <i>
               Deletion of entities while other threads are accessing fails as
               a result of a race condition between unlocking and deleting an
               Entity.
             <br/>
             <b>Solution: Deletion of Entities while other threads are accessing
             the Entity is delayed until ongoing access has finished.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-184 / 14348
          </td>
          <td>
            <b>New instance not automatically displayed in tester</b>
            <br/>
            <i>
             In Tester, if an application data writer has a QoS of
             autodispose_unregistered_instances set to false and then
             unregister_instance is called on a data writer for some instance,
             a sample reaches matching data readers with the no_writers state
             and is ignored.
             <br/>
             <b>Solution: A new boolean option has been added in the Preferences
             menu under the Settings tab, called "Ignore not_alive_no_writers samples"
             and is set to true by default. If the option is set to false,
             then these specific samples will be displayed in Tester.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>TSTTOOL-192 / 14352
          </td>
          <td>
            <b>Ospltest gives errors on startup</b>
            <br/>
            <i>
             The script directories are not included in the RTS installers.
             <br/>
             <b>Solution: Updated scripts and install directories to fix these
             errors.</b>
             </i>
           </td>
        </tr>
       </table>
    <h3>6.4.3p5</h3>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6027 /<br> 14397
          </td>
          <td>
            <b>Crash of DataWriter for multiple generations of one instance</b>
            <br/>
            <i>
             When a DataWriter can't deliver messages because a peer has no
             resources to accept them, the writer will temporary store the
             messages in its history and try to deliver the messages at a later
             point in time. If for one instance multiple messages are 'delayed'
             for multiple generations of one instance then a crash may occur.
             This will only occur if a retry is able to deliver the first
             generation of the instance but not all messages of the newer
             generations. In this situation the system will detect that the
             instance of the first generation has ended and disconnect the
             writer unaware that more generations exist. As a consequence the
             DataWriter will crash when it tries to deliver the remaining messages.
             <br/>
             <b>Solution: he solution to this problem is that the DataWriter
             actively reconnects when it successfully delivered a 'delayed'
             Unregister message but was not able to deliver all messages of
             newer generations.</b>
             </i>
           </td>
        </tr>
      </table>
    <h3>6.4.3p4</h3>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5880 /<br> 14330
          </td>
          <td>
            <b>Crash of RTSM tool</b>
            <br/>
            <i>
             The RTSM tool fails to attach to shared memory and crashes on a
             segmentation violation due to a mismatch in the layout of internal
             data-structures.
             <br/>
             <b>Solution: The issue was resolved and the code has been updated
             to prevent similar issues from occurring in the future</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5847 /<br> 14320
          </td>
          <td>
            <b>Unable to read sample that complies to readcondition</b>
            <br/>
            <i>
             Due to a locking problem in the product, a read call could return
             NO_DATA while actually there is data available
             <br/>
             <b>Solution: The issue has been resolved by adding a lock while
             freeing internal data</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5936 /<br> 14137
          </td>
          <td>
            <b>Durability Service Alignment Improvement</b>
            <br/>
            <i>
             When a node becomes master it requests samples from all the fellows.
             The master will request data for the groups that it knows about.
             Data for groups that not known to the master are aligned later using
             a different and potentially slower code path, resulting in less
             efficiency alignment.
             <br/>
             <b>Solution:  If the master should request samples from a fellow and
             it does not have received all groups from this fellow yet, then it
             will first request the groups of the fellow in order to know as many
             groups as possible before requesting samples. That will result in
             more efficient alignment.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6036
          </td>
          <td>
            <b>Incorrect behaviour of shared DataReaders</b>
            <br/>
            <i>
             In case multiple shared datareaders are created (by setting the
             share QoS policy), in certain situations the internal administration
             could be freed while another shared reader still depends on it. This
             could lead to undefined behavior such as a crash, or even reading
             data of a different topic if the internal administration was reused
             for other readers on a busy system.
             <br/>
             <b>Solution: The issue was resolved by fixing a bug related to
             refcounting</b>
             </i>
           </td>
        </tr>
      </table>

    <h3>6.4.3p3</h3>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5602 / OSPL-5676/<br>14112 / 14137
          </td>
          <td>
            <b>Alignment of historical data intermittently fails in case multiple
            master conflicts simultaneously appear.</b>
            <br/>
            <i>
             When multiple durability services are started but communication
             between them is disabled they all operate in isolation. When suddenly
             communication between them is enabled multiple master conflicts
             appears. In an attempt to solve these conflicts alignment of data
             that was published takes place. When the volume of data that was
             published in isolation is large, the alignment can become massive.
             In some cases the alignment was flawed, leading to an incorrect
             end-state where different nodes have different views on the data
             that was published.
             <br/>
             <b>Solution: The administration to keep track of alignment data has
             been changed, so that data of the same partition/topic from different
             durability services are not mixed anymore. Furthermore, the alignment
             procedure has been adapted which leads to a more efficient alignment
             scheme involving less alignment data.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5817/<br>14293
          </td>
          <td>
            <b>RTNetworking service may crash on an interface status change</b>
            <br/>
            <i>
             Due to a lock being released twice on the detection of a change in
             the status of a monitored network interface, the RTNetworking
             service could crash.
             <br/>
             <b>Solution: The relevant locking has been revised and the
             double-unlock has been removed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5818/<br>14292
          </td>
          <td>
            <b>Starting ospl deamon from different user accounts could result
            in a delay of 10 seconds.</b>
            <br/>
            <i>
             When 2 different users are working on the same node and both are
             able to attach to the same domain, a user could experience a delay
             of 10 seconds when the domain was started. Also the process monitor
             was not working correctly in this use case. This was caused by
             connecting the same domain, but a different named communication socket.
             <br/>
             <b>Solution: The name of the communication socket used by the process
             monitor is now consistent with the name of the key file in the tmp
             directory.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5827/<br>14297
          </td>
          <td>
            <b>Regular reports leading to a large info log</b>
            <br/>
            <i>
             Some info reports that don't convey any particularly interesting
             information are logged on application start. In a situation where
             applications are frequently (re)started this could quickly lead to
             a large info log file.
             <br/>
             <b>Solution: The info reports about ignored signals and
             initialization of the user-clock-module have been removed</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5831/<br>14296
          </td>
          <td>
            <b>Unable to launch OpenSplice Tuner after installing RTS</b>
            <br/>
            <i>
             The script to launch the Tuner depends on another script, which
             wasn't included in the RTS but only in HDE installers. Trying to
             start the tuner triggers an error 'ospljre: command not found'.
             <br/>
             <b>Solution:  The RTS installer now includes the ospljre script
             so the tuner can be launched.</b>
             </i>
           </td>
        </tr>
        </tr>
          <td>OSPL-5835/<br>14299
          </td>
          <td>
            <b>Crash of durability service during termination</b>
            <br/>
            <i>
             In specific circumstances, when durability is terminated while it is
             resolving the master of a namespace, the service could fail on a
             mutex lock that was already freed, and would still try to unlock it
             which resulted in undefined behaviour and potential crash of the service.
             <br/>
             <b>Solution: The termination mechanism was revised to free internal
             administration in the correct order and only unlock the mutex when
             the lock was successful</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5851/<br>14295
          </td>
          <td>
            <b>Error log file is created if buildin topics are disabled</b>
            <br/>
            <i>
             When builtin topics are disabled an error log is created with the
             error: DataReader (name="DCPSParticipantReader") not created:
             Could not locate topic with name "DCPSParticipant"
             <br/>
             <b>Solution:  The DCPSParticipantReader is not created anymore if
             the builtin topics are disabled.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5854/<br>14323
          </td>
          <td>
            <b>Lack of reporting when incompatible meta-descriptor is registered.</b>
            <br/>
            <i>
             When type-support is registered by an application for a type that's
             already known, the declaration needs to match the existing declaration.
             When this is not the case, the registration fails and an indescriptive
             error is logged by the serializer.
             <br/>
             <b>Solution:  A report was added that refers to a declaration mismatch
             and also mentions the incompatible part, so the user can find and
             correct the corresponding IDL declaration.</b>
             </i>
           </td>
        </tr>
       </table>


    <h2>6.4.3p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5673/<br>14138
          </td>
          <td>
            <b>On VxWorks RTP large delays may occur, even for threads running
            at the highest priority.</b>
            <br/>
            <i>
             Priority inversion is the phenomenon where a high priority threads
             runs into a lock that is taken by a low priority thread, the high
             priority thread will not proceed until the lock is freed by the low
             priority thread. The remedy to deal with priority inversion is
             priority inheritance. Priority inheritance temporarily increases the
             priority of the low priority thread until the lock is freed, thus
             allowing the high priority thread to proceed. Although VxWorks
             provides native support for priority inheritance, OpenSplice did not
             benefit from it. This could cause large delays, even in threads
             running on the highest priority.
             <br/>
             <b>Solution:  Priority inheritance for mutexes (which are used to
             implement locks) is now enabled in OpenSplice for VxWorks RTP.
             Priority inheritance can be enabled by setting
             OpenSplice/Domain/PriorityInheritance in the configuration. Note
             that no priority inheritance for condition variables is supported.</b>
             </i>
           </td>
        </tr>
       </table>

    <h2>6.4.3p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5219/<br>13337
          </td>
          <td>
            <b>When the system is terminated while types are being registered a
            crash can occur.</b>
            <br/>
            <i>
             When a system is started, types that have been specified in the idl
             specification are being registered. If during the registration of
             these types the system is terminated a crash may occur. The crash
             is caused because the registration process is using references to type
             definitions that may have been deleted already by the termination
             thread.
             <br/>
             <b>Solution: References to type definitions are now properly
             protected so that it is not possible anymore to delete references
             to types that are still in use by another thread.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5742/<br>14164
          </td>
          <td>
            <b>Stale information in ospl artifact file causing ospl to exceed
            ServiceTerminatePeriod</b>
            <br/>
            <i>
             When the ospl tool is used in blocking mode (-f option) the artefact
             file is not properly (un)locked and updated under all conditions.
             Stale administration data could cause the ospl tool to exceed the
             ServiceTerminatePeriod when stopping a domain or report an incorrect
             warning when starting the domain.
             <br/>
             <b>Solution: The issue was resolved by making sure the artefact
             file is properly managed in blocking mode.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5770<br>
          </td>
          <td>
            <b>RnR may cause a warning by spliced about resources on termination</b>
            <br/>
            <i>
             The RnR service doesn't properly clean-up one of the writers it uses.
             This causes the safety mechanism of spliced to kick in after RnR has
             terminated.
             <br/>
             <b>Solution: The RnR service now properly cleans up the writer.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5772<br>
          </td>
          <td>
            <b>NetworkingBridge may cause a warning by spliced about resources
            on termination</b>
            <br/>
            <i>
             The NetworkingBridge doesn't properly inform spliced that it has
             terminated. This causes the safety mechanism of spliced to kick in.
             Because the NetworkingBridge actually did clean up its resources,
             spliced can always successfully clean up after the NetworkingBridge
             <br/>
             <b>Solution: The NetworkingBridge now properly informs spliced, so
             that the clean up routines and the superfluous report don't occur
             anymore.</b>
             </i>
           </td>
        </tr>
     </table>

    <h2>6.4.3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5083<br>
          </td>
          <td>
            <b>CMPartiticipant built-in topic extended with federation and vendor ids</b>
            <br/>
            <i>
             The existing CMParticipant built-in topic needs to be extended with
             federation and vendor ids.
             <br/>
             <b>Solution: The content of the CMParticipant built-in topic has
             been extended to include a string that may be used as a federation
             identifier. For Vortex Cafe, each process is considered a
             federation. For other vendors' products, the federation id is based
             on our current understanding of the identifiers used by them, and
             this may change as our understanding grows. Also included is the
             vendor id code assigned by the OMG to the various vendors for use
             in the DDSI protocol, thus allowing tooling to show the vendor or
             use vendor-specific knowledge. The vendor code consists of two
             unsigned integers separated by a decimal point. (The vendor code
             for OpenSplice Enterprise is "1.2".)</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5357<br>
          </td>
          <td>
            <b>Ignoring all topics in _BUILT-IN PARTITION_ in DDSI2E breaks all communication</b>
            <br/>
            <i>
             DDSI2E internally relies on a topic in the built-in partition, but
             failed to note the presence of this topic/partition when ignored.
             While it is possible to ignore just this topic/partition, in
             practice, it is most likely to happen when ignoring all topics in
             this partition. A work-around is to configure topics C* and D* in
             this partition, as this does not include this particular topic.
             <br/>
             <b>Solution: The detection of the presence of this topic/partition
             has been updated.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5430 /<br>
            13801
          </td>
          <td>
            <b>Reference to OMG ISO C++ specification missing in documentation</b>
            <br/>
            <i>
             There is no reference in the ISO C++ PSM documentation to the OMG
             ISO C++ PSM specification.
             <br/>
             <b>Solution: A link to the OMG spec has been added.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5688<br>
          </td>
          <td>
            <b>Globally unique systemId needs to be generated with more care.</b>
            <br/>
            <i>
             Each federation generates its own id at start-up, which must be
             unique in the system. Sometimes id's could turn out to be the same
             causing undefined behaviour.
             <br/>
             <b>Solution: Unique system id generation has been improved to prevent
             duplicates when two copies of opensplice are started simulateously
             on the same linux or windows node.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5694/<br>14141
          </td>
          <td>
            <b>Insufficient checking in java native marshalling routines</b>
            <br/>
            <i>
             When a sample with uninitialized members of type union or enum is
             written in the Java PSM, the JVM may crash instead of receiving a
             proper error return code. Note that members are always initialized
             by default in the code generated by idlpp, but it is possible for
             the application to assign null to a member after initialization.
             <br/>
             <b>Solution: The marshalling routines were changed to return a
             BAD_PARAMETER code when an uninitialized member is processed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5740 /<br>
          14163
          </td>
          <td>
            <b>An incorrect error is logged when a library fails to load</b>
            <br/>
            <i>
             An incorrect error is logged when a library, such as a
             report-plugin, fails to load library names. Libraries (i.e. the report
             plugin), can be entered in the configuration file in a platform
             agnostic manner. OpenSplice will translate the name and when the
             library fails to load runs a fall-back mechanism to load the
             original name. In this process, details on the failure were lost.
             <br/>
             <b>Solution: The product has been changed to record a proper error
             message to the OpenSplice error log when a library fails to load.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5743 /<br>
          14165
          </td>
          <td>
            <b>The durability service could crash in case a namespace to a
            fellow is added for which no aligner exists.</b>
            <br/>
            <i>
             When a durability service receives a namespace for a fellow it adds
             the namespace for this fellow to the internal administration. Part
             of this administration is the merge state of the namespace. When no
             aligner for the namespace is known the merge state is NULL. Due to
             a bug setting a NULL value for the merge state would lead to a crash.
             <br/>
             <b>Solution: The code that deals with setting merge states of
             namespaces has been changed so that no crash occurs anymore.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5744<br>
          </td>
          <td>
            <b>Classic Java PSM QosProvider get_participant_qos() may crash</b>
            <br/>
            <i>
             When using the Java QosProvider in combination with get_participant_qos
             with a non null id the JVM could crash.
             <br/>
             <b>Solution: The problem in get_participant_qos is now fixed and the
             JVM will not crash anymore on a non null id. </b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5745<br>
          </td>
          <td>
            <b>autopurge_disposed_samples_delay zero is not instantaneous</b>
            <br/>
            <i>
             When autopurge_disposed_samples_delay is zero, then the purge is
             not instantaneous. It will be purged only after the monotonic clock
             has progressed at least one tick.
             <br/>
             <b>Solution: This is solved by changing a timing check in the purge
             handling from 'larger than' to 'equal or larger than'.</b>
             </i>
           </td>
        </tr>
       </table>
    <h2>6.4.2p5</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5616<br>
          </td>
          <td>
            <b>Added support for shared library builds on vxworks RTP</b>
            <br/>
            <i>
          Shared library support required for VxWorks RTP use on Pentium4 and E500V2.
               <br/>
               <b>Solution: Added shared library support for VxWorks RTP.
              Due to symbol table restrictions with the GNU toolchain the ddskernel library has been split into ddskernel and ddskernel2 for PPC Shared libraries.</b>
             </i>
           </td>
        </tr>
       </table>
    <h2>6.4.2p4</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1167/<br>10824
          </td>
          <td>
            <b>The ospl tool has wrong exit and status codes</b>
            <br/>
            <i>
               Regarding the status of a domain, the ospl tool returns wrong exit
               codes and depicts wrong status codes when listing domains.
               <br/>
               <b>Solution: The ospl tool has been extended with a status file
               that contains the states of the available domains.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5374/<br/>13672
          </td>
          <td>
          <b>Issues with RT Networking CPU usage when Record and Replay service
             is enabled.</b>
            <br/>
            <i>
               An issue in the Record and Replay service in certain circumstances
               could result in native networking using up all of the cpu resources.
               When a recording is stopped, Record and Replay stops reading
               samples matching the record interest expressions, but networking
               continues to deliver these samples until storage resources are
               exhausted. When exhausted, networking anticipates on resources
               becoming available again, and continues attempted delivery at an
               increased rate, resulting in cpu exhaustion.
               <br/>
               <b>Solution: A bug was found and resolved so record interest is
               properly disposed of by Record and Replay, after which networking
               stops delivering samples that are never read by Record and Replay.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5615/<br/>14114
          </td>
          <td>
            <b>The networking service may crash when topics with a name exceeding
            64 bits are used.</b>
            <br/>
            <i>
               The networking service uses an internal buffer to store the topic
               names associated with received messages. Initially this buffer is
               64 bits wide. When a topic name larger than 64bit is received the
               buffer should be increased in size accordingly. However it may
               occur that not enough memory is allocated which causes memory
               corruption to occur.
               <br/>
               <b>Solution: The issue is fixed by always allocating enough buffer
               space when a topic name is received which has a size larger than
               the current buffer size.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5387/<br/>13669
          </td>
          <td>
            <b>Non-default presentation QoS incorrectly refused by product.</b>
            <br/>
            <i>
               The middleware did not accept a publisher or subscriber QoS on
               which the presentation was set to instance scope with ordered_access
               enabled. This caused inter-operability issues with other DDS vendors,
               while in fact the implementation does by default support ordered
               access on instance level.
               <br/>
               <b>Solution: The restriction has been lifted so that publishers
               and subscribers can now be created with enabled ordered_access
               setting, as long as the scope is set to instance.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5482/<br/>14005
          </td>
          <td>
            <b>Crash of ddsi service when a DataReader with SubscriptionKey QoS
               policy is used.</b>
            <br/>
            <i>
               Management of builtin topics by the DDSI service contained a bug
               that could potentially crash the service when a builtin-topic
               sample is created for a DataReader that has the
               (OpenSplice-specific) subscription key QoS policy.
               <br/>
               <b>Solution: The implementation was fixed to correctly handle the
               subscription key policy.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5552<br/>
          </td>
          <td>
            <b>OSPL_HOME may not be set correctly when using an archived build</b>
            <br/>
            <i>
               Builds delivered in an archived format would still contain the
               installer macros to be expanded at install time, without the
               installer these macros would remain and cause the release.com to
               set an invalid OSPL_HOME.
               <br/>
               <b>Solution: The release.com now attempts to set OSPL_HOME using
               Bash when not using an installer. For users without Bash,
               a message will be presented expecting them to manually adapt
               the release.com with a valid OSPL_HOME.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5627/<br/>14118
          </td>
          <td>
            <b>Possible crash after exception handler has cleaned up resources.</b>
            <br/>
            <i>
               There was an issue with the exception handler cleaning up used
               resources which where still in use by the lease and resend managers.
               <br/>
               <b>Solution: Before the exception handler frees the used resources,
               first stop the lease manager and resend manager.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5657/<br/>14126
          </td>
          <td>
            <b>Bounds checking error on IDL sequences with #pragma stac</b>
            <br/>
            <i>
               When #pragma stac is applied to all members of a struct, it would
               also be applied to sequences in case the sequence contains strings
               (or a type that resolves to string). However the code generated by
               idlpp would not correctly handle this, leading to errors when a
               sample is published and bounds checking is enabled.
               <br/>
               <b>Solution: There is no real performance benefit in applying stac
               transformation to sequence elements so the pragma is now ignored
               for struct members of type sequence.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5660/<br/>14129
          </td>
          <td>
            <b>The durability service could crash when the service is terminating</b>
            <br/>
            <i>
               When the durability service is terminating it tries to clean up its
               resources. One of these resources is the fellow administration.
               When a fellow is being removed from the administration because it
               failed to updated its lease in time while at the same time
               durability is terminating, it is possible that durability tries to
               reference a fellow that has already been freed. This leads to a
               crash.
               <br/>
               <b>Solution: References to fellows are properly counted, and the
               fellow is only freed when no other threads keep a reference to the
               fellow object.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5675/<br/>14139
          </td>
          <td>
            <b>Open dynamic loaded libraries with RTLD_NOW.</b>
            <br/>
            <i>
               Dynamic loaded libraries were opened with RTLD_LAZY, which caused
               problems with external library loading (such as report libraries).
               <br/>
               <b>Solution: Dynamic loaded libraries are now opened with RTLD_NOW.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5693<br/>
          </td>
          <td>
            <b>Durability servers that operate in different roles and detect
            conflicting states for a namespace might handle the conflict wrongly,
            possibly resulting in a crash of durability.</b>
            <br/>
            <i>
               When different durability servers take responsibility of the same
               namespace but for different roles, merge policies can be applied
               to resolve these conflicts. Due to an error in the condition to
               resolve the conflict it is possible that a different merge policy
               is applied than specified. Also, when the correct merge policy is
               applied a crash could occur due to an attempt to access an object
               that has already been freed.
               <br/>
               <b>Solution: The condition to resolve the conflict has been
               changed s that the correct merge policies are triggered. Also,
               the crash has been prevented by properly refcounting the object.</b>
             </i>
           </td>
        </tr>
       </table>


    <h2>6.4.2p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4944<br>
          </td>
          <td>
            <b>ISO C++ documentation improved</b>
            <br/>
            <i>
               <br/>
               <b>Solution: Resolved problems with API being described, added code
               samples, added new API descriptions.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5194<br>
          </td>
          <td>
            <b>VxWorks RTP version now has descriptive thread names</b>
            <br/>
            <i>
               <br/>
               <b>Solution: Added descriptive names to the OpenSplice threads on
               the VxWorks RTP builds.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5338/<br/>13592
          </td>
          <td>
          <b>Writing samples from the DCPS Java API can result in an overflow
            of the internal references table of the JVM.</b>
            <br/>
            <i>
               During a write, a lot of Java object references can be created,
               depending on the type of a topic. Though the JNI specification
               allows only 16 references, in practice there were never any
               issues on Oracle JVM with using more references. Therefore the
               product did not explicitly delete references in favor of
               performance benefits during the write call. The PERC JVM however
               does not allow this relaxation of the JNI spec. overflowing the
               references table results in memory corruption.
               <br/>
               <b>Solution: The Java PSM (JNI layer) was changed, to free unused
               references so the table cannot overflow.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5615/<br/>14114
          </td>
          <td>
            <b>The networking service may crash when topics with a name exceeding
            64 bits are used.</b>
            <br/>
            <i>
               The networking service uses an internal buffer to store the topic
               names associated with received messages. Initially this buffer is
               64 bits wide. When a topic name larger than 64bit is received the
               buffer should be increased in size accordingly. However it may
               occur that not enough memory is allocated which causes memory
               corruption to occur.
               <br/>
               <b>Solution: The issue is fixed by always allocating enough buffer
               space when a topic name is received which has a size larger than
               the current buffer size.</b>
             </i>
           </td>
        </tr>
       </table>


       <h2>6.4.2p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5220/<br/>13333
          </td>
          <td>
            <b>Unfair claim of ownership by unregister message</b>
            <br/>
            <i>
                 Unregister messages could claim ownership of an instance and in
               combination with the deadline QoS and liveliness lost, this caused
               data reception 'gaps' when another writer with lower strength
               had already taken over.
               <br/>
               <b>Solution: Unregister messages will not claim ownership.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5228/<br/>13336
          </td>
          <td>
            <b>When the durability service terminates there is a possibility that
            the durability service crashes</b>
            <br/>
            <i>
                 When the durability service terminates it will clean up all its
               resources. While doing so there is a possibility that the action
               queue is already destroyed while another thread still tries to
               access the action queue. This situation could lead to a crash.
               <br/>
               <b>Solution: Before cleaning up most threads are stopped. This
               prevents that a thread accesses a piece of memory that has been
               freed by another process. Also, the order to cleanup resources has
               been changed so that the action queue is destroyed AFTER all
               threads that may use it are stopped. And finally initialization
               and deinitialization of objects in the durability service has been
               improved.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5361/<br/>13599
          </td>
          <td>
            <b>Difficulty determining if a Record and Replay service is finished
            replaying samples</b>
            <br/>
            <i>
                 In case all samples in a storage were replayed, the Record and
               Replay service would continue to poll the storage in case new
               samples were recorded, in order to replay them. This meant the
               storage remained open and this 'polling state' was not discernible
               by monitoring the (storage) status topic.
               <br/>
               <b>Solution: The behavior was changed to only enter the polling
               state in case a storage is used for recording as well, at the
               time the last sample is replayed. In case a storage is not used
               for recording, all replay-interest is removed, the storage is
               closed and a corresponding storage-status sample is published.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5419/<br/>13594
          </td>
          <td>
            <b>Liveliness detection and synchronization problem.</b>
            <br/>
            <i>
                 When disconnecting a node, then the liveliness changed is not always
               triggered on the remaining node when using exclusive ownership.
               When the liveliness changed is triggered, then the instance state
               is (often) still 'alive' when 'not alive' is expected.
               <br/>
               <b>Solution: Messages of low strength writers in a exclusive
               ownership setup are not handled. By not ignoring the 'unregister
                message of such a low strength writer, the liveliness is properly
                decreased. Also the the liveliness changed is now triggered after
                the related instance states have been set to 'not alive'.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5459/<br/>13924
          </td>
          <td>
            <b>With a (default) umask of 0022 different users on the same node
            interact with each other on a posix system.</b>
            <br/>
            <i>
                 When 2 different users are working on the same node with a umask
               setting of 0022 the splice deamon will attach to the same shared
               memory segment. This is caused by the key file with user rights
               set to 666 with the key for the shared memory to attach to.
               <br/>
               <b>Solution:  If the umask gives only read or write rights to a
               user/group/others then the key file gets no rights at all for
               that part. This will result in a key file with user rights 600
               on a default umask of 0022.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5468/<br/>13933
          </td>
          <td>
            <b>Customer code application build problem with 6.4.2</b>
            <br/>
            <i>
                 netdb.h system header file clashed with a symbol when building
               customer code.
               <br/>
               <b>Solution: Avoided an issue with conflicting symbols by not
               including the netdb.h system header file when building customer code.</b>
             </i>
           </td>
        </tr>
       </table>


       <h2>6.4.2p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5365/<br/>12971
          </td>
          <td>
            <b>Fixed startup failure with DDSI if configured to run using a single UDP unicast port</b>
            <br/>
            <i>
                 DDSI would not start when configured to use a single UDP unicast port.
               <br/>
               <b>Solution: Behaviour fixed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5413<br/>
          </td>
          <td>
            <b>When using DDSI, the "dispose all" command was transmitted
            best-effort</b>
            <br/>
            <i>
                 The QoS used for publishing a "dispose all" command throughout the
               domain caused it to be sent best-effort when using DDSI, creating
               the possibility of it reaching only a subset of the nodes. The
               different design of RT networking ensured that systems based o
               RT networking did not run this risk.
               <br/>
               <b>Solution: The QoS has been changed to reliable.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5416<br/>
          </td>
          <td>
            <b>The DCPSHeartbeat writers should be best-effort</b>
            <br/>
            <i>
                 For the OpenSplice-specific DCPSHeartbeat built-in topic, best-effort
               relability suffices. In OSPL V6.4.1 version it was changed to a reliable
               writer, which slightly affects behaviour when used with the RT
               networking protocol, in cases where the network is overloaded or
               very unreliable, as delivery of the DCPSHeartbeat may be blocked
               by preceding messages.
               <br/>
               <b>Solution: The DCPSHeartbeat writer QoS is once again best-effort</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5450<br/>
          </td>
          <td>
            <b>Deserialiser can incorrectly reject valid input because of an erroneous bounds check</b>
            <br/>
            <i>
                 An issue in the CDR deserialiser can cause a valid input to be rejected by a
                 sequence bounds check. The affected components are DDSI,
                 durability with a KV persistent store,
                 RnR with binary storage, and RT networking when using compression (not the legacy compression).
               <br/>
               <b>Solution: The check has been fixed.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5460<br/>
          </td>
          <td>
            <b>Potential misaligned access in the CDR deserialiser for 64-bit objects</b>
            <br/>
            <i>
                 The CDR deserialiser could access 64-bit objects without ensuring the access
                 is properly aligned. On most platforms, and most notably on the x86 and x64
                 platforms, such misaligned accesses are entirely legal, but on some platforms,
                 they cause a misaligned access exception.
               <br/>
               <b>Solution: The CDR deserialiser now avoids misaligned accesses.</b>
             </i>
           </td>
         </tr>
       </table>


       <h2>6.4.2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-3957<br/>
          </td>
          <td>
            <b>OpenSplice should support hibernation</b>
            <br/>
            <i>
               Modern platforms support the concept of hibernation and resuming.
               When hibernating, all processes are suspended and the complete
               RAM is written to permanent storage and during resuming that
               information is written back into RAM again with the purpose to
               have all processes continue where they left off before hibernation.
               However, during the period a system is hibernated, time elapses and
               as a result software may face time jumps when resuming again.
               OpenSplice is not able to cope with these time jumps, resulting in
               the potential termination of services or even a complete shut-down
               of the middleware. OpenSplice needs to be able to cope with
               hibernation to allow the product to be used in environments that
               rely on that functionality as well.
               <br/>
               <b>Solution: The various notions of time have been updated
               throughout the entire product allowing it to cope with time jumps
               as well as resuming after hibernate/suspend.</b>
             </i>
           </td>
         </tr>
         <tr>
           <td>OSPL-4196<br/>
           </td>
           <td>
             <b>Timestamps on WinCE aren't guaranteed to be represented in UTC</b>
             <br/>
             <i>
               OpenSplice internally uses the WinCE GetLocalTime() operation,
               which returns time in local time-zone. Depending on time-settings
               of the operating system, this time may not match UTC, which is
               used on other platforms.
               <br/>
               <b>Solution: The implementation now ensures the time is
               represented in UTC on WinCE as well.</b>
             </i>
           </td>
          </tr>
          <tr>
          <td>OSPL-4325<br/>
          </td>
          <td>
            <b>Error log about failure to remove DBF file on Windows after domain shutdown</b>
            <br/>
            <i>
               An issue with termination of OpenSplice services could result in
               a failure to remove the database (.DBF) file and corresponding
               message logged to the ospl-error logfile.
              </br>
              </br>
              <b>Solution:  The termination issue was resolved so that the
              database-file can be removed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4513/ OSPL-5285/<br/>12875/13407
          </td>
          <td>
            <b>Shared Memory not detecting terminated or killed processes</b>
            <br/>
            <i>
               Terminated or killed processes on Windows are not detected,
               which may lead to corrupt shared memory and will then not update
               the liveliness state of writers.
              </br>
              </br>
              <b>Solution:  Updated the implementation of the shared memory monitor
               for Windows, using specific Windows API calls. Now the termination
               of a process is detected by the Splice daemon and proper action
               is taken to clean up after termination of the process. This might
               lead to a shutdown of Splice in case the terminated process was
               modifying shared memory when terminated, as shared memory is
               corrupted in this case.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4590/<br/>12947
          </td>
          <td>
            <b>When durability terminates, the durability service should try to
            persist as much data as possible in case the persistent data queue
            still contains samples to persist.</b>
            <br/>
            <i>
               Until now a durability service that terminates does not store
               persistent data that is waiting to be persisted. An improvement to
               this behavior is to try and persist as much of the remaining data
               as possible without exceeding the ServiceTerminatePeriod. Persisting
               as much data as possible is a best-effort attempt to save valuable
               data in case a durability service is terminated.
              </br>
              </br>
              <b>Solution:  The durability service uses part of the
              ServiceTerminatePeriod to store remaining data that has not yet
              been stored at the time the durability service starts to terminate.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4695/OSPL-5262/<br/>13414
          </td>
          <td>
            <b>RMI: Interface unregistration problem</b>
            <br/>
            <i>
               When an interface is unregistered then the runtime is shutdown,
               a NullPointerException was raised, and any attempt to register
               this same interface a second time fails and leads to
               "Interface X already registered".
              </br>
              </br>
              <b>Solution:  The interface is now properly removed from the
              runtime interface registry.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4696<br>
          </td>
          <td>
            <b>Restarting an RMI runtime causes failure</b>
            <br/>
            <i>
               On Runtime stop, in CInterfaceRegistryImpl.java, the m_Reader
               reader is closed (directly or indirectly), but the field is not
               set to null. So after restart, the CInterfaceRegistry tries to
               resuse the reader with no success.
               <br/>
               <b>Solution: Fixed the m_Reader in clear method.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-4871<br/>
          </td>
          <td>
            <b>Reference guide update for WriterDataLifecycleQosPolicy</b>
            <br/>
            <i>
               WriterDataLifecycleQosPolicy missing autounregister_instance_delay
               and autopurge_suspended_samples_delay attributes.
              </br>
              </br>
              <b>Solution:  All reference guides have been updated with the
              descriptions.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4960<br/>
          </td>
          <td>
            <b>When Node Monitor is started it should publish all the enabled samples immediately</b>
            <br/>
            <i>
               When Node Monitor is started, the NodeStats and NodeInfoConfig
               writers should publish all the enabled samples immediately and have
               their DURABILITY QoS set to non-VOLATILE so that late-joiner
               DataReader configured also to non-VOLATILE to get the last sample
               per key, rather than wait (potentially for a long time) until all
               the intervals have elapsed after nodemon startup.
              </br>
              </br>
              <b>Solution: The NodeStats and NodeInfoConfig topics' durability
              QoS policy kind was changed from V_DURABILITY_VOLATILE to V_DURABILITY_TRANSIENT.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5146/<br/>13108
          </td>
           <td>
            <b>Not all signals properly handled when using pthread_kill(...)</b>
            <br/>
            <i>
              When using pthread_kill(...), for example to abort a process, the
              process would continue to run.
              </br>
              </br>
              <b>Solution:  The signalhandler has been modified so that signals
              generated with pthread_kill(...) are properly handled too.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5147<br/>
          </td>
          <td>
            <b>User should be aware that a runtime installation of OpenSSL is
            required for OpenSplice licensed features and/or ddsi2e and snetworking -
            an update</b>
            <br/>
            <i>
              Addition of TLS in ddsi2 removes the static link to OpenSSL in previous
              versions of OpenSplice on non-windows systems.
              </br>
              </br>
              <b>Solution:  The requirement for the OpenSSL runtime now only applies
              to ddsi2e and snetworking on non-windows systems.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5190<br/>13188
          </td>
          <td>
            <b>Disabling the Java shutdownHook</b>
            <br/>
            <i>
              <b>Solution:  The Java shutdownhook, used to clean up all created
              entities that have been created by a java application, but have not
              been cleaned up during the execution of the application can be
              disabled by a new introduced system environment property:
              "osplNoDcpsShutdownHook". See java reference manual for more
              details.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5244/<br/>13406
          </td>
          <td>
            <b>Waitset associated with wrong DomainParticipant causes problems during clean-up</b>
            <br/>
            <i>
               In case an application has multiple DomainParticipants participating
               in the same Domain and attaches a Status-, Read- or QueryCondition
               to a Waitset, the Waitset may be associated with the wrong
               DomainParticipant, because the algorithm to select one did it
               based on the DomainId instead of looking at the DomainParticipant
               associated with the Condition. This may cause problems when
               deleting one or both DomainParticipants and even lead to a crash
               of the application in some cases.
              </br>
              </br>
              <b>Solution:  The DomainParticipant is now selected based on the
              DomainParticipant associated with the Condition.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5266<br/>
          </td>
          <td>
            <b>DDSI2 warns about a message without payload</b>
            <br/>
            <i>
               DDSI2 used to emit warnings of the form "write without proper
               payload (data_smhdr_flags 0x2 size 0)" when receiving messages
               from a writer that have no content whatsoever. Such messages are
               allowed by the specification and hence should not result in a warning.
              </br>
              </br>
              <b>Solution:  The warning has been removed, it is still logged in the trace.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5268<br/>
          </td>
          <td>
            <b>C-language binding used wrong type for the "subscription_keys" of
            the CMDataReader built-in topic</b>
            <br/>
            <i>
               The C language binding accidentally used a sequence of strings
               where it should have been a single string to describe the
               subscriber-defined keys. This caused crashes for a C program
               trying to use the CMDataReader built-in topic, and additionally caused
               the C binding to differ from the other language bindings.
              </br>
              </br>
              <b>Solution:  The type has been changed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5321<br/>
          </td>
          <td>
            <b>Issue with DSCP SAC code-generation</b>
            <br/>
            <i>
               The code-generation templates for typed DataReaders and DataWriters
               contained an error in the definition of
               'FooDataReader_get_subscription_matched_status' and
               'FooDataWriter_get_publication_matched_status' methods. Since
               these methods were not available users were forced to use the
               regular DDS_-prefixed methods, resulting in inconsistent code.
              </br>
              </br>
              <b>Solution:  The issue has been solved and the correct
              definitions are now generated by idlpp</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5335<br/>
          </td>
          <td>
            <b>DDSI can flag dispose/unregister messages from Caf� for CM topics as invalid</b>
            <br/>
            <i>
               DDSI2 is designed to handle data containined "proper" payload, but
               some DDSI implementations in some cases do not provide a real payload,
               but only an alternative form of the key. DDSI2 translates these
               to well-formed payloads before interpreting them, but this
               translation was incorrect for some CM topics.
              </br>
              </br>
              <b>Solution:  The translation table has updated to cover all cases.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5406/<br/>13796
          </td>
          <td>
            <b>Unable to find an entry point in C# API</b>
            <br/>
            <i>
               When using a call like GetDiscoveredParticipants in the C# API, an
               "Unable to find an entry point named 'u_instanceHandleNew' in DLL
               'ddsuser'." exception could be thrown.
              </br>
              </br>
              <b>Solution:   This issue has now been fixed by referring the entry
              point to the correct library.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5411<br/>
          </td>
          <td>
            <b>RT networking interoperability fix due to ospl-4345 (6.4.1) fix</b>
            <br/>
            <i>
               At 6.4.1, RT networking interoperability with older versions was degraded.
              </br>
              </br>
              <b>Solution:   RT networking now correctly handle version prior to 6.4.1.</b>
            </i>
          </td>
        </tr>


       </table>




       <h2>6.4.1p9</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-3216<br/>
          </td>
          <td>
            <b>Built-in CMParticipant Topic accessibility.</b>
            <br/>
            <i>
               The CMParticipant built-in Topic should be accessible, to
               applications, through a built-in DataReader.
              </br>
              </br>
              <b>Solution:  The CMParticipant built-in Topic and DataReader are
              added to all language bindings.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4157/12727<br/>
          </td>
          <td>
            <b>C DCPS generated API doesn't compile with g++</b>
            <br/>
            <i>
               The C API no longer compiles when using g++
              </br>
              </br>
              <b>Solution:  The fault has been fixed and g++ will now compile the C API again.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4480/<br/>12870
          </td>
          <td>
            <b>Upon repeated stop/start of OpenSplice, the application received
            the error "Max connected Domains (127) reached"</b>
            <br/>
            <i>
               In our user layer the connected domains are stored in an array
               of 127 items. When a domain is connected, the next entry is used,
               skipping entries that have been freed by a domain disconnect. After
               127 connects, the end of the array is reached, which results in this error.
              </br>
              </br>
              <b>Solution:  When a domain is disconnected, the entry in the array
              will be marked free. When a new domain is connected, the array will
              be searched from the beginning for a free entry, re-using locations
              that were in used and were freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5192/<br/>13191
          </td>
          <td>
            <b>When the replace merge policy is invoked not all data is being replaced.</b>
            <br/>
            <i>
               When a fellow durability service that acts as master for a set of
               namespace leaves, a check is needed to see if an alternative aligner
               is available for each of these namespaces. If no alternative
               aligner is found the merge state for the namespace must be cleared
               to ensure that a merge action is triggered as soon as a new aligner
               joins the network. This check should be carried out for all namespaces.
               However, once no alternative for a namespace was detected the other
               namespaces were (wrongly) not checked anymore, resulting in the
               fact that the merge states for these namespaces are not cleared,
               and no merge policy is triggered for these namespaces.
              </br>
              </br>
              <b>Solution:  The code has been changed so that an alternative
              aligner is searched for all namespaces of the leaving fellow.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5197<br/>
          </td>
          <td>
            <b>CMSoap service can fail to terminate cleanly</b>
            <br/>
            <i>
               The timeout handling in accepting new conditions in the cmsoap
               service could cause cmsoap to fail to terminate cleanly, instead
               automatically killing itself after the (configurable) service
               termination period.
              </br>
              </br>
              <b>Solution:  The timeout specification has been updated to avoid this issue</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5202/<br/>13197
          </td>
          <td>
            <b>Tuner does not accept character code input for c_char fields.</b>
            <br/>
            <i>
               If a user wanted to write a character value for a c_char field
               that was not found on the keyboard (like the NUL character or the
               LF character) there was no way to input it in the Tuner writer window.
              </br>
              </br>
              <b>Solution:  The Tuner writer input for c_char fields now accepts
              octal character codes (eg. \000 for NUL, \012 for LF, \141 for 'a',
              etc).</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5234<br/>
          </td>
          <td>
            <b>Possible crash of networking service when running out of DefragBuffers</b>
            <br/>
            <i>
               The networking service could crash when it ran out of DefragBuffers
               and its garbage collector started releasing DefragBuffers. The crash
               happened when the garbage collector double released DefragBuffers
               that were still in use.
              </br>
              </br>
              <b>Solution:  The garbage collector no longer double releases in use DefragBuffers.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5239<br/>
          </td>
          <td>
            <b>DDSI could crash when a thread is killed</b>
            <br/>
            <i>
               When a thread was killed, where the participant was already deleted
               DDSI could crash trying to get subscriber/publisher out of this participant.
              </br>
              </br>
              <b>Solution:  Before trying to access the subscribers or publisher,
              check if the participant is still valid.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5251/<br/>13405
          </td>
          <td>
            <b>Using synchronous reliability could cause a crash due to a memory corruption</b>
            <br/>
            <i>
               Due to missing locking on a part of the synchronous reliability
               administration, the memory could become corrupted causing a crash
              </br>
              </br>
              <b>Solution:  Locking has been added to the relevant bits of the
              synchronous reliability administration.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5253<br/>
          </td>
          <td>
            <b>Recompilation rules broken for Java applications</b>
            <br/>
            <i>
               Java applications generated before V6.4.0p5 wouldn't run on
               later versions and required code to be regenerated and compiled.
               This was mentioned in the release notes (OSPL-4333).
              </br>
              </br>
              <b>Solution:  An overloaded constructor has been added that supports
               the old format of generated code, allowing the applications to
               be used according the the recompilation rules.</b>
            </i>
          </td>
        </tr>


       </table>

       <h2>6.4.1p8</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4614 / 12392<br/>
          </td>
          <td>
            <b>In some situations it is possible that a durability service
            processes pending messages from another durability service
            (a fellow) that has been removed recently, causing the fellow to
            be added again.</b>
            <br/>
            <i>
               When a durability service is busy it cannot process any incoming
               message from a fellow immediately. In case a message from a
               fellow is received and the fellow is terminated before the
               message is processed, then the fellow will be removed as peer
               from the durability service. But when the pending message is
               processed the durability service notices that this came from a
               fellow that is not known, and will (wrongly) added again.
              </br>
              </br>
              <b>Solution: When a fellow has terminated, its address will be
              remembered for some while. Messages originating from a fellow
              with an address that has recently terminated will not processed,
              thereby preventing that a 'new' fellow is added.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4985 / 13053<br/>
          </td>
          <td>
            <b>When multiple master conflicts appear at the same time, only one
            of them is handled, resulting in incorrect merges of historical
            data.</b>
            <br/>
            <i>
               In some cases it is possible that suddenly multiple nodes appear
               that are all master for the same namespace. This is for example
               the case when a firewall initially blocks traffic between 3 nodes
               A, B and C that all become master. When the firewall is disabled
               (thus enabling communication between all nodes) each nodes has 2
               possible master conflicts. Both of these conflicts should be
               handled to ensure that data is correctly merged.
              </br>
              </br>
              <b>Solution: No master conflicts are dropped anymore, and
              successive master conflicts are being re-evaluated because they
              may have been invalidated by resolving previous master conflicts.
              </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5125<br/>
          </td>
          <td>
            <b>Termination of DDSI2 in shared memory deployments on Windows
            causes warnings</b>
            <br/>
            <i>
               DDSI2 creates various objects for its internal administration and
               its interaction with the OpenSplice kernel. Some of these were
               not released in the terminated path, causing the DDSI2 domain
               participant to not be deleted at the expected time because there
               were unexpected outstanding references to it. This would then
               lead to an apparent crash of DDSI2, which would be logged.
               The problem surfaced only on Windows because of the differences
               in the way atexit() is handled on Windows and on other platforms.
               On the other platforms, the domain participant would eventually
               be deleted properly for a clean shutdown.
              </br>
              </br>
              <b>Solution: All objects are now released explicitly and the DDSI2
              domain participant is deleted as planned on all platforms.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.1p7</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5152<br/>
          </td>
          <td>
            <b>DDSI TCP interoperability with Vortex Cloud Routing Service</b>
            <br/>
            <i>
               When using OSPL clients with the Vortex Cloud where the routing
               service is involved then OSPL would fail to connect.
              </br>
              </br>
              <b>Solution:  TCP DDSI sends correct ENTITY_ID sub message to
              discovery and routing services, enabling cloud based routing</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.1p6</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4583/4586/<br/>12946
          </td>
          <td>
            <b>When durability is used in combination with DDSI and DDSI generates
             builtin topics, then durability may not align data because DDSI may
             drop durability messages.</b>
            <br/>
            <i>
               A durability service assumes reliable two-way communication with
               other durability services. This assumption is not true anymore
               when DDSI generates builtin topics (see the
               &lt;GenerateBuiltingTopics&gt;-tag). In this case it is possible
               that a durability service receives messages from another
               durability service, but responses to these messages are dropped
               by DDSI because not all readers have been discovered yet. If
               responses are dropped the durability protocol is broken, possibly
               leading to the inability to align data.
              </br>
              </br>
              <b>Solution:  If DDSI generates builtin topics then the durability
              service will only respond to a nameSpacesRequest message if all
              readers of the remote durability service have been discovered.
              Because the durability protocol always starts with the exchange of
              namespaces, discovery of all remote readers is now guaranteed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5068<br/>
          </td>
          <td>
            <b>release.com/.bat override OSPL_URI</b>
            <br/>
            <i>
               If an OSPL_URI is set and then the release.com/.bat is executed,
               the OSPL_URI is over-ridden.
              </br>
              </br>
              <b>Solution: OSPL_URI is now honoured.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5130<br/>
          </td>
          <td>
            <b>DDSI2 did not terminate within ServiceTerminatePeriod</b>
            <br/>
            <i>
               The DDSI2 service did not terminate within ServiceTerminatePeriod.
               The listen_thread was blocking on 'accept()' call.
              </br>
              </br>
              <b>Solution: On termination wake the listen_thread so termination
               can continue.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5141<br/>
          </td>
          <td>
            <b>DDSI2 TCP and TCP with SSL not consistent on blocking read/write</b>
            <br/>
            <i>
              DDSI2 TCP and TCP with SSL not consistent on blocking read/write
              leading to inconsisten behaviour.
              </br>
              </br>
              <b>Solution: A common timeout mechanism has been implemented for both TCP and
               SSL read and write operations that would block. TCP configuration
               options "ReadTimeout" and "WriteTimeout" have been added, these
               specify the timeout on a blocking read or write call, after which
               the call will be abandoned and the corresponding transport
               connection closed. These configuration options replace
               "ReadRetry", "ReadRetryInterval" and "WriteRetry" which have
               been removed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5143/<br/>13107
          </td>
          <td>
            <b>Possible compiler warning in the C++ language binding</b>
            <br/>
            <i>
              An initialiser used in the initialisation of a number of mutexes
              internal to the C++ language binding can cause compiler warnings.
              </br>
              </br>
              <b>Solution: The initialisation has been modified.</b>
            </i>
          </td>
        </tr>
       </table>

       <h2>6.4.1p5</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4983-1<br/>
          </td>
          <td>
            <b>DDSI over TCP interoperating with Vortex Caf\E9 may drop connection after 10s</b>
            <br/>
            <i>
               When a Vortex Caf\E9 process was connected to an OpenSplice
               Enterprise node using TCP where Caf\E9 was acting as a TCP client
               and Enterprise as a TCP server, Caf\E9 could consider the Enterprise
               node dead because it was not receiving participant discovery data
               as it expects.
              </br>
              </br>
              <b>Solution: Participant discovery data is now properly distributed
              over TCP connections.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.1p4</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4983<br/>
          </td>
          <td>
            <b>DDS using TCP can not handle high data load</b>
            <br/>
            <i>
               Under high load, DDSI TCP connections could be dropped and
               recreated, losing samples in the process and with various long
               timeouts interfering with the correct operation. This was caused
               by incorrectly handling a full socket buffer at the start of
               writing a message.
              </br>
              </br>
              <b>Solution: The code now correctly accounts for such events.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5118<br/>
          </td>
          <td>
            <b>Performance improvement for CDR deserialisation</b>
            <br/>
            <i>
               The CDR deserialisers all used a sub-optimal way of allocating
               sequences. Especially for sequences in small samples sent at a
               very high rate, this had a significant performance impact.
              </br>
              </br>
              <b>Solution: The allocation of sequences has now been changed to
              use a faster method.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.1p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5104<br/>
          </td>
          <td>
            <b>DDSI doesn't properly account transient-local data in its WHC</b>
            <br/>
            <i>
               DDSI stores unacknowledged samples and acknowledged but transient-local
               samples in a writer history cache. The amount of unacknowledged data
               is kept track of, but this was done incorrectly for transient-local
               data. This in turn could cause DDSI to lock up, in particular
               when a large amount of discovery data needed to be sent.
              </br>
              </br>
              <b>Solution: The amount of outstanding unacknowledged data now
              reflects acknowledgements of transient-local data as well.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5105<br/>
          </td>
          <td>
            <b>DDSI can throttle a writer without ensuring an ACK is requested immediately</b>
            <br/>
            <i>
               When the amount of outstanding unacknowledged data in a writer
               reaches a (configurable) threshold, the throttling mechanism
               blocks further data from that writer from being sent until the
               amount of unacknowledged data is reduced to below a (configurable)
               level. This requires the readers to send acknowledgements, which
               they are only allowed to do upon request from the writer. The
               last packet sent before the throttling often, but not always,
               includes a request for acknowledgements, and if it doesn't a
               100ms delay is incurred.
              </br>
              </br>
              <b>Solution: The writer now forces out a request for acknowledgements
              before blocking.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5106<br/>
          </td>
          <td>
            <b>DDSI delays NACKs unnecesarily when requesting samples for the first time</b>
            <br/>
            <i>
               DDSI2 follows the specification in delaying NACKs a little but only
               if the previous NACK was within the NackDelay interval. However,
               if it detects a need to request a retransmission of samples not
               covered in the previous NACK, waiting only introduces an unnnecessary delay.
              </br>
              </br>
              <b>Solution: The NACK scheduling now takes into account the highest
              sequence number in the preceding NACK.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.1p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5087/<br/>13087
          </td>
          <td>
            <b>High memory by DDSI2 on WinCE.</b>
            <br/>
            <i>
               On WinCE, DDSI2 could require large amounts of memory when transmitting
               large samples because of an issue in platform-specific code.
              </br>
              </br>
              <b>Solution: The underlying issue has been addressed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4506/<br/>12880
          </td>
          <td>
            <b>The use of a content-filter topic causes a memory leakage.</b>
            <br/>
            <i>
               When creating a content-filter topic a memory leakage occurs when
               evaluation the filter expression. The key field list used when
               evaluating the filter expression is not released causing the memory leak.
              </br>
              </br>
              <b>Solution: Release the key field list when evaluating the content-filter.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4878/<br/>13001
          </td>
          <td>
            <b>DeadlineQosPolicy when DataWriter is deleted keeps triggering</b>
            <br/>
            <i>
               Reader listener/waitset keeps getting triggered for deadline
               missed after instance is disposed and writer deleted.
               A v_dataReaderInstance was unintentionally re-inserted in the
               deadline list right after it was intentionally removed.
              </br>
              </br>
              <b>Solution: Removed v_dataReaderInstance re-insert</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4974-1<br/>
          </td>
          <td>
            <b>DDSI TCP on Windows fails with large messages</b>
            <br/>
            <i>
               When using large message payload, DDSI TCP would hang because
               the TCP buffer would overload.
              </br>
              </br>
              <b>Solution: Error handling improved for blocking TCP write functions.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4974-1<br/>
          </td>
          <td>
            <b>Change default value for DDSI TCP configuration property NoDelay </b>
            <br/>
            <i>

              <b>Solution: Changed the NoDelay DDSI TCP configuration property to true (from false)
               to reduce jitter.</b>
            </i>
          </td>
        </tr>
       </table>

       <h2>6.4.1p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4777-1/<br/>12989
          </td>
          <td>
            <b>In some situations it is possible that the durability service
            sends out responses to requests in the reverse order. As a resul
            recipients of these responses may perceive a "wrong" state of groups
            and namespaces.</b>
            <br/>
            <i>
               Due to a threading issue it is possible that a durability
               service sends out responses to request in the reverse order.
               In particular, the state of groups and namespaces could be reversed,
               causing recipients to believe that a group is 'incomplete' while
               in fact the master has announced is completeness. In this case the
               recipients will wait forever to become complete.
              </br>
              </br>
              <b>Solution: Once a group is complete it can never announce its
              'non-completeness' anymore. Also, order reversal of announcing
              namespace states has been prevented.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4942-1/<br/>13042
          </td>
          <td>
            <b>In rare occasions a process could fail to detach properly from
            SHM due to a race condition</b>
            <br/>
            <i>
               Due to a race condition in checking whether a live process is
               ready to detach, the process could conclude that it still had
               threads in SHM, causing the detach to fail unexpectedly. As soon
               as this was detected by spliced, the domain was brought down.
              </br>
              </br>
              <b>Solution: The race condition has been resolved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5053<br/>
          </td>
          <td>
            <b>DDSI "malformed packet received" error with state parse:info_ts</b>
            <br/>
            <i>
               DDSI verifies the well-formedness of the messages it receives,
               logging a "malformed packet" error if it is not. The message
               validator would reject a short timestamp even when the INVALIDATE
               flag was set on the submessage.
              </br>
              </br>
              <b>Solution: DDSI now accepts empty timestamp submessages with
              INVALIDATE set.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.1p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-3868<br/>
          </td>
          <td>
            <b>DDSI2 group instance leakage in shared memory</b>
            <br/>
            <i>
              The DDSI2 service caused a small memory leak in shared memory fo
              each group instance written.
              </br>
              </br>
              <b>Solution: Memory is now freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4095<br/>
          </td>
          <td>
            <b>DDSI stops when creating many readers/writers and no peers exist</b>
            <br/>
            <i>
              The DDSI discovery protocol exchanges information on all endpoints,
              and does so by creating an instance in a reliable, transient-local
              writer (one for writers, one for readers) for each existing endpoint.
              An issue was discovered where this data is counted as unacknowledged
              data even when there are no peers, and creating readers/writers may
              cause DDSI to reach the maximum allowed amount of unacknowledged
              data in the writer. This in turn blocks various processing paths,
              and if there are no peers, there is no way out.
              </br>
              </br>
              <b>Solution: When there are no peers, the data is no longer counted
              as unacknowledged data.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4607/<br/>12961
          </td>
          <td>
            <b>Deleting a writer does not free all shared memory allocated when creating it</b>
            <br/>
            <i>
              </br>
              </br>
              <b>Solution: Memory allocation is now tidied</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4713-1/<br/>12978
          </td>
          <td>
            <b>Readers not disposing after using built-in subscriber</b>
            <br/>
            <i>
              After termination, even after calling 'delete_contained_entities()'
              on the DomainParticipantFactory, the tester showed the participant
              as disposed with active DataReader for built-in topics. The
              'delete_contained_entities()' function did not delete all contained
              entities and the built-in subscriber was not deleted.
              </br>
              </br>
              <b>Solution: When calling 'delete_contained_entities()' on the
              DomainParticipant the built-in subscriber is now also deleted.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4713-2/<br/>12978
          </td>
          <td>
            <b>Shared memory runs out after running a very simple application in a loop</b>
            <br/>
            <i>
              A small memory leak (96bytes ::v_message&lt;kernelModule::v_participantInfo&gt;)
              made it possible to run out of shared memory when a very simple
              application (factory 'get_instance()', 'create_participant()'
              and exit) was run in a forever loop.
              </br>
              </br>
              <b>Solution: Memory is now freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4713-3/<br/>12978
          </td>
          <td>
            <b>Shared memory leakage on Windows platforms</b>
            <br/>
            <i>
              On Windows platforms our exit handler was registered but never
              called when 'get_instance()' was called from the customer application
              context, this caused memory leakage.
              </br>
              </br>
              <b>Solution: Our exit handler is now also called on Windows when
              our library is unloaded. However Windows terminate threads
              ungracefully which could still cause memory leakage if
              'delete_contained_entities()' is not called before library
              unloading.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4717/<br/>12982
          </td>
          <td>
            <b>When using wait_for_historical_data_w_condition with OR codition
            its possible that not all matching samples are returned.</b>
            <br/>
            <i>
              When wait_for_historical_data_w_condition is used the evaluation
              of the condition is not correct. If the conditions consists of OR
              expressions then not all parts of the OR expression are evaluated.
              </br>
              </br>
              <b>Solution: The evaluation of the condition now walks over all
              OR elements of the condition and does not stop when it finds a match.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4727<br/>
          </td>
          <td>
            <b>DDSI discovery heartbeat interval too long</b>
            <br/>
            <i>
              The DDSI specification gives a DDSI participant various ways of
              renewing its lease with its peers, one of which is a periodic
              publishing of a full participant discovery sample. To reduce the
              bandwidth needed, DDSI would instead send some other data, but this
              is not good enough to maintain liveliness with all other implementations.
              </br>
              </br>
              <b>Solution: DDSI now sends the participant discovery sample at an
              interval slightly shorter than the participant lease duration,
              which itself is taken from the OpenSplice domain expiry time,
              but never longer than the configuration SPDP interval.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4874<br/>
          </td>
          <td>
            <b>RnR replayed data not arriving on remote nodes using DDSI</b>
            <br/>
            <i>
              When RnR replays a recording it relies on the networking service
              to distribute the data from the RnR service to any remote data
              readers. The writer instance handles used by the replay did not
              match known writers for DDSI, hence DDSI was unable to determine
              where to send the data, and ultimately causing DDSI to drop the data.
              </br>
              </br>
              <b>Solution: RnR now creates local data writers and remaps the
              writer instance handles in the replayed data to correspond to
              these known writers, allowing DDSI to distribute the data
              throughout the network.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4962<br/>
          </td>
          <td>
            <b>Detecting which participant represents DDSI2 in a federation is difficult</b>
            <br/>
            <i>
              DDSI2 itself acts as a participant in the system, and hence creates
              a participant at the DDSI level as well. For other DDSI implementations
              it may be useful to be able to detect which of the remote participants
              is a DDSI2 service.
              </br>
              </br>
              <b>Solution: DDSI2 has been enhanced to indicate in its discovery
              information which of the potentially many participants in a
              federation is the DDSI2 service itself. This enhancement is backwards compatible.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4963<br/>
          </td>
          <td>
            <b>Domain ControlAndMonitoringCommandReceiver/Scheduling/Priority setting applied incorrectly</b>
            <br/>
            <i>
              The value configured for the
              ControlAndMonitoringCommandReceiver/Scheduling/Priority for the
              domain was applied to the ResendManager thread rather than to the
              CandM thread.
              </br>
              </br>
              <b>Solution: This has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4974<br/>
          </td>
          <td>
            <b>DDSI TCP may fail under high load
            <br/>
            <i>
              Under high load a DDSI TCP connection may fail and would not recover.
              </br>
              </br>
              <b>Solution: The socket waitset has been made threadsafe. Under UDP
              only a single thread accessed the waitset, under TCP multiple threads
              are used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4995<br/>
          </td>
          <td>
            <b>DDSI default socket buffer sizes increased</b>
            <br/>
            <i>
              The socket buffer sizes have a significant impact on performance,
              and in particular having a small receive buffer size when data comes
              in at a high rate can be a performance bottleneck. Unfortunately,
              there is no agreement across operating systems about the default
              maximum size, and hence in the past DDSI defaulted to a smallish buffer.
              </br>
              </br>
              <b>Solution: The defaults and the warning policy for failure to set
              the buffer size has been changed. Without specifying a receive buffer
              size (or "default"), DDSI will default to requesting 1MB, but accept
              whatever the kernel provides. Explicitly specifying a buffer size
              will still result in an error message if the kernel refuses to
              provide it.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5002<br/>
          </td>
          <td>
            <b>Invalid messages accepted by DDSI2</b>
            <br/>
            <i>
              The DDSI2 service verifies the well-formedness of the incoming
              messages, but two issue in the verification were discovered: firstly,
              it would accept invalid sequence numbers in data samples, even
              though the specification explicitly states such messages must be
              rejected, and secondly, it did not correctly verify that the start
              of the inline QoS or payload was indeed within the message.
              </br>
              </br>
              <b>Solution: Both points have been corrected.</b>
            </i>
          </td>
        </tr>
       </table>




       <h2>6.4.1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1084/<br/>10782
          </td>
          <td>
            <b>When RTnetworking compression is activated the number of network
            frames is not reduced.</b>
            <br/>
            <i>
              When RTnetworking compression is activated then compression is
              applied to each network frame which causes that the size of the
              frames is reduced but the number of frames remains the same.
              </br>
              </br>
              <b>Solution: When compression is activated the compression of the
              data is performed before fragmenting and the packing of data
              messages is performed before compression is applied to the data.
              Which results in a better compression ratio and a reduced number
              of fragments (packets).</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2023<br/>
          </td>
          <td>
            <b>DataReaderView doesn't take modified default DataReaderView QoS
            when created (sacpp & ccpp)</b>
            <br/>
            <i>
              The default DataReaderViewQos can be changed by calling
              set_default_datareaderview_qos() on the related DataReader.When a
              DataReaderView is created with DATAREADERVIEW_QOS_DEFAULT, then it
              should take the QoS that was set with set_default_datareaderview_qos().
               This didn't happen, the DATAREADERVIEW_QOS_DEFAULT was used as QoS.
              </br>
              </br>
              <b>Solution: During reader->create_view(), check if
              DATAREADERVIEW_QOS_DEFAULT was provided. If that's the case, then
              the readers' internal default DataReaderViewQos is used instead.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3789/<br/>12578
          </td>
          <td>
            <b>The 'autopurge_dispose_all' value is added to the ReaderDataLifecycleQosPolicy.</b>
            <br/>
            <i>
              When calling dispose_all() an a Topic, then related readers will
              receive a disposed notification for all disposed samples. For
              performance reasons, it should be possible to block those disposed
              notifications and only trigger the on_disposed_all() notification
              on the ExtTopicListener. Also the samples should be disposed
              automatically. This should be controlled on a 'per reader' basis.
              </br>
              </br>
              <b>Solution: The 'autopurge_dispose_all' is a new value in
              ReaderDataLifecycleQosPolicy, which is part of the DataReaderQos.
              When set to 'true' (default is 'false'), it makes sure that all
              related reader samples are purged when dispose_all() is called on
              a Topic. The related reader will not be notified that the samples
              have been disposed by a dispose_all().</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3976/<br/>12652
          </td>
          <td>
            <b>If master selection in durability takes a long time the system
            could stall</b>
            <br/>
            <i>
               At start up, the durability service tries to determine masters for
               all its namespaces. If during the master selection phase fellows
               are removed this also triggers master determination. In this case,
               the latter thread waits for the master selection lock which has
               been taken by the first thread. If the first thread takes a long
               time to determine masters for its namespaces (which is typically
               the case for large systems with many nodes and namespaces) then the
               second thread is stalled for a very long time. If this time exceeds
               the thread liveliness assertion period then the second thread is
               declared dead, which may lead to system failure.
              </br>
              </br>
              <b>Solution: While the second thread is waiting for the first thread
              to release its lock, liveliness of the second thread is asserted
              regularly. This ensures that the second is not declared dead, even
              if initial master selection by the first thread takes significant
              time.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4041/<br/>12669
          </td>
          <td>
            <b>Idlpp for C# generates incorrect code when using const to const assignment</b>
            <br/>
            <i>
              When generating code from an idl-file which contains a const variable
              which is used for assignment to another const variable, idlpp for C#
              generates incorrect code. The latter assignment was generated as
              (null), because the C# implementation was missing for this case.
              </br>
              </br>
              <b>Solution: Adjustements made to idlpp tool, it now implements the
              const to const assignment case.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4070<br/>
          </td>
          <td>
            <b>RTNetworking supports setting the Differentiated Services Code Point (DSCP)
            field in IP packets on windows</b>
            <br/>
            <i>
              To provide the setting of the DIffserv (DSCP) field in the IP packets
              the networking service used the IP_TOS option for this purpose. However,
              since Windows Vista and Window server 2008 setting the IP_TOS option
              is no longer supported. To use the Diffserv functionality on these
              versions or later versions of windows the new QoS2 API has to be used.
              </br>
              </br>
              <b>Solution: The networking service must map a configured Diffserv
              value on one of the Traffic Types supported by the windows QoS2 API.
              When administrative privileges are avalaible then the configured
              Diffserv value is set on the traffic flow associated socket which
              will result that the Diffserv field of the IP packets is set to
              the configured value. When no administrative privileges are
              available then the Diffserv field will be related to the Traffic
              Type that is selected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4282/<br/>12770
          </td>
          <td>
            <b>When using OSGi without proper exports a crash will occur.</b>
            <br/>
            <i>
              When using two OSGi bundles, one contains dcps.jar
              (dcpssaj-osgi-bundle.jar) and the second by idlpp generated
              typed code without exports and an application. When the second
              bundle accessed the first bundle which then tried to access a
              class form the second bundle using the JNI FindClass function a
              crash occurred because of the thrown exception.
              </br>
              </br>
              <b>Solution: To prevent this crash from happening the exceptions
              thrown by the JNI FindClass function are now caught and a log message
              is written to the error log explain what when wrong.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4371/<br/>12817
          </td>
          <td>
            <b>Unclear logging when services are killed because of elapsed
            serviceTerminatePeriod</b>
            <br/>
            <i>
              When the serviceTerminatePeriod elapses during shutdown the
              ospl-tool logged an ambiguous message to the info log. The Splice
              daemon should have logged a clearer service kill message, but this
              was never reached because the ospl-tool would terminate the Splice
              daemon prematurely.
              </br>
              </br>
              <b>Solution: Clarified service kill messages for both ospl-tool and
              Splice daemon. Increased ospl-tool wait period before sending kill
              signal to Splice daemon process group. Additionally, durability now
              logs messages when it fails to assert its liveliness within the
              expiration period.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4345/<br/>12809
          </td>
          <td>
            <b>The adminQueue may overflow when receiving thread is busy
            processing messages and the sending thread is not scheduled in time.</b>
            <br/>
            <i>
              The sending thread is responsible for transmitting ACK messages.
              For that purpose the receiving thread uses the adminQueue to inform
              the sending thread of the data messages received. When the receiving
              thread is busy processing received data the adminQueue may get full
              because the sending thread (lower priority) is not scheduled in time.
              </br>
              </br>
              <b>Solution: The receive thread is made responsible of sending the
              ACK messages. This has the effect that the timing requirements of
              the sending thread are relaxed.</b>
            </i>
          </td>
        </tr>
        </tr>
        <tr>
          <td>OSPL-4385/<br/>12821
          </td>
          <td>
            <b>When using edge case resource limits, OpenSplice didn't behave as expected.</b>
            <br/>
            <i>
              When using a resource setting of max_samples=1 and history = KEEP_LAST
              for the reader, samples weren't overwritten while as expected but an
              error was returned.
              </br>
              </br>
              <b>Solution: The reader and writer resource limits are now better
              checked so that samples are overwritten when allowed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4423<br/>
          </td>
          <td>
            <b>User should be aware that a runtime installation of OpenSSL is
            required for OpenSplice licensed features and/or ddsi2e and snetworking.</b>
            <br/>
            <i>
              Addition of TLS in ddsi2 removes the static link to OpenSSL in previous
              versions of OpenSplice on non-windows systems.
              </br>
              </br>
              <b>Solution: At runtime an installation of OpenSSL is required for
              licensed features, however on most systems this is standard.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4463/<br/>12863
          </td>
          <td>
            <b>The use of sequences is not supported in multi-domain applications.</b>
            <br/>
            <i>
              The issue is located in the copy-in routines generated by the IDL
              pre-processor. The copy-in routines are used when the application
              performs a write operation. To improve the performance of the
              copy-in routines these routines cache some type information about
              contained sequences. This causes a problem when writing the same
              type to multi-domains because the cached type information is domain specific.
              </br>
              </br>
              <b>Solution: An option (-N) is added to the IDL pre-processor which
              disables the type caching in the generated copy-in routines.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4530<br/>
          </td>
          <td>
            <b>Improved DDSI robustness</b>
            <br/>
            <i>
              In high-throughput situations, DDSI2 could behave quite badly,
              with retransmit storms and/or temporarily considering reliable
              readers unresponsive and treating them as effectively best-effort.
              The long default participant lease duration caused these effects
              to linger for a long time even after restarting part of the application.
              </br>
              </br>
              <b>Solution: The risk of retransmit storms and the associated effects
              has been reduced by improving the mechanism used to control the
              rate of retransmit requests and improved control over the amount
              of outstanding unacknowledged data, by configuring bytes rather
              than samples. The default participant lease duration is now
              controlled by the ExpiryTime configured for the domain, and will
              therefore typically have a more reasonable value.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4534<br/>
          </td>
          <td>
            <b> In cases where DDSI generates builtin topics there is no need
            for durability to align the builtin topics.</b>
            <br/>
            <i>
               DDSI can discover entities and could generate builtin topic
               information. This enables non-enterprise nodes in the DDSI network
               to become visible. Also, in cases where DDSI generates builtin
               topic information there is no need for durability to align builtin
               topic information, which saves bandwidth.
              </br>
              </br>
              <b>Solution: Durability will not align builtin topics in case ALL
              DDSI services generate builtin topics and no native networking
              services are configured. To force DDSI to generate builtin topics
              DDSI can set their <GenerateBuiltinTopics> value set to TRUE.
              In all other cases durability will align builtin topics to ensure
              backwards compatibility.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4596/<br/>12954
          </td>
          <td>
            <b>A sample predating the oldest sample in the history of a TRANSIENT
            or PERSISTENT instance could overwrite a newer sample</b>
            <br/>
            <i>
              Due to a fault in the mechanism used to insert a sample in the
              history of a TRANSIENT or PERSISTEN instance, a sample predating
              the oldest sample in the history would replace the oldest sample
              instead of being discarded. This would cause late-joining readers
              to observe an inconsistent history.
              </br>
              </br>
              <b>Solution: The mechanism has been fixed to properly order the
              samples, so the oldest sample will be discarded when it doesn't
              fit in the history instead of overwriting the oldest sample.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4596-1/<br/>12954
          </td>
          <td>
            <b>After durablity alignment of a dispose_all message only the first
            instance is NOT_ALIVE_DISPOSED</b>
            <br/>
            <i>
              When the durability service needed to align a fellow, a stored
              dispose_all message was only sent to the first instance for the
              topic. The dispose_all sample for following instances was incorrectly
              marked as duplicate because it only tested for writerGid, writeTime
              and sequenceNumber.
              </br>
              </br>
              <b>Solution: The durability service now also checks samples for
              keyvalues before marking as duplicate.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4596-2/<br/>12954
          </td>
          <td>
            <b>After durability alignment of a dispose_all message and a
            delete_dataWriter the instance_state does not go to NOT_ALIVE_NO_WRITERS</b>
            <br/>
            <i>
              When the durability service needed to align a fellow with a
              dispose_all message, an implicit registration message is created
              for a NIL writer. This NIL writer was never removed causing the
              dataReader instance_state to remain ALIVE.
              </br>
              </br>
              <b>Solution: The durability service now also sends an implicit
              unregister message after it sent a implicit register for a NIL writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4596-3/<br/>12954
          </td>
          <td>
            <b>When a dataReader instance_state is NOT_ALIVE_NOWRITERS and it
            receives a dispose_all the instance_state does not transition to
            NOT_ALIVE_DISPOSED</b>
            <br/>
            <i>
              When a dataReader is in NOT_ALIVE_NOWRITERS instance_state and the
              last action was TAKE the instance pipeline is destroyed. The group
              updates it's state when a dispose_all message is received, but
              could not forward it to the dataReader. The dataReader instance_state
              did not change.
              </br>
              </br>
              <b>Solution: The group now checks all dataReader instances, when a
              dataReader has NOWRITERS implicit registration and unregister
              messages are send so that the instance pipeline is reconstructed
              and destroyed after the dispose_all is received by the dataReader.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4614/<br/>12392
          </td>
          <td>
            <b>Durability does not handle merge policy correctly in some cases
            with terminating and (re)connecting fellows</b>
            <br/>
            <i><ul>
              <li>If durability detects a fellow is terminating, it removes the
              fellow from its administration. However, when receiving a new
              message from the fellow after it has been removed, resulted in
              adding the fellow to the administration again (even though the
              fellow is already terminating). This triggers faulty merge actions
              that cannot be completed.</li>
              <li>In case a merge action needs to be performed, durability
              sometimes needs to wait until to fellow to merge with reaches a
              certain state. Durability periodically checks whether that state
              has been reached. However, when the fellow terminates before it
              reaches that state, durability continues to wait for the desired
              fellow state even though it is clear that the fellow will never
              reach that state.</li>
              <li>When durability decides to remove a given fellow from the
              administration, it needs to check whether the 'merge state' of
              its name-spaces need to be cleared. This is required to ensure
              that a merge action is triggered once a new master is elected
              after the original one disappeared. A potential dummy fellow
              parameter (with no name-space information) was used in some cases
              to determine the name-spaces that require resetting. Obviously,
              name-space information may be missing from such dummy parameters
              causing name-space merge-states not to be reset. As a result
              durability may conclude that no merge action is required when a
              new master is elected.</li>
              </ul>
              </br>
              <b>Solution: <ul>
              <li>To prevent adding terminating fellows, the various paths that
              lead to adding a fellow to the administration have been modified
              to refrain from adding it if the fellow is in terminating or
              terminated state.</li>
              <li>The algorithm has been modified to cancel the merge when the
              fellow terminates or gets disconnected before it reaches the
              desired state.</li>
              <li>The actual fellow that is removed and not the dummy one is
              now used to determine further actions.</li>
              </ul>
              </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4777/<br/>12989
          </td>
          <td>
            <b>A deadlock can appear when durability tries to use the KV store
            during initial alignment, which causes durability to halt forever</b>
            <br/>
            <i>
               During initial alignment access to the KV store may be required.
               In this phase of the process two threads a competing for two
               resources, the durability administration and the store. These
               threads try to lock the resources, but in a different order.
               This could lead to a deadlock of the durability service.
              </br>
              </br>
              <b>Solution: The KV store does not require a
              lock on the durability administration anymore. This will prevent
              the deadlock.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4800/<br/>12990
          </td>
          <td>
            <b>Multiple Ctrl^C can cause a crash in the exit-request handler.</b>
            <br/>
            <i>
               Termination requests received in rapid succession could cause a
               crash in the exit-request handler.
              </br>
              </br>
              <b>Solution: The handlers installed by services are now executed
              only once.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4896/<br/>13015
          </td>
          <td>
            <b>The durability persistentDataListener thread failed to make progress when
            using the KV store, causing the system to terminate and execute its failure action.</b>
            <br/>
            <i>
               The KV store uses transactions to persist data. In case there are
               many samples to persist, the transaction can take a very long time.
               This may even outlive the time to assert liveliness. When the time
               to complete such transaction exceeds the time to assert liveliness
               the reposnsible thread is declared dead and no leases will be renewed
               anymore, causing the system to execute its failure action.
              </br>
              </br>
              <b>Solution:  To prevent that the persistentDataListener thread
              cannot make progress two improvements have been implemented. The
              first improvement is to use the liveliness expiry time instead of
              the heartbeat expiry time to decide if assertion of liveliness has
              succeeded or not. The first is typically larger than the second,
              causing the system to implement a more relaxed liveliness assertion
              policy. The second improvement is to ignore liveness checking in
              case of potential intensive operations on the KV store such as
              commit and delete.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4916/<br/>13019
          </td>
          <td>
            <b>OSPL Source build required MICO and had kvstore library names incorrect</b>
            <br/>
            <i>
               Customers with access to OSPL Source Build noted a dependency on MICO
               for building source code and that some libraries links were incorrect.
              </br>
              </br>
              <b>Solution:  MICO is now optional and links named correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4920/<br/>13020
          </td>
          <td>
            <b>When networking compression is used then occasionally an error
            "Received incorrect message" is reported.</b>
            <br/>
            <i>
               When compression is activated in the networking service and when a
               compressed network frame that is received contains user data messages
               for which the type is not or not yet known on the node then the
               networking service is not able to de-serialize that user data message
               and should skip this message and continue with the next user data
               message in the frame. However in that case the buffer administration
               is not correctly updated resulting in the reported error and the
               rest of the frame to be dropped entirely.
              </br>
              </br>
              <b>Solution:  The buffer administration has to be updated correctly
              when skipping a user data message for which the type information is
              not known.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4938/<br/>13038
          </td>
          <td>
            <b>Overflow for network queue resulted in a stackoverflow during cleanup of network reader</b>
            <br/>
            <i>
               Unregister messages were not being obeying max queue size.
              </br>
              </br>
              <b>Solution:  Changed check for message size to reject a message when
              queue is equal or greater than max queue size as unregister
              messages can increase the queue size beyond max size.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4942/<br/>13042
          </td>
          <td>
            <b>Reason for termination of domain not reported in all situations</b>
            <br/>
            <i>
               The splice-daemon attempts to clean up shared resources of
               processes that terminated without cleaning them up. If it fails
               to do so, it does not report anything in the log files in some
               situations before stopping the domain. Additionally, if the
               cleaning up did not complete within 1 second, the splice-daemon
               assumed that cleaning up had failed.
              </br>
              </br>
              <b>Solution: Extra logging has been added to ensure the reason for
              stopping is clear for users. Furthermore, the time out for
              cleaning up has been slaved to the existing lease expiry time-out
              (//OpenSplice/Domain/Lease/ExpiryTime) instead of a fixed period
              of 1 second.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-5020/<br/>13068
          </td>
          <td>
            <b>Inconsistency between report level verbosity and reports for FATAL and CRITICAL verbosity</b>
            <br/>
            <i>
               Reports at levels FATAL and CRITICAL are emitted as "FATAL ERROR"
               and "CRITICAL ERROR" respectively. This is not consistent and
               causes open and close tags with whitespace included (e.g.,
               "&lt;FATAL ERROR&gt;...&lt;/FATAL ERROR&gt;" for the report-plugin.
              </br>
              </br>
              <b>Solution: The reports are now emitted with text FATAL and
              CRITICAL, corresponding to the verbosity level.</b>
            </i>
          </td>
        </tr>
      </table>

       <h2>6.4.0p7</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4384/<br/>12820
          </td>
          <td>
            <b>Nested IDL modules not properly handled by the C++ RMI compiler</b>
            <br/>
            <i>
              With rmipp, the handling of nested IDL modules was generating incorrect
              code.
              </br>
              </br>
              <b>Solution: A bug in the rmipp code generators has been fixed, so that nested
              IDL modules map properly onto nested C++ namespaces.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4517/<br/>12882
          </td>
          <td>
            <b>Deadlock in listenerEvent when terminating domain and calling
            delete_contained_enties</b>
            <br/>
            <i>
              A deadlock could occur when an application created a domainParticipant
              with listeners and the domain was terminated while the application
              called deleted_contained_entities. The listernerEventThread would
              remain in infinite wait for never signaled waitset, because notify
              fails due to no longer running splice daemon.
              </br>
              </br>
              <b>Solution: The listenerEventThread now has a polling wait loop,
              allowing it to detect stop requests.</b>
            </i>
          </td>
        </tr>
       </table>



       <h2>6.4.0p6</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4046/<br/>12643
          </td>
          <td>
            <b>The cmsoap service crashes when there is no network connection
            present at startup of the service.</b>
            <br/>
            <i>
              The cmsoap service tries to determine the IP address through which
              it can be reached. These IP addresses are set in the user data field
              present in the DCPSParticipant builtin topic which enables other
              tools to connect to the soap service. However when no IP address a
              crash occurs because of access to uninitialized memory.
              </br>
              </br>
              <b>Solution: When the cmsoap service cannot detect an IP address it
              should use the loopback IP address instead.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4079/<br/>12677
          </td>
          <td>
            <b>The write method incorrectly returns TIMEOUT in case there are not
            enough resources available or can be freed in time.</b>
            <br/>
            <i>
              When the write method detects that there are not enough resources
              available and it will not be possible that resources will be
              available in time, e.g. max instances exceeded is should return
              OUT_OF_RESOURCES instead of TIMEOUT.
              </br>
              </br>
              <b>Solution: In case the max instance resource limit is exceeded
              return OUT_OF_RESOURCES</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4410/<br/>12826
          </td>
          <td>
            <b>Deadlock in parallel demarshalling termination</b>
            <br/>
            <i>
              When starting and stopping parallel demarshalling within a short
              time window it was possible that the parallel demarshalling
              termination was stuck in a deadlock. Not all spawned threads would
              terminate, because the terminate flag was reset before all parallel
              demarshalling threads were operational.
              </br>
              </br>
              <b>Solution: The API set_property function with property name
              parallelReadThreadCount is now blocking until all parallel
              demarshalling threads are started and operational and the
              terminate flag is now reset upon (re)start of parallel demarshalling.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4464/<br/>12864
          </td>
          <td>
            <b>"FATAL ERROR Open Splice Control Service status monitoring failed.
            Exiting." logged when sending signal to blocking OSPL tool.</b>
            <br/>
            <i>
              When "ospl -f start" was executed and a signal was sent to the OSPL
              tool, a FATAL ERROR message was logged. Not a real FATAL ERROR
              because the part of the OSPL tool that monitored the liveliness
              of the splice daemon wasn't aware of incoming signals and logged
              a FATAL ERROR while the splice daemon terminated normally.
              </br>
              </br>
              <b>Solution: Made the part of the OSPL tool that monitors the
              liveliness of the splice daemon aware of termination caused by a
              received signal.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4490/<br/>12859
          </td>
          <td>
            <b>When DataWriter exits unnaturally LivelinessStatus is incorrect.</b>
            <br/>
            <i>
              When DataWriter exits unnaturally the LivelinessStatus was updated
              incorrectly when the DataWriter was Alive, this caused an illegal state transition.
              </br>
              </br>
              <b>Solution: LivelinessState change for unnatural DataWriter exits
              now use the last known state before transitioning to DELETED.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4509<br/>
          </td>
          <td>
            <b>DDSI2E now accepts DDSI2 configurations</b>
            <br/>
            <i>
              </br>
              </br>
              <b>Solution: DDSI2E required that the configuration in the
              OpenSplice XML configuration file was tagged "DDSI2EService",
              but this made it impossible to switch to the DDSI2E service without
              changing the configuration file. DDSI2E now also accepts
              configurations under the DDSI2Service tag.</b>
            </i>
          </td>
        </tr>
       </table>

       <h2>6.4.0p5</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-878/<br/>10549
          </td>
          <td>
            <b>The 'ospl start' command can exit before the DDS Domain is up.</b>
            <br/>
            <i>
              On somewhat slower systems, the 'ospl start' command can exit before
              the DDS Domain is up. This would mean that creating a DomainParticipant
              immediately after 'ospl start' can fail.
              </br>
              </br>
              <b>Solution: The 'ospl start' command now waits until the DDS Domain
              is up before exiting.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4333/<br/>12801
          </td>
          <td>
            <b>Java language binding fails with multiple package redirects</b>
            <br/>
            <i>
              Java language binding fails when multiple packages are redirected
              and a type containing a type from a redirected package is registered.
              </br>
              </br>
              <b>Solution: Pass all redirect instructions for all types to the
              Java language binding.</b>
            </i>
          </td>
        </tr>
       </table>


       <h2>6.4.0p4</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4116<br/>
          </td>
          <td>
            <b>A potential crash could occur when durability terminates</b>
            <br/>
            <i>
              The durability service notifies the splice daemon too early in
              case durability is about to terminate. This could lead to a
              situation where the splice daemon already destroys shared memory
              while durability is still busy cleaning up objects and thus
              acessing the shared memory. This could lead to a system crash.
              </br>
              </br>
              <b>Solution: The durability services will now notify the splice
              daemon after it has cleaned up all objects and no access to shared
              memory is needed any more. Now the splice daemon can safely destroy
              the shared memory.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4250<br/>/12744
          </td>
          <td>
            <b>JNI attach listener crash results in OpenSplice crash without any error report</b>
            <br/>
            <i>
              When a crash occurs inside JNI call that attaches the listener thread
              to the application, OpenSplice crashes without a proper report
              </br>
              </br>
              <b>Solution: A proper error report is now generated so the customer knows what went wrong.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4251<br/>/12753
          </td>
          <td>
            <b>Late joining readers not getting complete historical data when more
            than one networking service configured</b>
            <br/>
            <i>
              When more than one networking service is configured duplicate message
              may be received. A reader will filter these duplicate message. However
              the group does not filter these duplicates and when the corresponding
              Topic QoS has a history depth greater than 1 this may result in that
              the duplicate message are stored in the group. This may cause that
              a late joining reader does not receive the correct number of samples.
              </br>
              </br>
              <b>Solution: The group should check if a duplicate message is received
               and drop the duplicates.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4270<br/>/12765
          </td>
          <td>
            <b>DDSI2 not supporting QoS changes not documented</b>
            <br/>
            <i>
              The DDSI2 networking service does not (yet) support QoS changes,
              instead silently ignoring them, but this was not mentioned in the
              documentation.
              </br>
              </br>
              <b>Solution: This limitation is now stated clearly in the DDSI2 release notes.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4320<br/>
          </td>
          <td>
            <b>Java 7 linux 64 bit crash with Listener example</b>
            <br/>
            <i>
               When running the Listener example under linux 64 bit with java 7 it could crash.
              </br>
              </br>
              <b>Solution: The default listener stacksize is 64k, for java 7 this
              needs to be at least 128k as a result of this the default listener
              stacksize is increased to 128k</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4342/<br/>12807
          </td>
          <td>
            <b>Deserialisation issues with Java CDR-based copy-out, high-performance
             persistent store and RnR binary storage</b>
            <br/>
            <i>
               The CDR serialiser used for Java CDR-based copy-out, the new
               high-performance persistent store and RnR binary storage could
               introduce incorrect padding in the CDR stream under some circumstances.
               To a reasonable approximation, this requires a type of unbounded size,
               or one where the maximum size is several times the minimum size, AND
               where the content of the data results in a serialised size larger than
               16kB, AND where a string, sequence or array with alignment of less than
               8 bytes requires a new block at a time the CDR stream is not aligned to
               a multiple of 8 bytes.
              </br>
              </br>
              <b>Solution: The CDR serialiser now maintains the alignment of the stream
              when it switches to a new block.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4366<br/>
          </td>
          <td>
            <b>Durability service may crash during termination</b>
            <br/>
            <i>
               During termination, the durability service stops its threads and
               cleans up its administration. Due to the fact the main thread
               cleans up some part of the administration that is used by the lease
               thread before ensuring that thread stopped, that thread may access
               already freed memory which may cause the service to crash.
              </br>
              </br>
              <b>Solution: The main durability service thread now ensures the
              lease thread has stopped before freeing the administration.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4390<br/>
          </td>
          <td>
            <b>SOAP service may crash when concurrently using and freeing the same entity</b>
            <br/>
            <i>
               The SOAP service allows PrismTech tools to connect to a node or process
               remotely. Due to the multi-threaded nature of the service, multiple
               requests can be handled concurrently. When the same entity is
               concurrently accessed and freed during two or more requests, the
               service may crash due to the fact one of the threads is trying to
               access already freed memory.
              </br>
              </br>
              <b>Solution: The internal API of the SOAP service has been re-factored
              to claim an entity when it is used and release it afterwards. When an
              entity is freed when one or more claims are still outstanding, new
              claims are denied and the actual deletion is postponed until all of
              the outstanding claims have been released.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4421<br/>
          </td>
          <td>
            <b>Error reports about instance handles are mixed up</b>
            <br/>
            <i>
               Each call that has an instance handle parameter as well as a sample
               parameter on the DataWriter entity (like for instance the write call),
               validate whether the provided instance handle belongs to the DataWriter
               and if so validates whether the key-values in the sample match the
               key-value that is associated with the instance handle. If one of
               these conditions is not true, an error is reported and the call
               fails. However, the errors that are printed in the two failure
               cases have been mixed up causing the wrong error message to be
               reported in both these cases.
              </br>
              </br>
              <b>Solution: The error reports have been updated to match the
              actual error that occurred.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.4.0p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4286<br/>12771
          </td>
          <td>
            <b>The read_w_condition may incorrectly return no data when a
            read_next_instance_w_condition is called before.</b>
            <br/>
            <i>
               The read_next_instance_w_condition may incorrectly set the no data
               property of the associated query to indicate that no data matches
               the query, This may cause that a following read_w_condition may
               return no data when data is available.
              </br>
              </br>
              <b>Solution: The read_next_instance_w_condition should only set
              the no data property of the associated query when a complete walk
              is performed on all the instances.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4327<br/>12797
          </td>
          <td>
            <b>A crash of the durability service may occur when samples containing
              strings with non-printable characters are stored in the XML persistent
              store.</b>
            <br/>
            <i>
               When the persistent XML store contains samples with strings containing
               non-printable characters then the durability service by crash because
               the layout of the XML storage file is not as expected.
              </br>
              </br>
              <b>Solution: The XML serializer used by durability to serialize the
              samples for the XML store should escape the non-printable characters.</b>
            </i>
          </td>
        </tr>
       </table>

       <h2>6.4.0p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4173<br/>12731
          </td>
          <td>
            <b>The notification of a sample lost event by the networking service may result in a crash of the networking service.</b>
            <br/>
            <i>
               When the networking service detects that samples have been lost it
               tries to notify the corresponding readers of this event. The sample
               lost event is recorded in the status of the reader. However there
               are internal readers used by Opensplice which do not have an
               associate status object causing the crash of the networking service.
              </br>
              </br>
              <b>Solution: Apply all internal readers with an status object.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4269<br/>12760
          </td>
          <td>
            <b>Ospl reports an error if no persistent file is present and using KV Store.</b>
            <br/>
            <i>
               With KV Store, at ospl startup, if no persistent file is available,
               ospl reports an error: getset_version: read of version failed.
              </br>
              </br>
              <b>Solution: The error message has been fixed. It was not required. No behavior change.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4319/4330/4344/4346<br/>12778/12779/12808
          </td>
          <td>
            <b>Crashes due to shared memory allocator issue</b>
            <br/>
            <i>
               A refactoring of common code introduced an issue in the shared
               memory sub-allocator dealing with allocating "large" objects that
               could result in crashes or reports of heap corruption under high-load
               scenarios. For crashes, this typically (but not necessarily) involves
               stack traces involving the "check_node" function.
              </br>
              </br>
              <b>Solution: The specific changes have been reverted until they can be
              corrected and re-tested.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4322<br/>12747
          </td>
          <td>
            <b>Nullpointer exception when creating a reader/writer using the Tuner</b>
            <br/>
            <i>
               When using the Tuner and creating a reader/writer it is possible to
               get a nullpointer excetion when selecting a different topic from the pulldown menu.
              </br>
              </br>
              <b>Solution: The nullpointer is now being caught and will no longer appear.</b>
            </i>
          </td>
        </tr>
       </table>

       <h2>6.4.0p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-3445<br/>12408
          </td>
          <td>
            <b>Inconsistent behaviour of service when handling signals.</b>
            <br/>
            <i>
               The service should handle asynchronous signals like SIGQUIT or SIGTERM
               as normal termination requests which should not trigger an failure action.
               However the handling of these termination request is not correct,
               which may result in a normal termination or may result in an exception
               which triggers the failure action.
              </br>
              </br>
              <b>Solution: When a service receives a termination request signal like SIGQUIT or SIGTERM
              it will initiate a normal termination of the service and will not trigger an
              failure action. When the service receives a synchronous signal like SIGSEGV
              as result of an exception or send asychronously to the service then the service
              should detach from shared memory and trigger the failure action.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3695/<br/>12553
          </td>
          <td>
            <b>Shared memory consumption would increase to unacceptable high levels
            when using KV persistency</b>
            <br/>
            <i>
               When KV persistency is enabled shared memory consumption would reach
               unacceptable high levels. This was caused by the following two phenoma.
               First, the StoreSessionTime configuration option was not respected
               causing the system to store KV samples as long as there are samples
               available. Second, an inefficient algorithm to store samples on disk
               was used, resulting in (expensive) disk access for every sample.
               Together, these phenoma caused the system to pile up samples in
               memory that cannot be stored on time.
              </br>
              </br>
              <b>Solution: A more efficient algorithm is used to store samples on
              disk, resulting in disk access for a set of samples instead of an
              individual sample. This will boost performance when writing samples
              to disk. This reduces the risk to pile up data in memory that caused
              the unacceptable high level of memory consumption. Also, the
              StoreSessionTime is respected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3866<br/>
          </td>
          <td>
            <b>On windows, setting a long lease time in the configuration results
            in an large error log during ospl stop</b>
            <br/>
            <i>
               When a termination request was made, some services with long lease
               times set in their configuration would generate on windows a large
               amount of error messages, as the termination was not acknowledged
               during the lease time.
              </br>
              </br>
              <b>Solution: The sleeping lease thread from ospld, durability and soap
              are signalled to stop during the sleep in case of a termination request.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4075<br/>12678
          </td>
          <td>
            <b>The network partition mapping of the expression . does not function correctly.</b>
            <br/>
            <i>
               When the network partition mapping expression . is evaluated to find
               the best match the the global partition is selected instead of the
               configured network partition.
              </br>
              </br>
              <b>Solution: Exclude the global partition from the search for a best
              matching network partition and select the global partition only
              when no other network partition can be found that matches the mapping
              expression.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4205<br/>12740
          </td>
          <td>
            <b>The read_instance method sometimes returns ALREADY_DELETED but the
            reader entity has not been deleted.</b>
            <br/>
            <i>
               This situation occurs when the instance that is referenced by the
               instance handle supplied to the read_instance has been deleted.
               For example when the instance has become disposed and unregistered.
               In that case the instance handle becomes invalid. The return code
               ALREADY_DELETED is incorrect and should be BAD_PARAMETER to indicate
               that the instance is not valid anymore.
              </br>
              </br>
              <b>Solution: When the read_instance detects that the provided
              instance handle has become invalid then return BAD_PARAMETER.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-4245<br/>12748
          </td>
          <td>
            <b>Linker error in custom library compilation for CORBA C++ cohabitation
            with V6.4.0</b>
            <br/>
            <i>
               DDS_CORE is not set anymore in custom lib environment so cannot
               be used anymore by the linker.
              </br>
              </br>
              <b>Solution: Changed custom lib makefiles by replacing DDS_CORE
              environment setting by ddskernel.</b>
            </i>
          </td>
        </tr>
       </table>

       <h2>6.4.0</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-31<br/>
          </td>
          <td>
            <b>idlpp did not generate valid java-code when a union had a case
            called 'discriminator'</b><br/>
            <i>
               When an idl-union contained a case called 'discriminator', the
               generated java-code would contain two conflicting definitions for
               the 'discriminator' method. This method is always included in a
               class generated from a union to obtain the value of the
               union-discriminator. With a discriminator case an additional
               discriminator method is added with the same signature that returns
               the value of the discriminator field.
              </br>
              </br>
              <b>Solution:  The solution is according to the IDL to Java
              specification which prescribes that the function returning the
              discriminator-value should be prefixed with a '_' if the union
              contains a case called 'discriminator'.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1631<br/>11194
          </td>
          <td>
            <b>Reporting does not include timezone information</b><br/>
            <i>
               In scenario's where nodes are joining and/or leaving a domain
               The timestamps in the default info and error logs did not include
               timezone information. When the timezone of a system is altered
               while OpenSplice is running, the reports may appear out of order.
              </br>
              </br>
              <b>Solution: To resolve any uncertainty the locale-dependent
              abbreviated timezone has been added to the date format.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1705/1713/1714<br/>
          </td>
          <td>
            <b>Durability service XML persistency handles topics with string keys
            incorrectly</b><br/>
            <i>
                If multiple string keys exist for a Topic that is being persisted
                by the durability service, samples for different instances may be
                interpreted as samples for the same instance causing potentially
                samples to be overwritten while they both are supposed to be
                maintained. Secondly, if one or more key-values contained new-lines,
                storage in XML was also done in a way that prevented the data from
                being republished correctly after system restart. Finally, if the
                string key was matching the "</TOPIC>" closing tag in the XML
                implementation, samples matching this key would not be persisted in all cases.
              </br>
              </br>
              <b>Solution: Key-values are now escaped when storing them in XML.
              The change is backwards compatible meaning that the new version can
              cope with old persistent stores.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2360<br/>
          </td>
          <td>
            <b>Strange error message from DDSI2 for truncated packets on Windows</b><br/>
            <i>
               On Windows, when a message is truncated because there is insufficient
               receive buffer space available, the error message produced by DDSI2
               would be somewhat confusing, because Windows reports this as an
               error whereas DDSI2 assumed POSIX behaviour of treating this as
               an unusual situation rather than an actual error. The behaviour
               of DDSI2 was ok, in that it discarded the message regardless of
               the platform.
              </br>
              </br>
              <b>Solution: The error reported by Windows is now recognised and
              reported properly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2485<br/>
          </td>
          <td>
            <b>Idlpp generated invalid java for a union with only a default case</b><br/>
            <i>
               When a union only contained a default-usecase, java-code was
               generated which did an invalid check on discriminator and therefore
               did not compile.
              </br>
              </br>
              <b>Solution: The discriminator-check is not valid when there is
              only a default case. The applied fix removes the check in this scenario.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2519<br/>
          </td>
          <td>
            <b>OS_INVALID_PID is accepted as a valid processID in the abstraction layer</b><br/>
            <i>
               The OS abstraction layer functions os_procDestory and os_procCheckStatus
               accepted OS_INVALID_PID as a valid input. Especially in
               os_procDestory, which is able to send signals to processes,
               this could have caused undesired behavior.
              </br>
              </br>
              <b>Solution: The functions os_procDestory and os_procCheckStatus
              now return invalid when processID OS_INVALID_PID is passed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2616<br/>
          </td>
          <td>
            <b>Internal change: File extension change for files generated for Corba co-habitation CPP</b><br/>
            <i>
               When generating from your idl file, the tao idl compiler would generated .i.
              </br>
              </br>
              <b>Solution: For the inline files these have now been changed to
              the default file extension used by TAO, which is .inl.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3042<br/>
          </td>
          <td>
            <b>The library versions of sqlite and leveldb supplied by Opensplice
            may conflict with system supplied builds.</b><br/>
            <i>
               An Opensplice delivery contains particular versions of the sqlite
               and leveldb libraries on which Opensplice is dependent. These
               libraries are installed in the Opensplice install directory.
               owever the versions of these libraries may conflict with newer
               versions which are available on the system on which Opensplice
               is installed.
              </br>
              </br>
              <b>Solution: The names of the supplied sqlite and leveldb libraries
              are made Opensplice specific by adding an ospl postfix.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3151-1<br/>
          </td>
          <td>
            <b>Receiving unidentifiable duplicate messages during durability
            alignment when using DDSI2</b><br/>
            <i>
               When using DDSI2 it was possible that during durability alignment
               a duplicate message was received which could not be identified as
               a duplicate because the sequencenumbers were different. DDSI2
               increments the sequencenumber for each message it sends, this
               sequencenumber is unrelated to the message sequencenumber which
               is not communicated to the receiving node when using DDSI2.
              </br>
              </br>
              <b>Solution: Communicate the message sequencenumber to the receiving
              node. Add a PrismTech specific flag to the SPDP to indicates that
              the message sequencenumber is send. Add the message sequencenumber
              to all messages transferred using DDSI2. Based on the existence of
              the PrismTech specific flag copy either the message sequencenumber
              or the DDSI2 sequence number into the internal messages.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3151-2<br/>
          </td>
          <td>
            <b>Publication/Subscription matched logic incorrect</b><br/>
            <i>
               On every non dispose Publication/Subscription matched message
               (with compatible reader/writer) the Publication/Subscription
               matched count was incremented. Only with a dispose
               Publication/Subscription matched message the Publication/Subscription
               matched count was decremented.
              </br>
              </br>
              <b>Solution: Now the Publication/Subscription matched count is only
              incremented when it's noticed for the first time or when QoS settings
              have become compatible when it was not before. The
              Publication/Subscription matched count is decremented on dispose
              Publication/Subscription matched message or on
              Publication/Subscription matched message when QoS settings are no
              longer compatible.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3452/<br>12411
         </td>
          <td>
            <b>Limiting sample size with DDSI2</b><br/>
            <i>
              <br>
              <b>Solution: DDSI2 now allows setting an upper limit to the allowed
              size of the serialised samples, as an added protection mechanism
              against running into memory limits. The limit is applied both on
              outgoing and on incoming samples, and any dropped samples are
              reported in the info log. By default, the limit is 1 byte short of
              2 GiB.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3462<br>
         </td>
          <td>
            <b>idlpp issues compiling in standalone C++ mode</b><br/>
            <i>
              Description: idlpp generated uncompilable code from anonymous sequence
              of sequences of basic IDL types following typedefs of those same
              basic types. It also produced incorrect definition of anonymous
              array slice types.
              <br><br>
              <b>Solution: idlpp generates the correct code in these circumstances.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3463<br>
         </td>
          <td>
            <b>Durability using KV persistency may report error while backing up</b><br/>
            <i>
              Description: When durability has been configured to use KV persistency
              and is backing up the persistent store an error may be reported when
              no data exists yet for a given name-space even though nothing goes wrong.
              <br><br>
              <b>Solution: The error message is not reported any more.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3512/<br>12427
         </td>
          <td>
            <b>Potential crash during initial alignment after a dispose_all_data call.</b><br/>
            <i>
              Description: The dispose_all_data call creates specific samples that
              were not compatible with durability alignment. The durability service
              could not handle these samples, while there are possible scenario's
              where these samples get stored in a persistent store. The service
              incorrectly forwarded all initial alignment data to the networking
              service, which could result in a crash since it could also not
              handle these samples, which are meant for local delivery only. A
              crash could also occur if the dispose_all_data sample was the first
              sample to be received, which could happen because of order reversal
              during alignment or in combination with lifespan QoS on the
              corresponding data.
              <br><br>
              <b>Solution: The durability service was modified to only exchange
              initial alignment data over the durability partition and not by
              directly delivering it to a networking service. Order reversal
              during initial alignment was changed such that samples are ordered
              first by timestamp, then by writer (instead of the other way around).
              Support was added for handling the case where it is still the first
              to be received, i.e. when the lifespan of data samples has expired.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3520<br>
         </td>
          <td>
            <b>Opensplice host and target type for Linux OS's has changed from
            x86.linux2.6 to x86.linux and x86_64.linux2.6 to x86_64.linux</b><br/>
            <i>
              Description: The installation path will be affected by this change,
              the top level directory on a linux platform would have been
              &ltARCH&gt.linux2.6 it will now be &ltARCH&gt.linux
              <br><br>
              <b>Solution: Before - PrismTech/OpenSpliceDDS/V6.3.2/HDE/x86_64.linux2.6/<br>
              Now - PrismTech/OpenSpliceDDS/V6.4.0/HDE/x86_64.linux/</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3587<br>
         </td>
          <td>
            <b>Durability exposes and aligns local DDSI2 partitions</b><br/>
            <i>
              Description: The DDSI2 service creates some local partitions
              formatted as '__NODE<ID>BUILT-IN PARTITION__' that are purely local
              to the federation. The durability service however aligns them with
              others causing the local data to be exposed to other federations as well.
              <br><br>
              <b>Solution: The durability service now checks whether a partition
              has the aforementioned format and refrains from exposing them to
              other durability services.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3601<br>
         </td>
          <td>
            <b>Extend support for network service data compression and durability datastores.</b><br/>
            <i>
              Description: Windows, Enterprise Linux and Solaris distributions
              now include support for zlib, lzf and snappy compressors in the
              networking service, and for LevelDB (not windows) and SQLite (not solaris)
              datastore plugins for durability.
              <br><br>
              <b>Solution: Extra platform support added.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3603<br>
         </td>
          <td>
            <b>Various internal trace messages are reported in ospl-info.log</b><br/>
            <i>
              Description: The ospl-info.log shows various messages about internal
              threads being started and stopped. These messages make no sense nor
              are they relevant to users. Also these message makes it harder to
              find the actual messages that are important.
              <br><br>
              <b>Solution: These internal messages are no longer reported.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3625/<br>12529
         </td>
          <td>
            <b>Spliced could crash when terminating under abnormal circumstances</b><br/>
            <i>
              Description: The spliced exit handling must stop all OpenSplice
              threads accessing shared memory before detaching from the shared
              memory. However, if processes have been killed using the KILL signal,
              this did not always happen correctly because spliced would
              incorrectly assuming the shared memory to still be in use.
              <br><br>
              <b>Solution: The termination code now ensures thread termination.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3644<br>
         </td>
          <td>
            <b>Durability service may perform alignment multiple times</b><br/>
            <i>
              Description: The durability service aligns sample per partition-topic
              combination and in some cases could perform this alignment multiple times.
              <br><br>
              <b>Solution: The durability service now checks actively whether it
              already performed alignment for a given partition-topic combination
              before initiating the alignment.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3755-1<br/>12573
          </td>
          <td>
            <b>Durability service does not terminate in time.</b><br/>
            <i>
              Termination hung because listener termination is unable to stop
              with active listener actions. Listener actions remained active
              because they were unaware of termination.
              </br>
              </br>
              <b>Solution: Listener actions are now aware of termination and stop.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3755-2<br/>12573
          </td>
          <td>
            <b>Services are able to outlive the splice daemon</b><br/>
            <i>
              When the splice daemon terminated without the use of the ospl tool,
              it was possible that a service outlived the splice daemon.
              </br>
              </br>
              <b>Solution: The splice daemon now kills all services which remain
              alive after the service terminate period elapsed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3755-3<br/>12573
          </td>
          <td>
            <b>After failure action systemhalt it was possible that shared memory
             was not cleaned-up/deleted.</b><br/>
            <i>
               When a service failed with failure action systemhalt set it was
               possible that shared memory and key-file were not cleaned-up/deleted.
               This occurred because the splice daemon incorrectly assumed that
               the died service was unable to decrease the kernel attach count
               upon its termination, the attach count was decreased twice
               causing failing calls to shared memory cleanup/deletion.
              </br>
              </br>
              <b>Solution: The splice daemon no longer assumes died services
              are incapable to preform proper termination, it now only decreases
              kernel attach count during termination when all services are terminated.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3761/4002<br>/12570
         </td>
          <td>
            <b>OSPL_LOGPATH included in host:port check for tcp logging mode</b><br/>
            <i>
              Description:  Log file names are checked for host:port combinations
              twice. The second check is done when the path prefix and log file
              name are concatenated, which leads to incorrect behavior if the
              value specified in OSPL_LOGPATH contains a colon.
              <br><br>
              <b>Solution: Split prefix and log file name before checking for a
              host:port combination.</b>
            </i>
          </td>
        </tr>

        <tr>
         <td>
            OSPL-3786<br>
         </td>
          <td>
            <b>Latency spikes on reliable channel</b><br/>
            <i>
              Description: On some occasions the latency on a reliable channel
              would spike periodically (at most once per resolution) due to a
              mechanism used to limit bandwidth kicking in even when the limit
              didn't need to be enforced.
              <br><br>
              <b>Solution: The logic has been enhanced to only activate the
              mechanism when bandwidth needs limiting.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3791/<br>12576
         </td>
          <td>
            <b>More strict SAC idlpp sequence support functions creation</b><br/>
            <i>
              Description: SAC idlpp creates support functions for sequences
              (like allocbuf()). These are created when an idl file defines an
              actual sequence (like sequence<Type>) but also when the type is
              related to a Topic (in other words, '#pragma keylist' is added to
              the type) to be able to create readers and writers. When idl file
              A contains a topic Type and idl file B includes A and defines a
              sequence of that topic, then sequence support functions are created
              in both A and B result files. The effect is that the generated code
              doesn't compile due to 'multiple definitions'.
              <br><br>
              <b>Solution: When creating a sequence, check if the sequence is within the
              same file as the type. If not, check if the type has a keylist
              related to it. If so, then the type is a topic and the sequence
              support function have already been created: do not create them a
              second time.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3851/<br>12627
         </td>
          <td>
            <b>DDSI uses more ports beyond those specified in the Deployment Guide</b><br/>
            <i>
              Description: The Deployment Guide describes exactly which set of
              ports is used by DDSI and how this set can be configured. Some
              versions of OpenSplice (6.3.x except 6.3.0) additionally used two
              or more (one more than the number of configured channels)
              kernel-allocated port numbers strictly for transmitting data.
              <br><br>
              <b>Solution: The use of the additional ports has been eliminated
              and the behaviour is in line with the deployment guide again.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3853<br>
         </td>
          <td>
            <b>Improve the performance of the waitset wait operation.</b><br/>
            <i>
              Description: The performance of the waitset wait operation can be
              improved by evaluating the conditions trigger status within the
              kernel layer.
              <br><br>
              <b>Solution: Evaluate the trigger status of the conditions
              attached to the waitset within the kernel layer.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3860<br>
         </td>
          <td>
            <b>Remove unnecessary allocation of a timestamp when updating the
            deadline administration.</b><br/>
            <i>
              Description: When updating the deadline information of a writer
              or reader a new timestamp is allocated. By using the timestamps
              already present in the corresponding sample the extra timestamp
              allocation can be removed.
              <br><br>
              <b>Solution: Use the timestamps present in the sample when updating
              the deadline information of the corresponding instance.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3861<br>
         </td>
          <td>
            <b>Improve the performance of the read/take operations by updating
            the corresponding administration without extra memory allocations.</b><br/>
            <i>
              Description: A read or take operation will update the reader
              administration. For this update memory is allocated. The performance
              of the read or take operation can be improved by removing the extra
              memory allocation.
              <br><br>
              <b>Solution: Update the reader administration without allocating
              temporal memory during this update.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3993<br>
         </td>
          <td>
            <b>The Java QosProvider constructor may throw a NullPointerException</b><br/>
            <i>
              Description: When a parse-error occurs, the Java constructor explicitly
              throws a NullPointerException. This is not in line with the other
              API's and the language-mapping.
              <br><br>
              <b>Solution: The QosProvider constructor doesn't throw a
              NullPointerException anymore. Instead the constructor always
              succeeds and subsequent invocations on the QosProvider will
              return DDS.RETCODE_PRECONDITION_NOT_MET.The API furthermore has more
              thorough error-checking within JNI. If an exception occurs,
              DDS.RETCODE_ERROR is returned instead of an exception.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3997<br>
         </td>
          <td>
            <b>Networking defragmentation buffers refcount issue</b><br/>
            <i>
              Description: Static analysis of RTnetworking code revealed a potential
              issue with the administration of the defragmentation buffers. An
              atomically modified counter was accessed without atomic access,
              allowing a potential race-condition.
              <br><br>
              <b>Solution: The counter is correctly accessed now.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-4007<br>/12660
         </td>
          <td>
            <b>Workaround for issue when using Jamaica VM</b><br/>
            <i>
              Description: When OpenSplice is used with JamaicaVM, JamaicaVM
              crashes due to a differnece in how a jni call to NewStringUTF is
              handled.
              <br><br>
              <b>Solution: A workaround is implemented in OpenSplice to assure
              that JamaicaVM does not crash anymore.</b>
            </i>
          </td>
        </tr>
      </table>

       <h2>6.3.3p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-3887<br/>
          </td>
          <td>
            <b>SAC QoS-provider doesn't return DDS_RETCODE_NO_DATA</b><br/>
            <i>
              When a QoS cannot be found, the SAC QoS-provider returns
              DDS_RETCODE_ERROR instead of DDS_RETCODE_NO_DATA.<br/><br/>
              <b>Solution: The API has been changed to return DDS_RETCODE_NO_DATA
              when a QoS cannot be found.</b>
            </i>
          </td>
        </tr>
       </table>
       <h2>6.3.3p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-3732/<br/>12565
          </td>
          <td>
            <b>DCPSPublication built-in Topic published unnecessarily</b><br/>
            <i>
              Even when publication of built-in topics is disabled through the
              configuration file, the DCPSPublication instance that corresponds
              to a DataWriter is still write-disposed and unregistered as
              volatile data when the DataWriter is deleted to allow DataReaders to
              clean up the resources associated with that DataWriter. However,
              if no instances are registered by a DataWriter at the time it
              is deleted, there is no need to publish it.<br/><br/>
              <b>Solution:If built-in topics are disabled through
              configuration and no instances are registered by a DataWriter when
              it is deleted, no more write-dispose and unregister of the
              DCPSPublication are performed.</b>
            </i>
          </td>
        </tr>
       </table>
     </p>
     <p>
       <h2>6.3.3p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
         <td>
            OSPL-2452<br>
         </td>
          <td>
            <b>Sample loss using DDSI2 and readers with resource limits</b><br/>
            <i>
              Description: DDSI2 could silently drop samples destined for readers
              at their resource limit, rather than blocking the writer and/or
              notifying the application or writing messages in the log.
              <br><br>
              <b>Solution: DDSI2 now blocks the data stream until the reader
              accepts the data, but for an unresponsive reader, it will
              eventually drop the data anyway. When this happens, a message to
              that effect will be logged.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-2823/3626/<br>12526
         </td>
          <td>
            <b>Sample lost administration memory leaks and unlocked modifications</b><br/>
            <i>
              Description: The internal administration related to the sample lost
               mechanism would leak some memory each time it was accessed. Also it
               could be accessed by multiple threads at the same time without
               any locking, which could potentially lead to undefined behaviour.
              <br><br>
              <b>Solution: The issues were resolved by freeing the administration
              when required to prevent memory leaks, and only allowing access
              while a lock is taken.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-2960/<br>12243
         </td>
          <td>
            <b>Improved Durability alignment through DDSI2</b><br/>
            <i>
              Description: The Durability Service assumes that all writer/reader connections
              are available when one is discovered. DDSI2 however discovers one
              by one. This means that Durability sometimes sends data which DDSI2
              will drop (because there's no related connection yet), which will
              cause Durability to never complete alignment. Also, DDSI2 had a
              problem that would sometimes increase the discovery time dramatically,
              which would trigger the Durability alignment issue.
              <br><br>
              <b>Solution: The Durability now creates all readers before enabling
              the listeners connected to them, meaning that required writer/reader
              connections are discovered before acting on incoming data.
              The DDSI2 service increases the writer heartbeat when a new reader
              is detected, making the writer/reader connection discovery more robust
              and quicker. </b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3186<br>
         </td>
          <td>
            <b>DDSI2 could use freed memory in its reordering of incoming samples</b><br/>
            <i>
              Description: The sample re-ordering mechanism internal to DDSI2 could
              use freed memory under very particular circumstances, requiring the
              number of buffered samples to have reached its maximum size as well
              as a particular order of arrival of out-of-sequence samples at the
              high end of the sequence number range. This is unlikely to occur
              in normal circumstances, using networks of "normal" reliability
              (such as Ethernet) and with receivers that generally keep up with the data flow.
              <br><br>
              <b>Solution: The algorithm has been adjusted to avoid referencing freed memory.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3187<br>
         </td>
          <td>
            <b>DDSI2 fails to work with small WHC water mark settings</b><br/>
            <i>
              Description: DDSI2 would completely fail to work when the WHC water
              marks were set very low. More precisely, if DDSI2 would block because
              the number of outstanding unacknowledged messages (N_UNACK) exceeded
              the high water mark, and its dynamic message packer had buffered so
              many samples that the readers would not yet be able to acknowledge
              enough samples to drop N_UNACK below the low water mark,
              communication would stop. This also affects endpoint discovery.<br>

              One of the consequences of the inability to operate with low water
              mark settings meant that it was impossible to operate DDSI2 in the
              safest configuration, where each sample must be acknowledged before
              sending the next, thereby potentially causing many more
              retransmissions. This is especially problematic if the network is
              unreliable, or the receivers are unable to keep up with the influx of data.
              <br><br>
              <b>Solution: The samples are now always flushed to the network before blocking.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3470/<br>12422
         </td>
          <td>
            <b>Compatible typeSupport is rejected without cause</b><br/>
            <i>
              Description: When a type was being registered that conflicted with
              a type that was registered earlier (for example because they had a
              conflicting keyList), then a PRECONDITION_NOT_MET was being
              returned, but no descriptive message was included as to the root cause.
              <br><br>
              <b>Solution: A message is now written into the error log and also
              accessible through the ErrorInfo interface.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3509<br>
         </td>
          <td>
            <b>Minimum networking ThrottleLimit is based on networking FragmentSize</b><br/>
            <i>
              Description: When the fragmentsize > throttlelimit, a scenario with
              high load can cause the throttlevalue to go below the fragmentsize.
              At that point networking is unable to send any messages.
              <br><br>
              <b>Solution: The minimum value of the
              "OpenSplice/NetworkService/Channels/Channel/Sending/ThrottleLimit" config
              element is now based on the "OpenSplice/NetworkService/Channels/Channel/FragmentSize" value.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3528/<br>124264
         </td>
          <td>
            <b>Excessive Lease manager logging</b><br/>
            <i>
              Description: In some cases where a writer was using the
              autounregister_instance_delay, the info log file would be flooded
              with messages stating that the leaseManager did not wake up in time.
              <br><br>
              <b>Solution: Logging has been cleaned up.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3596
         </td>
          <td>
            <b>RnR storage has problem replaying types containing arrays</b><br/>
            <i>
              The RnR storage does not correctly handle types which contain
              bounded array's or sequences. When the array or sequence contain
              references then the corresponding reference counts are not
              properly handled.
              <br><br>
              <b>Solution: The algorithm now correctly updates the reference
              counts when objects are copied. This includes references
              contained in bounded array's and sequences.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3636/<br>12534
         </td>
          <td>
            <b>Communication does not restart after ethernet cable unplug/replug</b><br/>
            <i>
              Description: When the network cable is unplugged and then replugged
              the reliable communication is not restored when reconnect is
              enabled and discovery is disabled.
              <br><br>
              <b>Solution: When discovery is disabled the send component of a channel
              detects the network communication problem and the recovery of the
              network communication. This status change should also be communicated
              to the receive component of the channel which should reset it's
              status in case a reconnect occurs.</b>
            </i>
          </td>
        </tr>
       </table>




       <h2>6.3.3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
         <td>
            OSPL-3281/<br>12378
         </td>
          <td>
            <b>To monitor the behaviour of the
            networking service and the durability service additional statistics of the
            internal queues have been added.</b><br/>
            <i>
              Description: The networking service maintains internal queues to
              provide reliability and fragmentation/defragmentation. The
              durability service maintains a queue for persistent data that has
              to be stored on disk. To monitor the behaviour of these queues extra
              statistics have been added which will be available through the C&M API.
              <br><br>
              <b>Solution: For the networking service additional statistic counters
              are available on the queues used by the resend administration (ResendQueue), the
              reorder administration (ReorderQueue) and the network queue between
              the writers and the networking service. For the durability service
              statistic counters are now on the persistent data queue (GroupQueue)</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3579<br>
         </td>
          <td>
            <b>Durability service may crash when aligning historical data</b><br/>
            <i>
              Description: In some cases while aligning historical data with another
              durability service, some variable in an internal algorithm may not
              be initialised, but used and freed later on anyway. This causes
              undefined behaviour, but mostly leads to a crashing durability service.
              <br><br>
              <b>Solution: The internal algorithm has been modified to initialize
              the aforementioned variable in any case.</b>
            </i>
          </td>
        </tr>
      </table>


       <h2>6.3.2p4</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
         <td>
            OSPL-2362/<br>11911
          </td>
          <td>
            <b>Ownership strength changes in DataWriter's QoS go unnoticed</b><br/>
            <i>
              Decreasing or increasing ownership strength would have no effect on
              ownership strength registered with instances on DataReader.
              <br><br>
              <b>Solution: Take into account ownership strength during evaluation
              of QoS updates.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-2480/<br>11804
          </td>
          <td>
            <b>When an idl files contains "too many" nested modules idlpp crashes
            when Java code is generated from the idl files.</b><br/>
            <i>
              If an IDL file contains "too many" nested modules the IDL preprocessor
              idlpp crashes when Java is generated. This is because in Java nested
              modules will lead to the generation of path names containing the
              names of modules: the deeper the nesting, the longer the path names
              will become. Because a fixed size container for path names was used,
              an overflow would lead to memory failure which could result in a crash.
              <br><br>
              <b>Solution: By dynamically allocating the path names there is
              always enough room available to hold the complete path of a module,
              thus preventing an overflow.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-2482<br>
          </td>
          <td>
            <b>Removed possible deadlock in d_storeGroupStoreXML if result is
            D_STORE_RESULT_PRECONDITION_NOT_MET or D_STORE_RESULT_ILL_PARAM</b><br/>
            <i>
              Lock on peristent XML store was not unlocked in case of a
              D_STORE_RESULT_PRECONDITION_NOT_MET or D_STORE_RESULT_ILL_PARAM
              result. This would have caused a deadlock.
              <br><br>
              <b>Solution: Lock is now always unlocked in all cases.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-2661<br>
          </td>
          <td>
            <b>Crash when using reliable-under-publisher-crash (RUPC) functionality</b><br/>
            <i>
              When a publisher node crashes and RUPC is enabled then it appears
              that a node that has not received all messages from the crashing
              publisher is not updated by the other nodes. The problem occurs
              because the announce messages that the RUPC function uses are
              sent to the wrong network addresses.
              <br><br>
              <b>Solution: The announce messages used by the RUPC protocol have
              to be sent to all addresses associated with the global partition.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3084/<br>12328
          </td>
          <td>
            <b>C++ copyIn/copyOut code generated from idl containing sequences
            of anonymous sequences failed</b><br/>
            <i>
              idlpp generated invalid code for sequences of anonymous sequences.
              The copyIn/copyOut routines generated did not recurse into sequences
              and kept overwriting the base sequence resulting in memory corruption.
              <br><br>
              <b>Solution: Using the loop index to indicate sequence depth during
              code generation produces copyIn/copyOut routines that recurse into sequences.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3085/<br>12326
          </td>
          <td>
            <b>Cast warning in generated idlpp c++ code due to cast from const
            pointer into non const pointer</b><br/>
            <i>
              The generated c++ code from idlpp contains a copy function for
              arrays (if present) which cast away a const pointer into a non
              const pointer. This will cause a warning with strict compiler
              warnings set.
              <br><br>
              <b>Solution: Generated copy code in c++ for arrays contains now
              also a const pointer for the from pointer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3222/<br>12371
          </td>
          <td>
            <b>Available traces for the throttling mechanism enhanced to be less performance
            intrusive</b><br/>
            <i>
              The trace level needed to obtain information on the throttling mechanism
              could be performance intrusive.
              <br><br>
              <b>Solution: A new tracing category "Throttling" has been introduced
              for both RTNetworking and secure networking, which allows throttling
              traces to be enabled separate from the other categories. On level 1
              throttling traces are only emitted on change.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3280/<br>12379
          </td>
          <td>
            <b>The domain service heartbeat properties should be made configurable seperately.</b><br/>
            <i>
              Currently the properties of the domain service heartbeat are
              controlled by the settings of the lease element. We want to split this
              coupling because it is desirable to specify the heartbeat frequency
              independently. Further it should allow to specify the heartbeat
              transport priority and the scheduling parameters of the heartbeat
              sending thread.
              <br><br>
              <b>Solution: Added a Heartbeat configuration item to the Daemon
              element of the Domain service configuration which allows to
              specify the frequency (expiry time and update factor) of the
              heartbeat, the transport priority QoS setting of the heartbeat
              writer and the scheduling parameters of the heartbeat sending thread.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3284/<br>12382
          </td>
          <td>
            <b>Manual start of networking doesn't allow communication.</b><br/>
            <i>
              The kernel group write is only allowed if the number of registered
              services is equal to the number found in the configuration. This is
              done to be sure all networking services are up and running before
              actually doing group writes. Writes before they are up and running
              are done again later with a resend, but where a manual start of
              working is done later, communication doesn't occur.
              <br><br>
              <b>Solution: The number of registered services must be equal or
              greater then the number found in the configuration to perform a
              write. A manually started, but later networking service will now join
              the system.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3355/<br>12382
          </td>
          <td>
            <b>When an aligner for a namespace appears and durability
            services exist that do not have a aligner for this namespace then no
            conflict is detected and no merge action is triggered, although it should.</b><br/>
            <i>
              In the scenario where a durability service has its 'aligner="false"'
              -property set for a namespace and its last available aligner leaves,
              the durability service will notice that no aligner is available
              anymore. When an aligner (re)appears for the namespace the
              durability service should perform the merge action as specified in
              the configuration, because the aligner may have "injected" new
              data in the system.
              <br><br>
              <b>Solution: When the last aligner for a namespace has left, the
              state for the namespace and the role will be cleared. When a new
              aligner arrives its initial state for the namespace is set to zero.
              Because the cleared state differs from the initial state a
              namespace state conflict is detected as soon as an aligner appears
              and a merge action is performed (when specified in the configuration).</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3466/<br>12420
          </td>
          <td>
            <b>Report plugin configuration not applied to applications</b><br/>
            <i>
              In certain scenario's, a user-defined report plugin was only used
              by OpenSplice services and not by applications using OpenSplice.
              <br><br>
              <b>Solution: The issue was fixed and the report-plugin
              configuration is now also used by applications</b>
            </i>
          </td>
        </tr>
      </table>

       <h2>6.3.2p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>
            OSPL-350/<br>bugzilla-44
          </td>
          <td>
            <b>Segmentation fault when writing or registering a sample with a null
            member in Java</b><br/>
            <i>
              saj_cfoiStruct did not check if structObject is NULL, which would
              result in a segmentation fault when writing or registering a sample
              with a null member in Java
              <br><br>
              <b>Solution: The saj_cfoiStruct function now checks if structObject
               is NULL.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-492/<br>bugzilla-49
          </td>
          <td>
            <b>Invalid conversion of multidimensional arrays from IDL to C#</b><br/>
            <i>
              Before invalid code was generated for multidimensional arrays,
              sequences of arrays, and operations on them for C#
              <br><br>
              <b>Solution: Some functions have been updated and use
              of snprintf has been removed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-2243/<br>11714
          </td>
          <td>
            <b>Resend of unregister message may cause a crash.</b><br/>
            <i>
              When an unregister message is rejected by the RT networking service then
              the resend of the unregister message may cause a crash because the
              corresponding instance administration is already freed.
              <br><br>
              <b>Solution: The RT networking service should not reject an unregister
              message which is consistent with the behaviour of the other readers.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-2799<br>
          </td>
          <td>
            <b>Maximum termination wait time is displayed incorrectly when doing ospl stop</b><br/>
            <i>
              When terminating OpenSplice with ospl stop a message appears on the
              command line with the maximum waiting time. This message was not in
              line with the possible configured ServiceTerminatePeriod.
              <br><br>
              <b>Solution: The maximum termination time will now be displayed correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-2789<br>
          </td>
          <td>
            <b>Compression may fail on Linux when using a large FragmentSize in
            RTNetworking</b><br/>
            <i>
              When a large FragmentSize was configured in RTNetworking, the
              service might run out of stack on Linux platforms.
              <br><br>
              <b>Solution: Memory needed for handling the (de-)compression of
              packets is now (pre-)allocated on heap.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-2865/<br>12165
          </td>
          <td>
            <b>RT Networking dies when the status of an unused network interface
            changes to down.</b><br/>
            <i>
              The RT networking service is reported dead when the status of a
              network interface that is not used changes its status; for example
              is configured down. This network state change triggers an event
              within the RT networking service which is not handled correctly
              which causes that the network service stops updating it's lease.
              <br><br>
              <b>Solution: The function checks the network interface status
              and correctly handles the network interface status events.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3023.1<br>
          </td>
          <td>
            <b>Could not add priority_kind to Domain/GeneralWatchdog/Scheduling/Priority in osplconf</b><br/>
            <i>
              osplconf did not know about priority_kind for Domain/GeneralWatchdog/Scheduling/Priority.
              <br><br>
              <b>Solution: osplconf metadata is updated.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3023.2<br>
          </td>
          <td>
            <b>OpenSplice_DeploymentGuide.pdf referred to old style configuration paths</b><br/>
            <i>
              OpenSplice_Deployment.pdf instructed the user to use
              Domain/Daemon/GeneralWatchdog, but user should use Domain/GeneralWatchdog.
              <br><br>
              <b>Solution: Documentation is correct now</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3023.3<br>
          </td>
          <td>
            <b>DDSI2 and DDSI2E services did not recognize both Scheduling element
            and the priority_kind attribute</b><br/>
            <i>
              The parser for both services had no notion of the Scheduling element,
              while the common code in the user layer requires it in order to work.
              <br><br>
              <b>Solution: The parser code is updated now and the osplconf
              metadata file is corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3053/<br>12324
          </td>
          <td>
            <b>Crash on deletion of DataWriter.</b><br/>
            <i>
              In some cases where a Reader or a Writer with an activated deadline
              or auto_unregister policy was being destroyed, the leaseManager
              would still try to notify about missed deadlines or actively send
              unregister messages to an already partially deleted Reader or
              Writer. This rarely occurred, but it is clearly unwanted and might
              crash or corrupt the system.
              <br><br>
              <b>Solution: By now stopping the deadline and auto_unregister
              algorithms before the actual destruction of the Reader or Writer,
              this particular sequence of events should no longer occur.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3209/<br>12368
          </td>
          <td>
            <b>Durability may fail to align non-volatile data when configuring
            delayed alignment for one or more name-spaces</b><br/>
            <i>
              With the delayed alignment enabled, durability accepts the
              introduction of a new persistent data-set in the system even after
              the start-up phase in case no data has been re-published from
              permanent storage by any durability service and no application has
              published any sample so far either. When a new data-set is detected
              in the operational phase and delayed alignment is required, the
              durability service marked all partition-topic combinations as
              incomplete instead of only marking the ones that match the name-space
              for which delayed alignment is required. As a result the
              partition-topic combinations that do not belong to the name-space
              will never be marked as complete again. In case another durability
              service wants to align from this durability service after that, it
              concludes that the set of data over there is incomplete, where it
              actually is complete. This leads the alignment process on the
              newly joining node to fail.
              <br><br>
              <b>Solution: The algorithm that marks the groups as incomplete
              (when detecting that delayed alignment needs to take place for a
              given name-space), has been modified to only mark those
              partition-topic combinations that match the name-space.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3224<br>
          </td>
          <td>
            <b>Durability KV store may access memory that is already freed.</b><br/>
            <i>
              When cleaning up the instance administration it may occur that an
              already freed reference is accessed.
              <br><br>
              <b>Solution: Set the freed reference to NULL and check if it is
              not NULL when accessing it.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3254<br>
          </td>
          <td>
            <b>Failed termination messages on windows when closing OpenSplice</b><br/>
            <i>
              When terminating OpenSplice with ospl stop on windows a number of
              messages with the following text "Failed to send the SIGTERM signal
              to the splice daemon process 0" could appear in the info log file.
              These messages appear for each service that is configured in the
              configuration file. In fact, the service is correctly terminated.
              <br><br>
              <b>Solution: These messages will no longer be reported when the
              services are terminated correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3290<br>
          </td>
          <td>
            <b>When rebuilding custom_libs with Visual Studio the dll files in
            $OSPL_HOME/bin are not replaced.</b><br/>
            <i>
               When rebuilding custom_libs with Visual Studio the dll files
               in bin were not replaced. The dll files were produced to the
               $OSPL_HOME/lib directory and required manually copying into the
               $OSPL_HOME/bin directory.
              <br><br>
              <b>Solution: When rebuilding custom_libs with Visual Studio the
              dll files in $OSPL_HOME/bin are replaced.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3342<br>
          </td>
          <td>
            <b>Potential uninitialized memory access in the Java language-binding</b><br/>
            <i>
              In the Java language binding there were (internal) error paths in
              which uninitialized values could be returned.
              <br><br>
              <b>Solution: The return-values have been initialized for the
              error-case aswell.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
            OSPL-3351<br>
          </td>
          <td>
            <b>RT Networking : Parallel demarshalling administration may leak</b><br/>
            <i>
              Due to an error in a method used by the cleanup routines to access
              the parallel demarshalling administration for Java, the related
              administration may leak if the number of threads is changed or the
              application stops.
              <br><br>
              <b>Solution: The error in the cleanup routines is resolved
              ensuring that the right pointer is returned.</b>
            </i>
          </td>
        </tr>
       </table>
     </p>

     <p>
       <h2>6.3.2p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>
            OSPL-2283/<br>11750
          </td>
          <td>
            <b>Memory leak in lookup_participant</b><br/>
            <i>
              The domain identifier was not freed.
              <br><br>
              <b>Solution: The domain identifier is now freed before leaving
              gapi_domainParticipantFactory_lookup_participant.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3158/<br>12351
          </td>
          <td>
            <b>Memory leak after deleting a waitset</b><br/>
            <i>
              The common destructor in GAPI did not free object if object was of type waitset.
              <br><br>
              <b>Solution: Memory leak fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3188<br>
          </td>
          <td>
            <b>Error in throughput measurement of streams example</b><br/>
            <i>
              The streams example measures throughput by taking the total amount
              of samples divided by the amount of time in one run. It consists
              out of a reader and a writer process which are started
              independently. Previously the reader would start the
              time-measurement when the process started, even though the
              writer-process was not yet running. This resulted in throughput
              that was too low because more time was spent measuring than was
              spent sending data.
              <br><br>
              <b>Solution: The example now starts the measurement when it
              receives the first sample.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3198<br>
          </td>
          <td>
            <b>Durability could crash during startup when doing initial merge</b><br/>
            <i>
              Due to a timing dependency on whether the role of a fellow was set the durability
              service sometimes assumed that the role was available when it wasn't, resulting
              in trying to read a NULL-pointer. Additionally, the service would use a fast
              spinning loop to determine whether communication with another fellow was approved
              and would loop forever if the fellow would not be approved.
              <br><br>
              <b>Solution: The role is now only accessed when it is available. Furthermore the
              spinning loop that determines whether communication with another service is possible
              is made less cpu-hungry (by introducing a sleep) and the loop skips a service for
              which the communication state is not approved instead of looping forever.</b>
            </i>
          </td>
        </tr>
        </tr>
            <tr>
          <td>
            OSPL-3200<br>
          </td>
          <td>
            <b>Consistent final value not always guaranteed with BY_SOURCE_TIMESTAMP</b><br/>
            <i>
              Where a single writer updates an instance with the same timestamp,
              a consistent final value for that instance was not guaranteed
              across all subscribers. Also when a time with all zeroes was supplied,
              the actual time would be used instead of the supplied time.
              <br><br>
              <b>Solution: When updating the administration of the readers the
              consistent final value is guaranteed by incorporating a writer-generated
              sequence number and time zero has no special meaning anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-3218<br>
          </td>
          <td>
            <b>Streams API returns old sample multiple times</b><br/>
            <i>
              After the streams get_w_filter API call returned NO_DATA,
              administration in the StreamReader caused a consecutive
              get_w_filter to return the last received sample. This pattern
              (NO_DATA, last sample) repeated itself until new data was
              received.
              <br><br>
              <b>Solution: The administration is now left in a correct state
              after NO_DATA is returned.</b>
            </i>
          </td>
        </tr>
       </table>
     </p>
     <p>
       <h2>6.3.2p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-3096</td>
          <td>
            <b>In single-process deployment CTRL-C doesn't work</b><br/>
            <i>
              Due to a signal-handler being overruled on POSIX systems,
              termination requests like CTRL-C (e.g., the signals SIGINT,
              SIGQUIT, SIGTERM, SIGHUP and SIGPIPE) would not result in an
              immediate stop of the application.
              </br>
              </br>
              <b>Solution: The signal-handler for single-process deployments is
              no longer overruled by handlers that don't stop the application.
               </b>
            </i>
          </td>
        </tr>
       </table>
     </p>
     <p>
       <h2>6.3.2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-10<br/>4508
          </td>
          <td>
            <b>TypeSupport with invalid type name causes crash during register_type</b><br/>
            <i>
              When a type support object is created with an type name which is not
              known in the meta database the register_type function crashes.
              </br>
              </br>
              <b>Solution: Code change made to prevent the crash and doc updated
              to improve descriptin of register_type.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1430<br/>7255
          </td>
          <td>
            <b>Durability Service behaviour with no aligner</b><br/>
            <i>
              The current behaviour of a durability service is: when no aligner
              is available a durability service that cannot act as aligner itself
              will wait until an aligner becomes available. This is not desirable
              in all cases.

              When no aligner is available a durability service that cannot act
              as aligner itself will wait until an aligner becomes available. If
              no aligner becomes available, the durability service will wait forever.
              In some situations it may be desirable to exit instead. This behaviour
              has been made configurable using the TimeToWaitForAligner-option.
              Currently two values are supported, 0.0 (exit if no aligner is available)
              and 1.0 (wait until an aligner becomes available). The default is
              1.0 which matches the original behaviour.
              When the durability service exits error code 1 (recoverable error) is returned.
              </br>
              </br>
              <b>Solution: The behavior of the durability service is configurable
              when no aligner is present.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1431<br/>10964
          </td>
          <td>
            <b>idlpp multiple prefix support for Java</b><br/>
            <i>
              idlpp did not support prefixing multiple modules for the java
              language binding using the -j option.
              </br>
              </br>
              <b>Solution: idlpp now supports prefixing multiple modules.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2307-1<br/>11751
          </td>
          <td>
            <b>Recursively resolved header files all present in top level header file.</b><br/>
            <i>
              idlpp would include all generated header files for C++ for which
              it found an idl preprocessor directive.
              </br>
              </br>
              <b>Solution: idlpp generated C++ code does not include all
              recursively resolved header files anymore and instead only
              references the top level include file.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2307-2<br/>11751
          </td>
          <td>
            <b>idlpp sometimes forgets top level line markers in preprocessor output</b><br/>
            <i>
              idlpp would forget to include top level line markers in preprocessor
              output if the included file contained a idl preprocessor include
              directive itself and no actual idl code was declared before the
              include directive.
              </br>
              </br>
              <b>Solution: idlpp preprocessor now prints a line marker for the
              file it's in, as soon as it finds a preprocessor include directive
              and the line marker wasn't printed yet.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2696<br/>11998
          </td>
          <td>
            <b>REPLACE and DELETE merge policy</b><br/>
            <i>
              </br>
              </br>
              <b>Solution: With the REPLACE merge policy it is possible to dispose
              and delete historical data on a node, and replace it with the
              transient and persistent data from another node. Immediately after
              successful completion of the REPLACE merge action the replacement
              data will be available to late joining readers, the data in the
              reader queue of existing readers will be disposed and replaced with
              the replacement data, and the generation count of the replacement
              data is increased.<br><br>
              With the DELETE merge policy it is possible to dispose and delete
              historical data on a node. Immediately after successful completion
              of the DELETE merge action the historical data in the reader queue
              of existing readers will be disposed and is not available any more
              to late joining readers.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2705<br/>12008
          </td>
          <td>
            <b>Java deadlock during shutdown</b><br/>
            <i>
              Newly spawned thread in shutdown hook to delete contained entities
              causes a deadlock.
              </br>
              </br>
              <b>Solution: In OSPL v6 a user should not request an exit from within a listener thread (until the final
               solution has been implemented - expected in v7). A solution is to
               spawn a thread that calls System.exit() instead of calling the
               method from within the Listener callback itself. OSPL now tries to detect
               this deadlock, reports an error and calls system.halt.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2762<br/>12045
          </td>
          <td>
            <b>Deadlock on receiving SIGSEGV in Java language binding </b><br/>
            <i>
              The signal handler used to install the default signal handler and
              re-raise SIGSEGV. This caused JVM to pass a SIGABRT to the signal
              handler thread itself which would try to notify itself, and as a
              result end up in a deadlock.
              </br>
              </br>
              <b>Solution: Upon receiving a SIGSEGV (synchronous signal)
              asynchronously the signal handler thread does not change the signal
              mask and now invokes kill instead of raise to avoid possible deadlocks.
               </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2844<br/>12163
          </td>
          <td>
            <b>Alignment of transient data can cause a crash after several restarts
            on a second node where the first one keeps running</b><br/>
            <i>
               Node a is started and publishes transient data. Node b is started
               and aligns with the first node. Result is stored in persistent file.
               After several restarts of node b the internal hash table in the
               durability service is out of sync caused by the data from the
               persistent file and the data received from node a and causes a crash.
              </br>
              </br>
              <b>Solution: Internal hash table in durability service was out of sync with the reality.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2889<br/>12174
          </td>
          <td>
            <b>Writing data with empty strings in Java may not result in correct data being received in DDS</b><br/>
            <i>
               Due to an issue with the routines used by the Java
               language-binding to do empty-string interning, non-empty strings
               could show up in the data received in DDS.
              </br>
              </br>
              <b>Solution: The empty-string interning mechanism has been fixed
              to prevent this issue.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2891<br/>12167
          </td>
          <td>
            <b>Data reader statistic "numberOfSamplesTaken" not being updated</b><br/>
            <i>
              The "numberOfSamplesTaken" statistic was not being updated correctly
              in the Tuner.
              </br>
              </br>
              <b>Solution: Fix applied and Tuner works ok.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2894<br/>
          </td>
          <td>
            <b>Logger example failed to build on Solaris</b><br/>
            <i>
              The logger example failed to build on Solaris due to make complications.
              </br>
              </br>
              <b>Solution: Make file corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2900<br/>12235
          </td>
          <td>
            <b>Instances may be corrupted when using small reader_data_lifecycle.autopurge_*_delay</b><br/>
            <i>
               In case a durability_service.service_cleanup_delay > 0.0 is used
               for a Topic in combination with a very small value (or zero) as
               reader_data_lifecycle.autopurge_*_delay by a DataReader for the
               same Topic, instances may be corrupted in that DataReader during
               the delivery of historical instances in case they had been
               disposed prior to delivery already, no more live writers exist
               for it and of which the service_cleanup_delay has not expired
               yet.
              </br>
              </br>
              <b>Solution: The internal algorithm that deals with the above Qos
              policies has been altered and no longer causes memory corruption.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2904<br/>12236
          </td>
          <td>
            <b>OSPL_LOGAPPEND does not work as expected</b><br/>
            <i>
              The environment variable OSPL_LOGAPPEND does not work like expected.
              A "true" has the same effect as "false". Only if the variable
              is not defined do you get the append behaviour.
              </br>
              </br>
              <b>Solution: The evaluation of OSPL_LOGAPPEND has been corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2919<br/>12244
          </td>
          <td>
            <b>Opensplice DDS services always wait worst-case terminate period
            in case of a service crash</b><br/>
            <i>
              When using Opensplice DDS and for some reason a service crashes or
              is deliberately crashed, the Opensplice DDS services always wait
              the complete ServiceTerminatePeriod to exit.
              </br>
              </br>
              <b>Solution: This extra delay in the termination of services has now
              been resolved and the services will now close as soon as possible.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2972<br/>
          </td>
          <td>
            <b>Invalid configuration values on Windows 64-bit</b><br/>
            <i>
              On Windows 64-bit platforms, some configuration values were parsed
              incorrectly, leading to unexpected behaviour. For example the
              MaxBurstSize of the RTNetworking service didn't seem to work on
              Windows 64-bit platforms.
              </br>
              </br>
              <b>Solution: The parsing and printing of 64-bit values has been
              fixed for Windows 64-bit platforms.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2977<br/>
          </td>
          <td>
            <b>Appending to a stream may return RETCODE_TIMEOUT if flush limit is reached</b><br/>
            <i>
              Users of the Streams API should take into account that the append
              call may return RETCODE_TIMEOUT, as a result of an implicit flush
              that hits the writer resource limits.
              </br>
              </br>
              <b>Solution:  To handle this situation the following snippet of code is suggested:<br>
               <PRE><code>
              result = stream->append( ... );
              while (result == RETCODE_TIMEOUT) {
              result = stream->flush( ... );
              }</code></PRE></b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2979<br/>
          </td>
          <td>
            <b>Out of range user-friendly configuration values might result in strange values being used.</b><br/>
            <i>
              When a out-of-range user-friendly size expression with the suffixes
              k/K/m/M was used, the reported replacement value wasn't actually used.
              </br>
              </br>
              <b>Solution:  When an out-of-range value is specified,
              the reported replacement value is actually used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-3036<br/>12173
          </td>
          <td>
            <b>Applications using DCPS Java API may crash when creating a
            participant in case stack-traces are not available.</b><br/>
            <i>
              The DCPS Java API is trying to resolve the class name of the
              main-class as name of the participant. The algorithm that
              determines this information was not robust against stack traces
              not being available causing an ArrayIndexOutOfBoundsException.
              </br>
              </br>
              <b>Solution:  Made algorithm robust against not being able to
              inspect the stack. In this case a warning is issued in the logs
              and an alternative name is chosen based on process id.</b>
            </i>
          </td>
        </tr>
       </table>
     </p>
     <p>
       <h2>6.3.1p5</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-707/2546/2590/2735<br/>10333/11875/11882/12037
          </td>
          <td>
            <b>Crash in memory manager</b><br/>
            <i>
              In case of concurrent allocating and freeing small blocks distributed
              over the memory in a particular way, a free operation could shrink
              the region of used memory by multiple blocks of memory where it
              should have shrunk by only one. This in turn could lead to the
              shared memory allocator to allocate a block of memory twice, most
              of the time leading to a crash in the allocator itself.
              </br>
              </br>
              <b>Solution: The condition for shrinking the region has been fixed
              such that this can no longer occur. There is no effect on memory usage.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2246/<br/>11720
          </td>
          <td>
            <b>Topic content filter doesn't work with no-writers or disposed events</b><br/>
            <i>
               Data is not filtered as expected when using a content filtered
               topic and the events are disposed or the writer is deleted.
              </br>
              </br>
              <b>Solution: Changes have been made to the content filtered topic
              code so that now the instance part of the filter will be always be
              evaluated, but the data part of the filter will only evaluated
              when a valid sample is written.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2303/<br/>11761
          </td>
          <td>
            <b>OpenSplice DDS services always wait worst-case terminate period in case of a service crash.</b><br/>
            <i>
              When using Opensplice DDS and a service crashes or is deliberately crashed,
              the Opensplice DDS services always wait the complete ServiceTerminatePeriod to exit.
              </br>
              </br>
              <b>Solution: This extra delay in the termination of services has now been
              fixed and the services will now close as soon as possible.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2535<br/>
          </td>
          <td>
            <b>New reader statistic numberOfSamplesLost</b><br/>
            <i>
              </br>
              </br>
              <b>Solution: The new reader statistic numberOfSamplesLost counts the
              number of samples that have been lost for that reader during
              network transportation.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2631<br/>
          </td>
          <td>
            <b>wait_for_historical_data_w_condition accepted DURATION convenience types as Time input.</b><br/>
            <i>
               The reference manuals advocated the use of DDS_DURATION_ZERO and
               DDS_DURATION_INFINITE as valid input arguments for the Time_t
               min_source_timestamp and max_source_timestamp parameters. This is
               confusing because timestamps are expected, but durations are
               considered valid input. Furthermore, supplying DDS_DURATION_INFINITE
               leads to an uninitialized variable, which could potentially lead
               to non-deterministic behavior.
              </br>
              </br>
              <b>Solution: The reference manuals have been updated, so now it is
              more clear what parameters are expected. Also the initialized
              variable bug has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2720<br/>
          </td>
          <td>
            <b>Configurator Tool does not handle lower case values for sizes</b><br/>
            <i>
              Values such as 32k are not accepted by the configurator, but are valid
              by OpenSplice.
              </br>
              </br>
              <b>Solution: Setting size values with k, m and g are now valid in the configurator.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2754/<br/>12042
          </td>
          <td>
            <b>ospl tool updated to ensure orphaned key files on unix are cleaned</b><br/>
            <i>
              On Windows ospl tool ensures that any orphaned key files (i.e. where there are
              no running processes present that match the key file) are deleted before
              starting OpenSplice.
              </br>
              </br>
              <b>Solution: Extended the Windows functionality to Linux, but additionally only tidy
              orphaned files that belong to the same user starting OpenSplice.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2781<br/>
          </td>
          <td>
            <b>Configurator doesn't recognize throttle_limit as a number</b><br/>
            <i>
              The Configurator did not allow the use of k, m or g when setting its value.
              </br>
              </br>
              <b>Solution: Configurator fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2787/<br/>12056
          </td>
          <td>
            <b>DCPS C++ CORBA co-habitation custom library rebuild failed to re-build</b><br/>
            <i>
              The DCPS C++ CORBA co-habitation custom library failed to re-build
              because of a missing macro definition in the supplied makefile.
              </br>
              </br>
              <b>Solution: The missing macro definition has been added to the makefile.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2790/<br/>12047
          </td>
          <td>
            <b>Missing PID when displaying Java participants in the tuner</b><br/>
            <i>
              When using the Tuner in the participant view and the participant
              application is a Java application which is not a DDS service like
              the Tuner or the Tester, only the name of the class is displayed,
              not the PID. When several instances of the same application are
              running on the same node, it is not possible to distinguish one from another.
              </br>
              </br>
              <b>Solution: The java participant applications now show also a pid
              behind their name to make it easier to distinguish them. The
              participant naming is now also consistent with the C, C++ and C#
              applications.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2804/<br/>12123
          </td>
          <td>
            <b>Alignment of transient data failed in multiple writer-scenario</b><br/>
            <i>
              In a scenario with two writers writing the same instance where the
              2nd writer is deleted, alignment of transient data failed because
              the registration for the first writer was not aligned while the
              unregistration of the 2nd writer was.
              </br>
              </br>
              <b>Solution: For each registration that has no samples anymore in the
              store, durability sends an extra registration for that writer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2816/<br/>12145
          </td>
          <td>
            <b>Deadlock involving groups using synchronous write</b><br/>
            <i>
              Incorrect initialisation of a lock involved in processing acknowledgements
              on synchronous writes could cause a deadlock when different processes
              were trying to access the data structure. At the point where this
              occurs, a lock on the group with which that synchronous write is
              associated may be held, which could cause hanging processes in
              various configurations and for seemingly unrelated reasons.
              </br>
              </br>
              <b>Solution: The initialisation of the lock has been corrected.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
     <p>
       <h2>6.3.1p4</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-2763<br/>
          </td>
          <td>
            <b>Shared memory leaks on deletion of DataReader.</b><br/>
            <i>
              Under certain conditions memory would leak when a DataReader was deleted.
              </br>
              </br>
              <b>Solution:  The leak has been fixed.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
     <p>
       <h2>6.3.1p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>TSTTOOL-123<br/>
          </td>
          <td>
            <b>Quoted partition name in Tester script is interpreted as part of actual partition name.</b><br/>
            <i>
              If a user wanted to execute a reader command in a Tester script that specified a partition
              with dots or stars in its name, then the only way for compilation of the script to be successful
              was to surround the partition name in quotes. However, the quotes were then interpreted as part of
              the partition name when the reader is created.
              </br>
              </br>
              <b>Solution:  In a reader command, if the partition name is wrapped in quotes, then the quotes are dropped
              after compilation. If a user still wanted to include quotes as part of the actual partition name, then
              they can escape the quotation marks in the partition name in script.</b>
            </i>
          </td>
        </tr>
        <tr>
         </tr>
      </table>
    </p>
     <p>
       <h2>6.3.1p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-2733
          </td>
          <td>
            <b>Issue in Streams API could result in crash on Windows.</b><br/>
            <i>
              A problem was found in the Streams API with the initialization of mutexes and condition variables, that could
              result in data not being published and/or undefined behaviour of applications build on top of the Streams API on Windows.
              </br>
              </br>
              <b>Solution: The incorrect initialization was fixed.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p>
          <h2>6.3.1p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-2729
          </td>
          <td>
            <b>Problems starting OpenSplice when OSPL_URI is quoted.</b><br/>
            <i>
              OpenSplice DDS will not start with an OSPL_URI surrounded by quotes (" ").
              <br><br>
              <b>Solution: OSPL Tool now handles a quoted OSPL_URI.</b>
            </i>
          </td>
        </tr>
        <tr>
         </tr>
      </table>
    </p>

      <h2>6.3.1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-614/OSPL-1843/<br>10335
          </td>
          <td>
            <b>New durability persistence store implementation using a key-value storage</b><br/>
            <i>
              To store durable data the durability persistency store implementation
              should be extended with a robust and fast storage mechanism
              which will replace the MMF store implementation which is known to
              suffer from robustness issues when a crash occurs.
              <br><br>
              <b>Solution: The new persistency store implementation is based on a
              key-value storage which makes use of third-party products like
              Sqlite or Leveldb to store the durable data on disk.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1430/<br>7255
          </td>
          <td>
            <b>Durability crashes when starting a new instance of ospl if there is no aligner</b><br/>
            <i>
              A fix was applied for this issue n 6.3.0p5. This fix has been removed
              in 6.3.1 and a new fix will be applied in a subsequent release.
              <br><br>
              <b>Solution: N/A</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2131/<br>11470
          </td>
          <td>
            <b>Improvement in read performance of certain complex Java CORBA DDS data structures </b><br/>
            <i>
              <br>
              <b>Solution: An option has been added to improve the read performance
              of certain complex Java CORBA DDS data structures by limiting the
              overhead caused by JNI invocations during data passing from C to Java.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2244<br>
          </td>
          <td>
            <b> IPv6 interface detection in Windows causes a crash</b><br/>
            <i>
              If an application in Windows used the networking service and
              tried to detect available IPv6 interfaces a system crash occurred.
              This made IPv6 on Windows unusable.
              <br><br>
              <b>Solution: The implementation of the IPv6 interface detection
              is improved so that it no longer causes crashes.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2474
          </td>
          <td>
            <b>Default RMI ServiceDiscoveryTimeout needs to be increased.</b><br/>
            <i>
              Due to some changes in the default timing for the alignment of
              historical data, alignment takes a bit longer to start by default.
              This sometimes causes RMI to fail to locate services as the
              default time-out of RMI no longer matches the default durability
              configuration. Even though the discovery period of RMI can be
              influenced by using the --RMIServiceDiscoveryTimeout=<seconds>
              command-line option, the default of RMI should match the default
              durability settings.
              <br><br>
              <b>Solution: The default time-out of RMI service discovery has
              been increased from 10 to 30 seconds.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2504/<br>11790
          </td>
          <td>
            <b>Issues with recording and replaying particular data types and content</b><br/>
            <i>
              Serialization/Deserialization used by the XML-storage component of
              the Record and Replay service causes issues if particular character-data
              is present in DDS samples which are recorded and or replayed.
              Specifically newline characters and unbounded character sequences
              containing illegal XML content are not supported.
              <br><br>
              <b>Solution: Relavant parts of the product were changed and the
              limitations are lifted.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2660<br>
          </td>
          <td>
            <b>The durability service may crash when using dynamic name-spaces.</b><br/>
            <i>
              When a new durability service joins the domain, it may introduce a new
              namespace in the domain that did not exist on the existing nodes in the
              domain. In this situation an already running durability service may
              crash when it has a matching durability policy configured for the new
              name-space (so when using the dynamic name-space feature).
              In this situation the existing durability service dynamically
              registers the new name-space as well and applies the configured
              policies to data that matches that name-space. One of the internal
              algorithms assumed that all name-spaces for a given durability
              service are fixed after start-up.
              <br><br>
              <b>Solution: The internal algorithm in the durability service has
              been changed to be robust against a changing set of name-spaces.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2676/</br>11993
          </td>
          <td>
            <b>Deployment guide on OSPL behavioue for signals incorrect.</b><br/>
            <i>
              The deployment guide for SIGINT was incorrect in the deployment
              guide. Additionally, SIGQUIT behaviour was incorrect in the codebase.
              <br><br>
              <b>Solution: Deployment guide and codebase corrected for signals.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>



    <p>
      <h2>6.3.0p5</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1430/<br>7255
          </td>
          <td>
            <b>Durability crashes when starting a new instance of ospl if there is no aligner</b><br/>
            <i>
              Durability did not check if there was an aligner when dds is started.
              If there is no aligner and a new nodes starts, then the two nodes
              will reach a inconsistent state.
              <br><br>
              <b>Solution: Durability will now check if there is a aligner.
              If durability service cannot find an aligner, then it will send a
              signal to splicedemon to perform system halt.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2106/2361/<br>11859
          </td>
          <td>
            <b>Wrong evaluation of depricated enable_invalid_samples QoS</b><br/>
            <i>
              When the enable_invalid_samples Qos is set to false, this QoS is evaluated as true.
              <br><br>
              <b>Solution: The defect in QoS evaluation algorithm is fixed and will now be evaluated correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2126/<br>11591
          </td>
          <td>
            <b>Illegal time messages are reported</b><br/>
            <i>
              Injecting disposed AND unregistered transient/persistent data into
              a late joining reader that had its autopurge_disposed_samples_delay
              set to a very low value (zero or very close to zero) could in some
              cases corrupt the shared memory because the instance dispose
              message would already purge the instance before its corresponding
              unregister message could register itself into it as well.
              <br><br>
              <b>Solution: This loophole in the purging algorithm has now been closed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2125<br>
          </td>
          <td>
            <b>Durability Memory Leaks</b><br/>
            <i>
              Fixed a memory leak during configuration parsing in durability.
              Additionally fixed reading of an uninitialised value in the service
              termination thread in the user layer.
              <br><br>
              <b>Solution: Code fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2210/2305<br>11702
          </td>
          <td>
            <b>Remove level 4 warnings caused by parallel demarshalling on windows</b><br/>
            <i>
              Parallel de-marshalling, introduced after V6.3.0 caused some W4
              on windows.
              <br><br>
              <b>Solution: Warnings have been resolved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2541<br>
          </td>
          <td>
            <b>The tuner always displays the value 0 in the SampleSequenceNumber field</b><br/>
            <i>
              When reading a sample with the Tuner the field SampleSequenceNumber
              in the sampleinfo table is always 0.
              <br><br>
              <b>Solution: The value of the SampleSequenceNumber field now
              displays the correct value</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2545/<br>11872
          </td>
          <td>
            <b>Durability does not apply merge-policies with other roles after
            initial alignment</b><br/>
            <i>
              Durability assumed that all federations in the domain would be able
              to communicate with each other directly and therefore assumed merge
              policies did not need to be applied during initial start-up.
              Furthermore, it selected an initial source of alignment independent
              of role where it should select one with the same role as itself.
              <br><br>
              <b>Solution: Durability now selects an initial aligner with the
              same role and applies the configured merge policies with other
              roles immediately after start-up.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2617/<br>11894
          </td>
          <td>
            <b>idlpp-generated C-code does not compile with C++ compiler</b><br/>
            <i>
              The idlpp-generated code for standalone C contains code that
              assigns the result of a malloc directly to a character-pointer
              variable. Even though assigning a void-pointer to any other pointer
              is allowed in C, it is not in C++ and this makes it impossible
              to compile the generated C code with a C++ compiler.
              <br><br>
              <b>Solution: The result of a malloc is now casted to a
              character-pointer before the assignment.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2618/<br>11895
          </td>
          <td>
            <b>idlpp-generated functions for the standalone C API don't compile
            with .NET 2008 C++ compiler</b><br/>
            <i>
              External symbols are used within idlpp-generated functions for the
              standalone C API preventing the code to compile when using in C++.
              <br><br>
              <b>Solution: External symbols have been moved outside the function.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p>
      <h2>6.3.0p4</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1697/<br>11351
          </td>
          <td>
            <b>Durability periodic report leads to fast growing trace file</b><br/>
            <i>
              The trace file is growing as result of a periodic report at the level FINE.
              <br><br>
              <b>Solution: Increase the level of this report to the FINEST level.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1897/<br>11066
          </td>
          <td>
            <b>When an invalid handle is applied to read_instance it should
            return BAD_PARAMETER instead of PRECONDITION_NOT_MET</b><br/>
            <i>
              When read_instance is called with an instance handle that is not
              associated with an known instance then the operation should return
              BAD_PARAMETER instead of PRECONDITION_NOT_MET.
              <br><br>
              <b>Solution: When detecting that the provided instance handle is
              not valid because it does not reference an instance associated
              with the data reader then the return code is changed to BAD_PARAMETER. </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1990/<br>11560
          </td>
          <td>
            <b>Cppgen crash under windows when using a path larger than 1024 characters</b><br/>
            <i>
              When using idl with c++ generation and a path lager than 1024
              characters is used cppgen will crash under windows.
              <br><br>
              <b>Solution: Cppgen is fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2051/<br>11575
          </td>
          <td>
            <b>Potential crash while removing synchronous readers and writers</b><br/>
            <i>
              A race condition existed in the product which could, in specific
              circumstances, result in a crash of the spliced process. The crash
              could occur when a pair of synchronous reader and writer are
              deleted at roughly the same time.
              <br><br>
              <b>A lock was introduced to protect shared parts of the
              administration related to synchronous readers and writers.
              Because of this lock, the race condition can no longer occur.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2134<br>
          </td>
          <td>
            <b>The durability service adds a small delay between handling alignment
            requests which may cause a longer alignment time when there are
            requests waiting to be served.</b><br/>
            <i>
              After handling each alignment (sample) request the durability
              service adds a small delay. This delay is not necessay when there
              are still requests waiting to be served. Removing this delay will
              improve the alignment time.
              <br><br>
              <b>After handling an alignment (sample) request check if there
              are still request waiting.When there are requests waiting start
              handling the first request from the waiting list immediately.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2326/<br>11766
          </td>
          <td>
            <b>Report plugin gives unhelpful message at runtime if not built correctly</b><br/>
            <i>
              If OpenSplice was built without the directive
              INCLUDE_PLUGGABLE_REPORTING set to 'yes', then enabling the report
              plugin feature would result in an obscure error message 'ReportPlugin
              registration failed: -1' and the splice daemon will not start.
              <br><br>
              <b>Solution: The error messages related to this issue have been
              changed. Now a more explanatory message indicating what happened,
              and what should be done to resolve the issue, is produced. In
              particular, the error message will now state that the
              INCLUDE_PLUGGABLE_REPORTING directive should be set of 'yes' in
              order to use the pluggable reporting capability of OpenSplice.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2484/<br>11819
          </td>
          <td>
            <b>Namespace mismatch in durability service</b><br/>
            <i>
              The namespace configuration of the durability service allows
              catch-all partition expressions and also more specific
              partition.topic expressions. By mixing these two methods on
              different nodes in a domain, a durability service could incorrectly
              determine a topic does not belong to a particular namespace and
              enter an infinite loop waiting for namespaces from remote nodes
              that do include the topic.
              <br><br>
              <b>Solution: The matching algorithms in the durability service were
              made more robust to deal with this situation and now determine
              the correct namespace for a particular topic.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2510<br>
          </td>
          <td>
            <b>Memory leak when using CReader.read and CReader.take</b><br/>
            <i>
              The CReader.read and CReader.take leaked memory by creating a new
              readCondition with every call.
              <br><br>
              <b>Solution: The creation of the readCondition is moved to the same
              location as the creation of the Reader itself. Typically a reader
              is only created once, now the readCondition is also only created
              once, which minimizes leakage.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2530<br>
          </td>
          <td>
            <b>RT Networking control port not set on first message</b><br/>
            <i>
              The control port is determined when the interface becomes available.
              This occurs after the first message is initialized. In a single
              process configuration this may cause that the ACK messages are
              sent to the wrong control port.
              <br><br>
              <b>Solution: When the send channel is notified that the interface has become
              available set the control port into the current write buffer in
              case of a reliable channel.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p>
      <h2>6.3.0p3</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-2410/<br>11793
          </td>
          <td>
            <b>Potential inconsistency of builtin-data when nodes are concurrently leaving and joining a domain </b><br/>
            <i>
              An issue in the product could result in inconsistent state related
              to a node that has left a domain, between nodes still present in
              the domain. The problem occurred when a node leaves the domain and
              a new node joins a domain before all remaining nodes are aware of
              the node that left.
              <br><br>
              <b>Solution: The product was modified w.r.t. processing of builtin-
                           data, to properly handle this situation.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2409/<br>11792
          </td>
          <td>
            <b>Durability never reaches the state 'operational' in a scenario
            containing two or more nodes without the explicit configuration of
            a namespace for the builtin topics. </b><br/>
            <i>
              When the durability configuration file does not configure a
              namespace for the builtin topics, a namespace called
              AutoBuiltinTopics should be created automatically. This namespace
              should be responsible for aligning various builtin topics. Due to
              a flaw one of the builtin topics, CMParticipantInfo, was not
              included in the namespace. Also, a policy for the namespace was
              not provided. As a consequence the nodes initially would try to
              align their namespaces, but would never reach a 'complete' state
              because the CMParticipantInfo topic could never be aligned. This
              causes the system to indefinitely try to align the namespace for
              this topics, and never reach the 'operational' state.
              <br><br>
              <b>Solution: The CMParticipantInfo has been added to AutoBuiltinTopics
                           namespace. Also a policy for this namespace has been
                           added. Now durability will reach the 'operational'
                           state in a scenario containing two or more nodes
                           without the explicit configuration of a namespace for
                           the builtin topics.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2408/<br>11791
          </td>
          <td>
            <b>Enhanced bind behaviour control</b><br/>
            <i>
              By allowing more control over the behaviour of the RTnetworking
              services regarding the bind-address and port reuse, more advanced
              deployments can be supported. Two attributes have been added to
              the General/NetworkInterfaceAddress section of the RTnetworking
              service: 'bind' and 'allowReuse'. The 'bind' attribute controls
              whether the networking service binds to the wildcard-address or
              ('any'), or to the NetworkInterfaceAddress ('strict'). The boolean
              'allowReuse' attribute specifies whether the SO_REUSEADDR option
              is specified before binding a socket.
              <br>(Note: The deployment manual will be updated at a later
              release regarding these options. The configurator tool can be used
              to configure the options).
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p>
      <h2>6.3.0p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
           <td>OSPL-2150/<br/>11428
           </td>
           <td>
             <b>When a disconnect occurs samples are marked as NOT_ALIVE. After a
                reconnect the samples should become ALIVE again, but this does not
                occur for existing datareaders in the case where the
                BY_RECEPTION_TIME policy is used.</b><br/>
             <i>
               In the case where a reader and a writer are running on different
               nodes and the nodes are disconnected, then samples at the reader
               are marked as NOT_ALIVE. When the connection is re-established and
               durability is configured to re-align the lost samples, the reader
               is expected to show the samples again. Because the unregistration
               of a sample is treated as a new sample and this sample has the
               policy BY_RECEPTION_TIME, the unregistration is treated as a
               new sample that needs to be aligned by the durability. Consequently,
               the unregistration incorrectly reverts the aliveness of the sample.<br/><br/>
               Additionally, the process to determine a new master durability service
               is not taking into account the proper time-outs causing temporary
               conflicting masters and that results in superfluous alignment of
               data to take place.<br/><br/>
               <b>Solution:  The durability service is changed. It now prevents
               to process updates of samples from writers with your own ID. This
               means that you do not process data from anybody that tells
               something about your data.<br/><br/>
               The durability service now also takes into account the proper
               heartbeat expiry-time when determining a new master to prevent
               superfluous alignment of data. As a consequence the default
               heartbeat time-out has changed to prevent alignment in default
               configurations to take longer. The default expiry-time is now
               4 seconds instead of 10. The deployment guide has not been
               updated yet to reflect this change.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2219<br/>
           </td>
           <td>
             <b>Restart failure action on networking service causes communication error</b><br/>
             <i>
               When the restart failure action on the networking service is enabled
               and the networking service is restarted. Communication errors can
               be observed with the new networking service.  <br/><br/>
               <b>Solution: The defect in the service restart mechanism is now fixed
               and the networking service will correctly communicate.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2220<br/>
           </td>
           <td>
             <b>Memory leak of the database type v_message&lt;kernelModule::v_participantInfo&gt;</b><br/>
             <i>
               In the case that a restart failure action is enabled on a service
               it can be observed that the database object count of
               v_message&lt;kernelModule::v_participantInfo&gt; rapidly increases.<br/><br/>
               <b>Solution: The defect in the service restart mechanism is now
               fixed and the v_message&lt;kernelModule::v_participantInfo&gt; will not
               leak anymore.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2315/<br/>11768
           </td>
           <td>
             <b>C++ ping examples does not communicate with Java pong example</b><br/>
             <i>
               The C++ ping application does not initialise its string parameter
               leading to errors in the Java pong application. <br/><br/>
               <b>Solution: The C++ ping example has been updated to
               initialise all sample data before writing it.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2317/<br/>11767
           </td>
           <td>
             <b>Durability service issue with multiple namespaces of a remote node</b><br/>
             <i>
               There was an issue with the durability service's management of a
               remote node's namespaces. It may incorrectly determine a remote
               namespace already exists and so it may not be added to the node's
               administration.<br/><br/>
               <b>Solution: The issue has been corrected by comparing the namespace
               names that are offered by a remote node rather than comparing
               the namespace properties as was being done before.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2327/<br/>11774
           </td>
           <td>
             <b> idlpp compiler generating non-compilable code with '-l isocpp'
             from IDL containing structs that aren't topics</b><br/>
             <i>
               The idlpp compiler was generating non-compilable code with
               '-l isocpp' or '-l isoc++' from IDL containing structs that aren't
               topics. The code did not compile because REGISTER_TYPE_TRAITS
               entries were being generated for these structs in error.
               <b>Solution: The idlpp compiler has been fixed to not generate
               REGISTER_TOPIC_TRAITS entries for non-Topic types; such
               definitions now generate compilable code.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2431<br/>
           </td>
           <td>
             <b>During the alignment process durability performs new, but
                superfluous alignments while being busy in an existing alignment.</b><br/>
             <i>
               When a node is busy in an alignment action and it is triggered to
               align the durability service will perform this action. This is not always
               necessary and causes alignment data to flood the network.<br/><br/>
               <b>Solution: Superfluous alignment request during an existing
               alignment are ignored. This prevents flooding the network.</b>
             </i>
           </td>
        </tr>
    </table>
    </p>
    <p>
      <h2>6.3.0p1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
           <td>OSPL-1478/<br/>11042
           </td>
           <td>
             <b>idlpp OSPL_BOUNDS_CHECK NULL detection error log incorrect</b><br/>
             <i>
               idlpp generated incorrect tracing for OSPL_BOUNDS_CHECKing. When
               a variable was initialized to NULL the BOUNDS check reported an
               out of range instead of NULL<br/><br/>
               <b>Solution: Updated idlpp so it logs the correct message
               when NULL is detected.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-1673/<br/>11578
           </td>
           <td>
             <b>DDSI2 sometimes logs socket error 10035 on Windows</b><br/>
             <i>
               Socket blocking and waiting behaviour on Windows does not really
               support waiting for packets to arrive on a number of sockets in
               one thread, while trying to send packets in blocking mode on
               another thread, as the socket is either blocking or non-blocking.
               The auxiliary data transmission used a socket that was also in use
               for receiving data. Consequently, once the socket send buffer
               filled up, the Windows kernel could return error 10035, EWOULDBLOCK.
               In such a case the packet would be dropped. The protocol is such
               that these lost packets would not affect correctness, but it does
               impact timing and is generally undesirable.<br/><br/>
               <b>Solution: All outgoing traffic now uses dedicated transmit sockets.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-2026/<br>11571
          </td>
          <td>
            <b>Windows Service access rights</b><br/>
            <i>
            Application running as normal user is unable to create a participant
            or topic when running OpenSplice as a service on Windows.<br/><br/>
            <b>Solution: The communication pipe for the service thread was created
            using default access rights, which blocks writes from low-privileged
            users. This rights have been changed so that every user can write to
            the pipe. </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2123<br>
          </td>
          <td>
            <b>DDSI2E mapping of transport_priority to channel incorrect</b><br/>
            <i>
            DDSI2E internally maps transport priorities to channels for processing
            protocol messages, retransmits and incoming data on the threads that
            best correspond to the priority of the message. This mapping differed
            from the mapping of samples to channels which is done internally by
            the kernel. In consequence, processing could take place on a different
            thread than intended by the configuration. This only affected scheduling,
            especially under very high CPU loads. In the particular case that
            channels were defined in order of descending priority, the mapping
            was always correct.</b><br/><br/>
            <b>Solution: The mapping function has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
           <td>OSPL-2244<br/>
           </td>
           <td>
             <b>IPv6 interface detection in Windows causes a crash</b><br/>
             <i>
               If an application in Windows used the networking service and tried
               to detect available IPv6 interfaces a system crash could occur.<br/><br/>
               <b>Solution: The implementation of the IPv6 interface detection
               is improved so that it no longer causes crashes.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2257<br/>
           </td>
           <td>
             <b>Broader use of empty-String interning in Java copy routines</b><br/>
             <i>

               <b>Solution: The empty string interning optimisation for the Java
               copy routines has been applied to bounded strings as well,
               improving performance when the data includes a lot of empty
               bounded strings.</b>
             </i>
           </td>
        </tr>
      </table>


    <p><h2>6.3.0</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
    <tr>
      <td>OSPL-28/<br>4767
      </td>
      <td>
        <b>Wrong returncode in register_type API call</b><br/>
        <i>
          When calling the API register_type function with a type name that is
          already registered but with a different metadescriptor retcode
          DDS_RETCODE_OK is returned. This should be DDS_RETCODE_PRECONDITION_NOT_MET.
          <br><br>
          <b>Solution: The defect in the register_type function is solved and
          the correct returncode is returned.</b>
        </i>
      </td>
    </tr>
        <tr>
          <td>OSPL-148
          </td>
          <td>
            <b>Improve Windows/WindowsCE condition variable implementation</b><br/>
            <i>
               The Windows and Windows CE condition variable implementation contained
               a bug where if there were no threads waiting on the condition variable,
               an open handle from a semaphore would not be closed when returning from the function.
              <br><br>
              <b>Solution: The condition variable signal algorithm has been improved
              to test whether there are any waiting threads before opening the
              handle. This resolves the bug and improves performance slightly
              because it avoids opening the semaphore when it is not required.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-528/<br>9909
          </td>
          <td>
            <b>Memory leak in create_querycondition</b><br/>
            <i>
              Creating and deleting a QueryCondition leads to a memory leak.
              <br><br>
              <b>Solution: The memory leak has been fixed.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-885/1645<br>
          </td>
          <td>
            <b>C# idlpp crash when no module given in IDL</b><br/>
            <i>
              When defining idl for C# and a structure is defined without a
              module the idlpp compiler crashes.
              <br><br>
              <b>Solution: The defect in C# Idlpp is now fixed and it is now
              possible to define structures without a module tag.
            </i>
          </td>
        </tr>
       </tr>
        <tr>
          <td>OSPL-1046<br>
          </td>
          <td>
            <b>Data send on a particular network partition should not be received
            by DDS instances that are not connected to that network partition</b><br/>
            <i>
              When network partitions are configured and data is sent on a specific
              partition (not the default partition) then other DDS instances should
              not receive this data when they are not connected to that specific
              partition. Currently when networking receives data from a partition
              which is not connected or unknown then networking delivers this data
              in the default partition.
              <br><br>
              <b>Solution: When networking receives data on a network partititon
              that is either not connected or unknown to that networking instance
              then the data should be dropped.
            </i>
          </td>
        </tr>
       </tr>
        <tr>
          <td>OSPL-1187<br>
          </td>
          <td>
            <b>Removal of Sun Code snippet and license acknowledgement</b><br/>
            <i>
              OpenSplice Tuner and Tester used a code snippet from Sun that
              had a license accreditation.
              <br><br>
              <b>Solution: Removal of the code means license acknowledgement is removed.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-479/OSPL-1435/<br>10963
          </td>
          <td>
            <b>DDSI2 support for IPV6</b><br/>
            <i>
              <b>Solution: Support is added for IPV6 on DDSI2.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1438/<br>10974
          </td>
          <td>
            <b>Visual studio limitation on large topic structure definitions</b><br/>
            <i>
               For very large topic structure definitions, the Visual Studio
               compiler runs into a limitation of the maximum length of a string.
               If the metaDescriptor character string data exceeds 64k in size,
               the Visual Studio C++ compiler fails to build the generated code.
              <br><br>
              <b>Solution: The metaDescriptor string is replaced by an array
              which resolves the maximum string limitation in Visual Studio.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1526
          </td>
          <td>
            <b>Try automatic repairing option of the configurator did not work for number values</b><br/>
            <i>
               When loading a config file into the configurator with faulty number
               values the configurator asks if it should repair those faulty values.
               After this is done the corrected values are not written to the
               config file when this is being saved.
              <br><br>
              <b>Solution: The defect in the configurator is now fixed and the
              corrected values are now being stored.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1537/<br>11089
          </td>
          <td>
            <b>spliced may crash after a service with the "systemhalt" failure action dies</b><br/>
            <i>
               If a service dies it may not have performed a detach from the
               OpenSplice kernel/database and so the attached services count may
                be incorrect. That could lead to a crash of a database thread
                because the spliced may get detached too early.
              <br><br>
              <b>Solution: When spliced detects that a service has died, it
              ensures that attached services count is correctly maintained,
              so such a crash cannot occur.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1549/<br>11096
          </td>
          <td>
            <b>Durability report about name-space backup unclear</b><br/>
            <i>
               When the durability service on start-up detects that the current
               set of persistent data on disk is not complete due to the fact
               that the service did not manage to fully complete the alignment
               of the set of persistent data during the previous run, it will
               check if there is an older but complete set still available on disk.
               If so it will replace the newer but incomplete set with the older
               but complete set after which durability services in the domain will
               determine who has the latest complete set and use that one everywhere.
               In case there locally is no older complete set, the error is reported.
               The report is not deemed very clear though and besides that it is
               not considered an error but merely a warning. Alignment will still
               be able to continue.
              <br><br>
              <b>Solution: The report has been rewritten to make situation clearer
              and is now reported as warning instead of as error.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1591/<br>11165
          </td>
          <td>
            <b>Services do not use the shared memory threshold correctly</b><br/>
            <i>
              When the shared memory database is configured to have a threshold,
              the services are entitled to use half of that region meaning they
              are able to continue to run when shared memory gets low. The issue
              was that the services were in fact using that threshold region in
              the same way as regular applications - i.e. they were unable to use
              any of that region. There was also a bug where terminating services
              with a failureAction of "kill" were being left in a zombied state
              and not necessarily exiting.
              <br><br>
              <b>Solution: A fix has been applied that allows the services to
              correctly use up to 50% of the threshold region. A fix has also
              been applied to the monitoring of the died/terminating services
              so that they cannot be left in a zombied state.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1615<br>
          </td>
          <td>
            <b>Durability service does not reach COMPLETE state when some
            partition-topic combinations are not covered by the name-space configuration</b><br/>
            <i>
              In case the durability service is configured with name-space settings
              that do not cover all locally available partition-topic combinations,
              the service did not reach the COMPLETE state that indicates that
              all partition-topic combinations that the durability service
              manages have been fully aligned.
              <br><br>
              <b>Solution: The state of locally available partition-topic
              combinations that are not supposed to be managed by the durability
              service are no longer included in determining whether or not the
              alignment is complete.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1634<br>
          </td>
          <td>
            <b>New command line parameter for the configurator</b><br/>
            <i>
              The OpenSplice Configuration Editor now contains a new command line
              instruction.
              <PRE><code>-uri=[URI] the domain config file that needs to be opened e.g. osplconf -uri=$OSPL_URI</code></PRE>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1642/<br>11224
          </td>
          <td>
            <b>C++ memory leaks of the default QoS sets stored for the DDS Entities</b><br/>
            <i>
              The statically initialized default QoS set for each DDS Entity
              (Publisher, Data Writer etc) are stored as pointer types and they
              are not deallocated, so they will appear as a leak when main exits.
              <br><br>
              <b>Solution: Storing these default QoS sets as _var types means
              that the container class takes care of the deallocation, even for
              the statically initialized types.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1644<br>
          </td>
          <td>
            <b>Inappropriate warning when retrieving network interface information on Windows</b><br/>
            <i>
              When a network interface is present but not connected or configured
              then a warning is given which is not appropriate.
              <br><br>
              <b>Solution: The discovery of the network interfaces should only
              consider network interfaces that are available and the warning is
              now only output when required.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1654<br>
          </td>
          <td>
            <b>Networking crashes when a network partition is configured without an explicit name</b><br/>
            <i>
              Networking crashes when a network partition is specified without a name.
              The name attribute of a network partition is optional, when not
              specified the address should be set to the address of the network
              partition which is omitted.
              <br><br>
              <b>Solution: When the name of a network partition is not set the
              name is set to the address attribute of the network partition.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1668<br>
          </td>
          <td>
            <b>Dispose/writedispose action returns error in the Tuner</b><br/>
            <i>
              When creating a reader/writer in the Tuner and then clicking on the
              Dispose or WriteDispose button an error message may be shown in the
              status pane stating that the dispose failed while the dispose
              from system-perspective was successful.
              <br><br>
              <b>Solution: The algorithm for showing this message is fixed and
              the message will not be displayed anymore.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1708<br>
          </td>
          <td>
            <b>Default MMF persistency configuration settings don't work i.c.w
            single-process mode</b><br/>
            <i>
              When selecting MMF as persistency and running in single-process mode,
              the durability service would not start due to wrong default settings
              for mapping address and size of the store.
              <br><br>
              <b>Solution: The default settings are changed in case of single-process
              mode to ensure proper functioning of the durability service in that
              deployment mode as well.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1751<br>
          </td>
          <td>
            <b>DDSI2 configuration defaults incompatible with various locales</b><br/>
            <i>
              When starting DDSI2 in a locale in which the decimal separator
              character was different from "." (for example, the French locale,
              in which it is ","), some DDSI2 default values would be flagged
              as erroneous, preventing DDSI2 from starting.
              <br><br>
              <b>Solution: DDSI2 has been modified to avoid this problem.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1806/<br>11373
          </td>
          <td>
            <b>Tuner crash when trying to read samples which contain a lifespan
            that is not set to infinite</b><br/>
            <i>
              The Tuner crashes when trying to read samples which have a
              limited lifespan set.
              <br><br>
              <b>Solution: The defect is now fixed and the Tuner will not crash.
            </i>
          </td>
        </tr>
        <tr>
           <td>OSPL-1889
           </td>
           <td>
             <b>An IDL file that contains a typedef to an enum causes problems in Java.</b><br/>
             <i>
               If the IDL contains a typedef to an enum, then the middleware should
               unwind the typedef to retrieve the definition of the enum. This was
               not done properly, which effectively caused the system to crash.
               To avoid this, customers had to rewrite their IDL in such a way
               that it did not contain any typedef to enums.
               <br><br>
               <b>Solution: The implementation of the IDL processing function for
               Java has been modified so that the issue does not cause crashes
               anymore in Java.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-1913/<br/>11463
           </td>
           <td>
             <b>Issue performing dispose or writedispose as last operation in coherent set</b><br/>
             <i>
               The completion of a coherent set is signaled to the DataReader side by
               way of sending a new sample that contains the transaction information.
               That sample is created as a clone from the last sample sent from within
               the set. That sample was not being stored correctly in the case of a
               dispose or writedispose so was not being sent.
               <br><br>
               <b>Solution: The algorithm has been corrected so that any form of a
               writing a sample will store the sample so it can be resent when
               the transaction is completed.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2000/<br/>11528
           </td>
           <td>
             <b>The wait_for_historical_data_w_condition() call on the DataReader
             is not working properly</b><br/>
             <i>
               The wait_for_historical_data_w_condition can return BAD_PARAMETER even
               when all input parameters are correct due to the fact the durability
               service is not ready to receive the request from the DataReader at the
               time it was issued. Furthermore, the durability service may also fail
               to deliver all historical data that matches the condition if the
               condition contained a filter expression.
               <br><br>
               <b>Solution: The wait_for_historical_data_w_condition algorithm now
               waits until all configured durability services are operational before
               issuing the request. If none have been configured or one or more that
               have been configured are not operational before the given time-out
               expires, PRECONDITION_NOT_MET is returned. Furthermore, the internal
               algorithm to evaluate the filter expression has been modified to
               ensure correct evaluation of the expression plus parameters against
               the set of available historical data.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2004<br/>
           </td>
           <td>
             <b>Exception handling is broken on POSIX</b><br/>
             <i>
               Even though the application signal handler is properly called from
               the thread that caused synchronous signal, it is ALSO called from
               within the signalHandlerThread before that. It is obviously not
               supposed to do that.
               <br><br>
               <b>Solution: Modified POSIX signal handler to not invoke application
               signal handler from within our dedicated signal handler thread
               in case of a synchronous signal that was raised from within the
               application itself.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2010<br/>
           </td>
           <td>
             <b>Wireshark on windows</b><br/>
             <i>
               Wireshark for RT networking was extremely difficult to build by a
               user on Windows platforms.
               <br><br>
               <b>Solution: PrismTech has decided to provide this
               now pre-built on its website. We continue to leave the source code
               and build files in place, should a user want to use them.</b>
             </i>
           </td>
        </tr>
        <tr>
           <td>OSPL-2052/<br/>11569
           </td>
           <td>
             <b>Incorrect dependencies for dcpssaj.jar</b><br/>
             <i>
               The manifest file of the dcpssaj.jar file contains a lot of dependencies which are not needed.
               <br><br>
               <b>Solution: The unrequired dependencies are now removed.</b>
             </i>
           </td>
        </tr>
     </table>

     <p><h2>6.2.3p1</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
         </th>
          <th width="75%">
            Description
          </th>
        </tr>
    <tr>
      <td>OSPL-2110/<br>11592
      </td>
      <td>
        <b>On Windows the disconnection or connection of a network interface cable
        causes large memory loss</b><br/>
        <i>
          A status change of the network interface triggers an event. A memory
          leak occurs when handling this event. Because the status of the event
          is not automatically reset the function to handle event is called
          repeatedly.
          <br><br>
          <b>Solution: The memory leak is removed and the network interface status
          change event notification is re-enabled to allow new network interface
          status changes to be detected.</b>
        </i>
      </td>
    </tr>
    </table>

     <p><h2>6.2.3</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
         </th>
          <th width="75%">
            Description
          </th>
        </tr>
    <tr>
      <td>OSPL-948/<br>10679
      </td>
      <td>
        <b>DDSI2 sends duplicate samples for writers that publish in multiple
        partitions simultaneously</b><br/>
        <i>
          A write in multiple partitions simultaneously is received by DDSI2 as a
          number of messages from a single writer, one for each partition.
          All these message were forwarded by DDSI2 and filtered out by the
          subscribers running OpenSplice. However, this required extra bandwidth
          and besides caused other vendors' implementations of the DDSI protocol
          to report them as independent updates.
          <br><br>
          <b>Solution: This behaviour has now been changed, so that DDSI2 filters
          out such duplicates in the vast majority of cases. In particular, as long
          as the writer, its local subscribers and the networking services do not
          run into resource limits, it is guaranteed to filter out all duplicates.
          The filter can, under very unlikely circumstances, conclude a message
          is a duplicate when in reality it is not. This requires publishing more
          than 4 billion samples with a single writer while carefully controlling
          the behaviour of the writer history cache using resource limits on local
          readers; a situation no real system is likely to ever encounter.
          We advise that on systems that may run into this, the
          Unsupported/ForwardAllMessages setting be set to true.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-966/<br>10707
      </td>
      <td>
        <b>The RTSM tool is no longer working</b><br/>
        <i>
          As a protection mechanism the RTSM tool does not continue processing
          when it thinks that a given address is not valid. One of the steps it
          takes to determine whether an address is valid, is to check whether the
          address lies in the virtual memory range that is expected based on the
          configured size of the shared memory segment. However, the tool determined
          the size of a shared memory segment in the wrong manner. Due to this
          issue the tool reports perfectly valid addresses as 'faulty' and no
          further processing is done.
          <br><br>
          <b>Solution: The algorithm that calculates the size of the shared memory
          and correct range for addresses has been repaired to ensure valid
          addresses are no longer reported as 'faulty' and all further processing
          can be done.</b>
        </i>
      </td>
    </tr>
    <tr>
       <td>
         OSPL-991/<br/>10735<br/>
         OSPL-1771/<br/> 11395
       </td>
       <td>
         <b>take() and take_w_condition() do not have the same behaviour/random
         crash on take next instance</b><br/>
         <i>
         The dispose_all_data operation on the topic was not treated identically
         to sending a separate dispose message for every entity. This manifested
         itself especially in the way the disposed data was delivered to late
         joiners (which sometimes couldn't see that the data was actually disposed)
         and in the way disposed data ignored the cleanup delay specified on the
         durability service.
         <br><br>
         <b>Solution: The new implementation of dispose_all_data is much
         more inline with sending separate dispose messages for every instance,
         and thus late joiners will always see the correct instance state.
         Furthermore the dispose messages obey the same cleanup delays as normal
         dispose messages. However, the dispose_all_data function will still be
         me more efficient with respect to the utilization of network network
         bandwitdth, CPU cycles and memory than the manual transmission of
         separate dispose messages.</b>
         </i>
       </td>
    </tr>
        <tr>
          <td>OSPL-1205/<br>10845
          </td>
          <td>
            <b>Terminating applications report pthread_join failed with error 35</b><br/>
            <i>
              When an application with a defined exithandler terminates and this
              exithandler contains an exit call the ospl-error file will report
              the message: pthread_join failed with error 35.
              <br><br>
              <b>Solution: The defect in the OpenSplice signalhandling is fixed
              and this error will not be reported anymore.
            </i>
          </td>
        </tr>
     <tr>
        <td>
           OSPL-1334/<br/>10898<br/>
           OSPL-1335/<br/>10899
            </td>
            <td>
              <b>PurgeList may illegally remove a groupInstance.</b><br/>
              <i>
               The purgeLists are sometimes populated by the same instance multiple
              times (for different generations). Although the purgeList expiry
              algorithm should handle these situations correctly, it seems some
              scenario's are not properly handled yet, and a groupInstance from the
              emptyPurgeList may be freed while it is already reincarnated as a new
              generation.<br/><br/>
             <b>Solution: New code has been added that prevents outdated
             generations from being inserted into the emptyPurgeList, thus
             preventing this list from having duplicate entries and thus
             preventing it from deleting a groupInstance that is already
             reincarnated.</b>
           </i>
         </td>
       </tr>
    <tr>
      <td>OSPL-1341 / 10914<br><br>OSPL-1930 / 11473
      </td>
      <td>
        <b>The reliable network communication may not operate correctly when the
        first messages of a sending node arrive out of order.</b><br/>
        <i>
          When reconnection is enabled and when the first messages of another
          starting node arrive out of order and also the discovery heartbeats
          arrive later then the first message the reliable network channel will
          not operate correctly.
          <br><br>
          <b>Solution: The notification that a node has become alive by the
          discovery protocol should not reinitialize the reliable channel
          administration associated with that node when the reliable channel
          had already detected that the node was alive.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1445/<br>10981
      </td>
      <td>
        <b>Possible deadlock when using the API find_topic function</b><br/>
        <i>
          When in a multithreaded application, one thread uses the API find_topic
          function while the topic is not defined and the timeout is set, and another
          thread executes a function that needs the domain participant, the last
          thread will get blocked because the API find_topic function locks the
          domain participant and does not release it until the topic is found.
          <br><br>
          <b>Solution: The defect in the find_topic API function is now fixed
          and no deadlock will occur.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1495/<br>11040
      </td>
      <td>
        <b>In DBMSConnect the mapping from DDS unsigned types to database record
        fields is not correct</b><br/>
        <i>
          When a topic contains unsigned integer fields the mapping to the
          corresponding database schema is not correct. This means that the
          data written from DDS to database is not equal to the data that is
          stored in the database. This mapping is dependent on the DBMS used.
          For example MySQL supports unsigned integer fields while Microsoft SQL
          does not.
          <br><br>
          <b>Solution: The mapping of the DDS Topic fields to the corresponding
          database schema should be made dependent on the type of DBMS that is
          used. This also applies to the reading and writing of the database
          records.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1557/<br>10734
      </td>
      <td>
        <b>Crash of durability during initial alignment</b><br/>
        <i>
          Unregistrations that are aligned by the durability service are stored
          without any data to reduce footprint. These unregistrations can only
          be re-published locally when an instance handle is provided by the
          durability service while doing so. In some scenario's, the durability
          service did not use an instance handle while doing so, which made the
          service crash while locally republishing an aligned unregistration.
          <br><br>
          <b>Solution: The durability service now ensures an instance handle is
          created in any case while also making sure a registration is created
          in the instance for the writer that originally wrote the aligned
          sample.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1512/<br>11066
      </td>
      <td>
        <b>PRECONDITION_NOT_MET doc update</b><br/>
        <i>
          The reference manuals did not correctly document all cases where
          PRECONDITION_NOT_MET could be returned.
          <br><br>
          <b>Solution: Manuals updated.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1595/<br>11664
      </td>
      <td>
        <b>Not receiving data locally when networking is configured but network
        interface is disconnected</b><br/>
        <i>
          When networking does not find an suitable and available network
          interface networking will terminate. Because the configuration specifies
          networking to be present the sending of data by a publisher will be
          affected because it will wait until the network service becomes available.
          <br><br>
          <b>Solution: When networking finds a suitable network interface but the
          interface is currently not connected then networking has to continue
          and monitor the status of the networking interface to become available
          and notify the system that it is available.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1669<br>
      </td>
      <td>
        <b>Merged historical data is not delivered to active datareaders.</b><br/>
        <i>
          When configured for merging after re-connecting, the durability service
          does not deliver the aligned samples to existing data-readers. Only newly
          created data-readers from that point forward will get the data. This is
          caused by some internal optimisation mechanism between the transient store
          and existing data-readers. The connection between transient store and existing
          data-readers can be closed in some situation where remote nodes disconnect
          and the durability service does not re-instate this connection when the node reconnects.
          <br><br>
          <b>Solution: The connection between transient store and existing readers
          is now checked and re-instated in case a node reconnects.</b>
        </i>
      </td>
    </tr>
    <tr>
     <td>OSPL-1681/<br>11284
     </td>
     <td>
       <b>leaseManager reports lease update is behind schedule</b><br/>
       <i>
         It was possible that the ospl-info log file reports numerous warnings
         that the lease is behind schedule. These invalid warnings are comming
         from the leasemanager that had a fault in the evaluation algorithm of deadline
         leases which caused these warnings to be displayed while the lease was correct.
         <br><br>
         <b>Solution: The defect in the leasemanager algorithm is now fixed and
         these invalid warnings will not occur anymore.</b>
       </i>
     </td>
    </tr>
    <tr>
     <td>OSPL-1682/<br>11291
     </td>
     <td>
       <b>crash of spliced</b><br/>
       <i>
         In certain scenario's the reader purgeList was doing invalid memory
         reads by trying to access already deleted purgeItems, thereby causing
         potential memory corruption of totally unrelated objects.
         <br><br>
         <b>Solution: The purge algorithm has been modified to prevent this
         situation, thus preventing the memory from becoming corrupted and
         improving the overall stability of the system.</b>
       </i>
     </td>
    </tr>
    <tr>
      <td>OSPL-1703/<br>11354
      </td>
      <td>
        <b>Crash of networking when terminating as result of the reception of a signal.</b><br/>
        <i>
          When the network service is terminated as result of a signal then the kernel
          objects created by networking are freed while some of the networking threads
          may still try to access these kernel objects.
          <br><br>
          <b>Solution: The termination of the networking threads is synchronised
          with the freeing of the networking kernel resources.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1729<br>
      </td>
      <td>
        <b>Calling DomainParticipantFactory.set_qos(_) using the Java API segfaults.</b><br/>
        <i>
          The underlying JNI layer of the Java DCPS API used the wrong jfieldID
          when getting a value from Java.
          <br><br>
          <b>Solution: The correct jfieldID is now used.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1816<br>
      </td>
      <td>
        <b>Re-using a Record and Replay storage in consecutive replay scenarios</b><br/>
        <i>
          A issue in the Record and Replay service caused unexpected behaviour when
          a storage is re-used while it is was previously set to pause using the
          setreplayspeed command.
          <br><br>
          <b>Solution: The issue is resolved so now a storage can be used multiple
          times during the lifecycle of a Record and Replay service without any
          constraints.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1852<br>
      </td>
      <td>
        <b>Topic disappearance when topic created in parallel</b><br/>
        <i>
          In a scenario where a specific topic was created for the first time in
          the system, but for which a duplicate was created by another application
          before the original could enable its topic, a refCount to the resulting
          topic got dropped and that topic could suddenly disappear while still
          being used by the system.
          <br><br>
          <b>Solution: A change in the refCounting algorithm has now solved
          this issue.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1855<br>
      </td>
      <td>
        <b>Synchronous signals sent by external mechanism could cause deadocks.</b><br/>
        <i>
          <br><br>
          <b>Solution: Signals that are raised by an asynchronous mechanism
          will now be handled in an asynchronous manner. That means that handling
          of the signal is delayed until all threads have successfully finished
          their consultations/modifications of the shared memory, leaving it in
          a consistent state. Also our signal handler will no longer be installed
          when an (ignorable) signal is set to SIG_IGN.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1867/<br>11443
      </td>
      <td>
        <b>System termination fallback mechanism</b><br/>
        <i>
          When a service crashes and system termination is set into progress,
          a safe system termination is not always guaranteed i.e. the service
          could end up in a deadlock stalling system termination. This is not
          acceptable and the system should always terminate.
          <br><br>
          <b>Solution: A new service termination thread is spawned when the
          system state of a process is set to terminating. This thread waits
          for 5 seconds to see if the process goes from the terminating state
          to the terminated state. If this has not happened after 5 seconds
          the process is being killed by means of _exit.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1923 / 11472
      </td>
      <td>
        <b>Application not detached from shm after deletion of all the
        participants.</b><br/>
        <i>
          When an application creates 2 or more participants and all these
          participants are deleted using the delete_participant function,
          the application still hold a reference to the shared memory.
          <br><br>
          <b>Solution: The defect is now fixed and when all participants from an
          application are gone the connection to the shared memory is also
          gone.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1926 / 11518
      </td>
      <td>
        <b>Read of disposed instance may return null for key string fields on java</b><br/>
        <i>
          When reading an disposed instance using the java API then it can occur
          that the returned sample has the key fields not set. This occurs when
          the topic contains a non key string field which precedes fields that
          are key fields.
          <br><br>
          <b>Solution: The copy function used when reading a sample has to walk
          over all fields of a sample and not stop at the first non key string
          field.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1934
      </td>
      <td>
        <b>Durability PartitionTopic configuration for a NameSpace is not
        available in configurator</b><br/>
        <i>
          The ability to configure partition-topic combinations for NameSpace
          contents for the durability service has been added recently. The
          configurator tool was not updated to include this configuration option.
          <br><br>
          <b>Solution: The configurator tool has been extended with the missing
          PartitionTopic configuration option.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1936/<br>11509
      </td>
      <td>
        <b>Reporting/tracing of networking service could be improved</b><br/>
        <i>
          In case the networking service is configured to allow re-connections,
          it should report when a remote node re-connects. In addition, the
          networking service should also report the channel when reporting a
          missing packet in its trace (if configured) as well as report when and
          if the missing packet is received to be able to find out if
          the service recovered from the missing packet or not.
          <br><br>
          <b>Solution: The reporting and tracing extensions requested above have
          been added to the networking service.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1946/<br>11512
      </td>
      <td>
        <b>With compression or encryption enabled lost packets may not be resent </b><br/>
        <i>
          In some situations networking would try to access the compressed and/or
          encrypted content of a packet in its resend administration, causing
          packets to not be re-transmitted.
          <br><br>
          <b>Solution: All information needed for (re-)sending of a packet are
          read from the packet-buffer before compression and/or encryption.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1952<br>
      </td>
      <td>
        <b>Re-creation of topic definitions from the XML persistent store can
        cause application and service to crash </b><br/>
        <i>
          The XML persistency implementation of the durability service uses a
          deprecated (de)serializer to store topic definitions to disk and to
          recreate them after start-up. When using this particular serializer it
          is possible that types can be resolved from DDS already by other
          services or applications before they are finalized. When such an
          incomplete type is used the process that uses it crashes.
          <br><br>
          <b>Solution: The durability service now uses a new (de)serializer that
          ensures that types cannot be resolved before being finalised. For
          backward compatibility, the durability service is still able to read
          the old serialized format from disk. In that case it still uses the
          old serializer, but it ensures to write any new definitions using the
          new serializer and therefore the new format.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1998/<br>11540
      </td>
      <td>
        <b>Durability and Memory Consumption</b><br/>
        <i>
          When using the durability service in combination with the XML persistent
          store and the following topic QoS settings: durability_service
          QosPolicy: history_kind = KEEP_LAST_HISTORY_QOS, history_depth = 1
          destination_order QosPolicy: kind = BY_RECEPTION_TIMESTAMP_DESTINATIONORDER_QOS.
          Memory leakage can be observed.
          <br><br>
          <b>Solution: The defect in durability service is now fixed and in the
          described scenario data will not leak anymore.</b>
        </i>
      </td>
    </tr>
    </table>

     <p><h2>6.2.2</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
    <tr>
     <td>OSPL-1682/<br>11291
     </td>
     <td>
       <b>crash of spliced</b><br/>
       <i>
         In certain scenario's the reader purgeList was doing invalid memory
         reads by trying to access already deleted purgeItems, thereby causing
         potential memory corruption of totally unrelated objects.
         <br><br>
         <b>Solution: The purge algorithm has been modified to prevent this
         situation, thus preventing the memory from becoming corrupted and
         improving the overall stability of the system.</b>
       </i>
     </td>
    </tr>
    <tr>
      <td>OSPL-1729<br>
      </td>
      <td>
        <b>Calling DomainParticipantFactory.set_qos(_) using the Java API segfaults.</b><br/>
        <i>
          The underlying JNI layer of the Java DCPS API used the wrong jfieldID
          when getting a value from Java.
          <br><br>
          <b>Solution: The correct jfieldID is now used.</b>
        </i>
      </td>
    </tr>
    <tr>
      <td>OSPL-1852<br>
      </td>
      <td>
        <b>Topic disappearance when topic created in parallel</b><br/>
        <i>
          In a scenario where a specific topic was created for the first time in
          the system, but for which a duplicate was created by another application
          before the original could enable its topic, a refCount to the resulting
          topic got dropped and that topic could suddenly disappear while still
          being used by the system.
          <br><br>
          <b>Solution: A change in the refCounting algorithm has now solved
          this issue.</b>
        </i>
      </td>
    </tr>





        <tr>
          <td>OSPL-774
          </td>
          <td>
            <b>New rmipp option to export generated code</b><br/>
            <i>
              <br><br>
              <b>Solution: A new command line option has been added to the RMI
              pre-processor to be able to create a Windows DLL from generated code.
              This option is: -P dll_macro_name[,<header-file>]<br>
              Only applicable to C and C++. Sets export macro that will be
              prefixed to all functions in the generated code. This allows
              creating DLL's from generated code. Optionally a header file
              can be given that will be included in each generated file.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1076/<br>10787
          </td>
          <td>
            <b>Compression in networking is not configurable other than on/off.</b><br/>
            <i>
              Data compression typically involves a trade-off between CPU usage and the amount
               of compression achievable. The utility of the compression feature would be
               increased by allowing more flexibility in terms of this trade-off.
              <br><br>
              <b>Solution: The compression "level" of the existing zlib compressor may now be
              configured. Other compression algorithms may also be used. See the
              Deployment Guide for details.</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>OSPL-631/<br>10459
         </td>
         <td>
           <b>Using read or take with max_samples limit set can cause some key-values to be never read.</b><br/>
           <i>
             The read and take operations return instances starting with lower key-values.
             If not all available data is read at once (e.g., when having set the
             max_samples limit) and the lower key-values keep receiving updates,
             a subsequently performed read operation will return the updated instances,
             which may prevent the higher key-values to be read.
             <br><br>
             <b>Solution: The read and take operations are changed to provide data circularly
             from a cursor. This means that these operations will 'resume' a read as
             if read_next_instance was succesively called. This way all instances
             can be read even when lower key-values get updated between two
             read-operations with a max_samples limit.</b>
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1023
         </td>
         <td>
           <b>Service failure actions can be taken multiple times</b><br/>
           <i>
             When multiple services are known to the service framework, a failure
             action can be taken multiple times.
             <br><br>
             <b>Solution: The defect in the service failure action algorithm is
             now fixed and the action will only be done once.</b>
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1051
         </td>
         <td>
           <b>OpenSplice RMI Services Activation/De-activation in one call</b><br/>
           <i>
             Services activation was done in a per-service basis. A given service
             becomes active (waits for incoming requests) by calling
             'DDS_Service::run(service_name, ...)' which could be either blocking
             or non-blocking according based on the provided arguments. To activate
             multiple services, the "run" operation must be called as many times
             as there are services.
             <br><br>
             <b>Solution: Two new operations has been added to the "CRuntime" class
             to activate and de-activate all the registered services in one call:<br>
              - CRuntime::run()<br>
              - CRuntime::shutdown(bool wait_for_completion = true)<br>
             The "run" operation is a blocking operation that blocks the calling
             thread until the shutdown is called. DDS_Service::run operation is
             kept but it becomes non blocking. It is recommended to use the
             CRuntime object for services activation and de-activation.
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1166/<br>10823
         </td>
         <td>
           <b>Terminating the "ospl -f start" operation may not kill all services </b><br/>
           <i>
             The algorithm in ospl that performs the shutdown of the splice daemon
             and its child services in the case of blocking mode did not correctly
             detect whether the splice daemon had been terminated. This meant that
             it was possible that the splice daemon and its services may not necessarily
             have been terminated when "ospl -f start" returns.
             <br><br>
             <b>Solution: The shutdown algorithm in ospl has been improved to correctly
             detect the termination status of the splice daemon in both normal and
             blocking modes.</b>
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1172/<br>10827
         </td>
         <td>
           <b>Performance difference between the datareader listener and subscriber listener </b><br/>
           <i>
             In certain scenarios the subscriber listener handling is faster than
             the datareader listener handling.
             <br><br>
             <b>Solution: The handling algorithm of the datareader listener is
             improved to match the subscriber handling.</b>
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1341/<br>10914
         </td>
         <td>
           <b>The reliable network communication may not operate correctly when the
           first messages of a sending node arrive out of order.</b><br/>
           <i>
             When reconnection is enabled and when the first messages of another
             starting node arrive out of order and also the discovery heartbeats
             arrive later then the first message the reliable network channel will
             not operate correctly.
             <br><br>
             <b>Solution: The notification that a node has become alive by the
             discovery protocol should not reinitialize the reliable channel
             administration associated with that node when the reliable channel
             had already detected that the node was alive.
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1384/<br>10907
         </td>
         <td>
           <b>OpenSplice logs errors when the XML configuration file contains DOCTYPE descriptors</b><br/>
           <i>
             <br><br>
             <b>Solution: Thecode that logged the errors has been removed.
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1393/<br>10907
         </td>
         <td>
           <b>The DDSI2 service uses Watchdog scheduling parameters from RT networking config</b><br/>
           <i>
             The DDSI2 service incorrectly used the NetworkService/Watchdog element
             instead of DDSI2Service/Watchdog element from the OpenSplice configuration
             file for determining the scheduling parameters of the watchdog thread.
             <br><br>
             <b>Solution: It now uses DDSI2Service/Watchdog.
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1051
         </td>
         <td>
           <b>OpenSplice RMI Services Activation/De-activation in one call</b><br/>
           <i>
             Services activation was done in a per-service basis. A given service
             becomes active (waits for incoming requests) by calling
             'DDS_Service::run(service_name, ...)' which could be either blocking
             or non-blocking according based on the provided arguments. To activate
             multiple services, the "run" operation must be called as many times
             as there are services.
             <br><br>
             <b>Solution: Two new operations has been added to the "CRuntime" class
             to activate and de-activate all the registered services in one call:<br>
              - CRuntime::run()<br>
              - CRuntime::shutdown(bool wait_for_completion = true)<br>
             The "run" operation is a blocking operation that blocks the calling
             thread until the shutdown is called. DDS_Service::run operation is
             kept but it becomes non blocking. It is recommended to use the
             CRuntime object for services activation and de-activation.
        <tr>
         <td>OSPL-1478/<br>11042
         </td>
         <td>
           <b>BOUNDS_CHECK does not report the name of the incorrect member when the member is null.</b><br/>
           <i>
             The copyin routines for C and C++ did not check for null-values in string fields.
             <br><br>
             <b>Solution: Check for null-values in string-fields is added.
           </i>
         </td>
       </tr>
        <tr>
         <td>OSPL-1479/<br>11041
         </td>
         <td>
           <b>Bugfix to allow correct rebuilding of the C++ APIs ( customlibs ) on 64bit linux systems..</b><br/>
           <i>
              Rebuild would fail due to an incorrect makefile.
             <br><br>
             <b>Solution: Makefile corrected.
           </i>
         </td>
       </tr>
        <tr>
         <td>OSPL-1489/<br>11042
         </td>
         <td>
           <b>Licenses inconsistent.</b><br/>
           <i>
             The $OSPL_HOME/LICENSE file was version 2.6, but the StdLicenseterms2.3.pdf is for 2.3
             <br><br>
             <b>Solution: PDF corrected to 2.6.
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1511
         </td>
         <td>
           <b>Setting the buffer size of the receive socket to the configured value may fail</b><br/>
           <i>Setting the receive buffer size of the receive socket to the configured value
              may fail which may cause message loss in case of worst case expected network load.
             <br><br>
             <b>Solution: When setting the receive buffer size of the receive socket
             to the configured value a warning report is logged if the operating system
             doesn't apply the setting.</b>
           </i>
         </td>
       </tr>
       <tr>
         <td>OSPL-1558
         </td>
         <td>
           <b>Durability sometimes creates conflicting namespace for __BUILT-IN
           PARTITION__ when partitionTopic is used in namespace-definition.</b><br/>
           <i>Durability could automatically create conflicting namespace for
             __BUILT-IN PARTITION__ when partitionTopic is used.
             <br><br>
             <b>Solution: Durability now only creates a namespace in which the
             builtin-topics are matched, instead of matching the whole
             builtin-partition. </b>
           </i>
         </td>
       </tr>
       <tr>
          <td>OSPL-1597/<br>10974
          </td>
          <td>
            <b>DDSI2 external address setting refuses valid IP addresses </b><br/>
            <i>
              The DDSI2 General/ExternalNetworkAddress configuration setting contained
              an error in validating the specified address, causing valid IP
              addresses to be rejected.
              <br><br>
              <b>Solution: The validation code has been corrected.</b>
            </i>
          </td>
        </tr>
       <tr>
          <td>OSPL-1639
          </td>
          <td>
            <b>The OpenSplice DDS Tuner can not write bounded strings.</b><br/>
            <i>
              When a bounded string is written by the OpenSplice DDS Tuner and
              this value is read back by the system a null reference occurs
              where the text of the string should have been.
              <br><br>
              <b>Solution: The defect in the OpenSplice DDS Tuner is now fixed
              and bounded strings are now correctly written to the system.</b>
            </i>
          </td>
        </tr>
       <tr>
          <td>OSPL-1621/<br>11189
          </td>
          <td>
            <b>Leakage of instances when built in topics not configured.</b><br/>
            <i>
              When OpenSplice is configured not to transmit builtin topics
              the deletion of a datawriter was no longer reported to the rest of
              the Domain resulting in the leakage of instances that belonged to
              that datawriter and that were not unregistered explicitly prior to
              the deletion of that datawriter.
              <br><br>
              <b>Solution: A sample for the DCPSPublication topic is now always
              sent on the deletion of a datawriter, regardless of the configuration
              setting for builtin topics. However, this sample is non-transient
              and will be consumed immediately, so that it does not accumulate
              unnecessary resources.</b>
            </i>
          </td>
        </tr>
     </table>

    <p><h2>6.2.1p2</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-626
          </td>
          <td>
            <b>Allow configuration of the UDP port number ranges that networking
                        may use for the reliable channels for receiving ACK and resend messages </b><br/>
            <i>
              Each reliable network channel requires an extra UDP port which other
                            nodes used to send their ACKs and resend messages to. When several
                            single process instances are used each single process requires for
                            each reliable channel an unique UDP port number for this purpose.
                            This enables control over the used UDP port numbers which may be
                            needed when firewall are used.
              <br><br>
              <b>Solution: A configuration element "AllowedPorts" is added to either the networking
                            channels or channel configuration which specifies the range (list)
                            of UDP port numbers available. When specified an available port
                            from the configured port range is used. When not specified the
                            configurated channel port+1 is used when it is avaliable, otherwise
                            a dynamic allocated UDP port is used. (Note: Deployment manual will
                            be updated at 6.2.2 release. Use the configurator tool for further
                            information.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1326
          </td>
          <td>
            <b>Idlpp crashes when two types in different modules have the same name
                        and one of them has a keylist</b><br/>
            <i>
              When an idl is used with 2 or more types in different modules and
                            they have the same name and one or more have a keylist idlpp will crash.
              <br><br>
              <b>Solution: The fault in idlpp has been fixed and it is now save
                            to use multiple types with the same name and a keylist in idl.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1474/<br>11036
          </td>
          <td>
            <b>Windows Event Log plug-in failure to initialise</b><br/>
            <i>
              The Windows Event Log plug-in could fail with an access violation. This could
              prevent the OpenSplice daemon starting as a Windows service as this plug-in is
              configured to be used by default in this circumstance.
              <br><br>
              <b>Solution: The Event Log now initialises correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1488
          </td>
          <td>
            <b>DDSI2 fails to handle keyed topics with 64 members (and some variations) </b><br/>
            <i>
              DDSI2 potentially performs some key re-ordering to remove the dependencies
              between the ordering of keys in the original IDL topic definition, the order
              of in which the members of the topic are serialised and the order in which
              the key fields are serialised. An mistake was found in the arithmetic that
              caused it to refuse topics with more than 64 members, or one with a key inside
              a nested struct with 32 fields inside one with 33 fields, &c. In such a case,
              no DDSI DataReaders and DataWriters would be created and no communication
              would take place for that topic, and the error log showed a generic error
              message: "handlePublications: new_writer: error -1" or
              "handleSubscriptions: new_reader: error -1".
              <br><br>
              <b>Solution: It now performs as intended and handles large structs correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1521/<br>11075
          </td>
          <td>
            <b>Datareader deletion could cause memory leaks</b><br/>
            <i>
              Under certain conditions datareaders would not clean up their resources
                            when becomming deleted. These leaks could occur when readers refuse
                            to connect to writers because of a partition mismatch.
              <br><br>
              <b>Solution: The defect in the cleanup algorithm of datareaders is now
                            fixed and will not leak anymore.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1522/<br>11074
          </td>
          <td>
            <b>A reader subscribing to transient data using the "*" (wildcard)
                        partition when samples are written to multiple partitions may lead to
                        duplicate samples at the reader</b><br/>
            <i>
              When a reader subscribes to a newly created topic/partition combination,
                            because a writer on that partition has just been created, it should also
                            request the historical data associated with combination. The issue was
                            that it was actually being delivered the historical data for all
                            partitions, including those for which samples may have already been
                            delivered and read/taken.
              <br><br>
              <b>Solution: The behaviour has been changed so that historical samples
                            are only delivered for the specific topic/partition that the reader
                            has just subscribed to.</b>
            </i>
          </td>
        </tr>
     </table>
    <p><h2>6.2.1p1</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1326
          </td>
          <td>
            <b>idlpp crash when two types in different modules have the same name
               and one of them has a keylist</b><br/>
            <i>
              When an idl is used with 2 or more types in different modules and they
              have the same name and one or more have a keylist idlpp will crash. For example;
              <pre>
              module A {
                  struct Z {
                      long m;
                  };
              };
             struct Z {
                  long m;
             };
             #pragma keylist Z
             </pre>
              <br><br>
              <b>Solution: The fault in idlpp has been fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1422
          </td>
          <td>
            <b>Lookup_instance leaks memory </b><br/>
            <i>
              The DataReader_lookup_instance call leaked one string for topics
              with string-keys, when a non-existing instance was looked up in a
              non-empty set.
              <br><br>
              <b>Solution: The leak is fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1424
          </td>
          <td>
            <b>DDSI2 can stop accepting data when receiving fragments of old samples</b><br/>
            <i>
              TDDSI2 defragmentation buffers have limited capacity and when full
              decide what to accept and what to drop on a policy that favours
              lower sequence numbers of higher ones in case of reliable communication.
              This policy causes the buffers to fill up slowly when every now and then
              a subset of the fragments of a sample arrive, but not all of them.
              This can only happen for "old" samples, as they have will been delivered
              to the data readers and no retransmits will be requested. Under normal
              circumstances it is very rare to receive some retransmitted fragments
              after receiving the full sample, but on networks with long delays,
              reordering and packet loss, such as a WAN, this problem is quite likely
              to surface.
              <br><br>
              <b>Solution: The current solution actively drops already accepted data
              from the defragmentation buffers, preventing uncontrolled build-up of
              incomplete samples.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1425
          </td>
          <td>
            <b>DDSI2 can request a retransmit of a full sample in addition to a
               retransmit of some of its fragments</b><br/>
            <i>
              TDDSI2 considers the full state of a proxy writer and its data readers
              when generating a retransmit requests in response to a writer heartbeat,
              with the aim of generating the least costly retransmit request for the
              missing data. However, it can generate a retransmit request of some
              fragments of a sample while simultaneously requesting a retransmit of the
              full sample.
              <br><br>
              <b>Solution: The retransmit request for the full sample is now suppressed
              in cases where a retransmit of some fragments is requested.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1426/<br>10961
          </td>
          <td>
            <b>DDSI2 can crash on topic creation</b><br/>
            <i>
              On platforms where malloc(0) returns a null pointer, DDSI2 could
              crash on topic creation.
              <br><br>
              <b>Solution: DDSI2 now explicitly allows for this.</b>
            </i>
          </td>
        </tr>
     </table>
    <p><h2>6.2.1</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-871
          </td>
          <td>
            <b>Incorrect determination of the lifespan</b><br/>
            <i>
              The lifespan expiry time is determined when a message is inserted
              in the reader. If the ReaderLifespanQos is enabled, the lifespan
              duration is always extracted from the ReaderLifespanQos. Both
              ReaderLifespanQos and the (inline) LifespanQos of the message should
              be considered. The earliest expiry time should be the one that is used.
              <br><br>
              <b>Solution: The expiry time determination algorithm has been fixed
              and will now also consider the (inline) LifespanQos of the message.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-997/<br>10738
          </td>
          <td>
            <b>ReaderLifespan QoS not rewarded in combination with read with condition</b><br/>
            <i>
              When a read with condition is done and the ReaderLifespan QoS is set
              the Qos is not evaluated. This possibly results in that the reader
              can return samples that are already expired.
              <br><br>
              <b>Solution: The read with condition algorithm has been fixed and
              will now check the ReaderLifespan Qos when it is set.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-1181</br>
          </td>
          <td>
            <b>Record and Replay Service does not adhere to service lease settings</b><br/>
            <i>
              The Domain configuration of OpenSplice contains a section on service
              leases: a mechanism that monitors the health of OpenSplice services.
              The RnR service did not correctly register with this mechanism, causing
              a message to appear in the ospl-info.log informing the user the R&R
              service has died while actually it was still running.
              <br><br>
              <b>Solution: The service now adheres to the 'Domain/Lease/ExpiryTime'
              configuration settings and the message is only printed to the
              OpenSplice info log when the service actually terminates unexpectedly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1302
          </td>
          <td>
            <b>DDSI2 can miss local discovery events causing issues or blocking behaviour</b><br/>
            <i>
              DDSI2 needs to react to the creation of readers, writers and participants
              on the same node, as well as to the creation of new partition-topic
              combinations and some housekeeping events. Under adverse timing conditions
              it may fail to process all events if some are combined into a single event
              record for efficiency.
              <br><br>
              <b>Solution: It now always processes all events in a single event record.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1324
          </td>
          <td>
            <b>Time-range not applied correctly in REPLAY commands</b><br/>
            <i>
              An issue with processing time-ranges in REPLAY-commands caused problems
              when multiple interest-expressions are used in a single REPLAY-command.
              Time-ranges would only be applied to the first interest expression,
              causing data matching other interest-expressions to be replayed
              without the desired time-range constraints.
              <br><br>
              <b>Solution: Time-ranges are now applied to all interest-expressions
              in the command. In case the storage also contains data that should
              be replayed regardless of time-range constraints, a second
              REPLAY-command can be used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1392
          </td>
          <td>
            <b>OSPL-XML metadescriptor optimizations </b><br/>
            <i>The XML metadescriptor, which is used internally to communicate idl
              types, was using fully-scoped where a relatively scoped name would
              suffice, and did no optimization on the ordering of types, which
              potentially results in a lot of unnecessary module open and close tags.
              <br><br>
              <b>Solution: Relatively scoped names are used wherever possible.
              An algorithm is introduced that orders types based on their module,
              so the number of module-transitions is minimized.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1392
          </td>
          <td>
            <b>Time-range not applied correctly in REPLAY commands</b><br/>
            <i>
              An issue with processing time-ranges in REPLAY-commands caused problems
              when multiple interest-expressions are used in a single REPLAY-command.
              Time-ranges would only be applied to the first interest expression,
              causing data matching other interest-expressions to be replayed
              without the desired time-range constraints.
              <br><br>
              <b>Solution: Time-ranges are now applied to all interest-expressions
              in the command. In case the storage also contains data that should
              be replayed regardless of time-range constraints, a second
              REPLAY-command can be used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1395
          </td>
          <td>
            <b>VxWorks distribution incorrectly included duplicated header files</b><br/>
            <i>
              Prior to this release the VxWorks distribution erroneously included a
              second copy of some files from $OSPL_HOME/include/include in the directory
              $OSPL_HOME/include. This resulted in compilation errors whenever the
              required OpenSplice product include directories were specified in the
              order: -I"$(OSPL_HOME)/include" -I"$(OSPL_HOME)/include/sys".
              <br><br>
              <b>Solution: The duplicated files have been removed. Compiler header
              directory include directives can be specified in any order.</b>
            </i>
          </td>
        </tr>
    </table>
    <p><h2>6.2.0</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-511
          </td>
          <td>
            <b>Default OSPL xml configuration files not available in RTS</b><br/>
            <i>
              The xml configuration files suppolied by default with the HDE were not in
              the RTS installer.
              <br><br>
              <b>Solution: Configuration files standardised across HDE and RTS installers.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-532<br>
          </td>
          <td>
            <b>DDSI2 keyhash generation is wrong for some cases</b><br/>
            <i>
              DDSI2 always generates the keyhashes included in the DDSI standard
              (they are spec'd as optional). For topics of which the (big-endian) CDR
              representation of the key is or may be longer than 16 bytes the keyhash
              is computed as the MD5 hash of the serialized key.DDSI2 produces an incorrect
              hash on little-endian platforms for keys containing bounded strings where
              the total serialized length of the key is &gt= 17 and &lt=32 bytes. In
              practice this affects interoperability when multiple nodes publish
              the same instance:- between little- and big-endian machines running
              DDSI2- between little-endian DDSI2 and any other DDSI implementation.
              <br><br>
              <b>Solution: Corrected the keyhash generation.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-511
          </td>
          <td>
            <b>OpenSplice Threads should be named</b><br/>
            <i>
              To assist users (especially in the single process architecture) to name
              the threads used by OSPL.
              <br><br>
              <b>Solution: Major OSPL threads have been named.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-558
          </td>
          <td>
            <b>Narrow operation in examples leaking</b><br/>
            <i>
              The use of the c++ _narrow operation on the created readers and
              writer entities in some of the OpenSplice examples led to memory leaks.
              <br><br>
              <b>Solution: These leaks have been resolved by better use of the
              _var helper type when calling the _narrow operation.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-593
          </td>
          <td>
            <b>OpenSplice domain service installation as a native windows service</b><br/>
            <i>
              At OpenSplice v6.1 running the OpenSplice domain services
              under the Windows Service Control Manager required the Windows Common
              Language Runtime. It also required the nomination on installation of
              a single global log directory that would hold all OpenSplice service
              and application process logs. When selecting to install the domain as
              a Windows service an incompatible 'single process mode' OpenSplice XML
              configuration file was installed by default.
              <br><br>
              <b>Solution: Installing and running OpenSplice services as a Windows
              Service now uses only unmanaged APIs so the .NET CLR is no longer
              required. A global log directory is no-longer required or prompted
              for during product installation. The installer will specify a 'shared
              memory' service configuration and will include a domain configuration
              entry to direct the log output from service processes to the Windows
              Event Log,for instance:
              <pre>
   &lt;Domain&gt;
       ...
       &lt;ReportPlugin>
          &lt;Library file_name="service_logger" /&gt;
          &lt;Initialize symbol_name="service_logger_init" /&gt;
          &lt;TypedReport symbol_name="service_logger_typedreport" /&gt;
          &lt;Finalize symbol_name="service_logger_shutdown" /&gt;
          &lt;SuppressDefaultLogs&gt;True&lt;/SuppressDefaultLogs&gt;
          &lt;!-- Change below to 'False' if you wish to log OpenSplice system log events from application proceses
              to the EventLog also --&gt;
          &lt;ServicesOnly>True&lt;/ServicesOnly&gt;
       &lt;/ReportPlugin&gt;
   &lt;/Domain&gt;
             </pre>
             See the Deployment Guide for more information regarding log plug-in configuration.
             Note that by default OpenSplice log events from application processes
             will still now be written to ospl-error/info.log files in the local
             directory, just as when not running OpenSplice as a Windows Service,
             but that this can be changed as per the comment in the snippet above.
             Note also that this entry can still be added to your OpenSplice domain
             configuration when OpenSplice is not installed as a Windows Service to
             direct service (or service and application) log output to the Event Log.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-659
          </td>
          <td>
            <b>The C++ using the standalone C DCPS API 'PingPong' example has been removed</b><br/>
            <i>
              The C++ using the standalone C DCPS API 'PingPong' example duplicated the code already
              available in the standalone C DCPS API 'PingPong' example unnecessarily given that the
              use of a C API from C++ is trivial.
              <br><br>
              <b>Solution: The example has been removed. Users wishing to use the standalone C DCPS
              API from C++ should refer to the the standalone C examples. These can be compiled
              with a C++ compiler.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-711 / OSPL-788
          </td>
          <td>
            <b>Scheduling parameters for the top level service threads does not work when using single process deployment</b><br/>
            <i>
              When applying scheduling(OpenSplice/Domain/Service/Scheduling)
              properties when using single process deployment the scheduling
              properties are not applied on the service.
              <br><br>
              <b>Solution: The defect is now fixed and the service thread will
              get the configured scheduling settings.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-775
          </td>
          <td>
            <b>Exceptions 'throw' statements in rmi C++ generated code </b><br/>
            <i>
               For consistency and best practice, the C++ exceptions specification has been ignored
               and instead the IDL to C++ mapping rules followed.
            <br><br>
              <b>Solution: All 'throw' specifications were removed from the RMI C++ generated code.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-778
          </td>
          <td>
            <b>DDSI2 does not always deliver data for wildcard-based subscriptions</b><br/>
            <i>
              To deliver data from remote writers to subscriptions using partition wildcards,
              DDSI2 sometimes needs to locally register the actual partition used.
              However, the optimisation used to avoid doing so unnecessarily did not correctly
              handle case in which a wildcard reader was matched to writers in different
              partitions.
              <br><br>
              <b>Solution: Optimisation corrected.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-781 /<br/>10369
          </td>
          <td>
            <b>The reliable channel does not operate correctly when using IPv6 on the Windows platform</b><br/>
            <i>
              For Windows a bind is performed on the sending socket using the same portnumber as used by the receiving socket.
              This may cause that ACK messages are not received and that the reliable communication fails.
              <br><br>
              <b>Solution: The bind is not necessary at the sending socket and is removed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-801
          </td>
          <td>
            <b>C API ReaderDataLifecycleQosPolicy QoS policy attribute inconsistency</b><br/>
            <i>
              There is an inconsistency between the naming of the ReaderDataLifecycleQosPolicy
              QoS policy attribute 'invalid_sample_visibility' between the C (SAC) API and
              other APIs. In the other APIs it is called 'invalid_sample_visibility' and
              in C (SAC) API it is called 'invalid_samples_visibility' (in plural form).
              <br><br>
              <b>Solution: The ReaderDataLifecycleQosPolicy QoS policy attribute in the C
              (SAC) API is now also changed to 'invalid_sample_visibility'</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-807/<br>5725
          </td>
          <td>
            <b>Durability resource_limits.max_samples QoSPolicy is not applied properly</b><br/>
            <i>
              When a max_samples resource limit is set in the QoS of the Topic,
              samples could get rejected even though the limitwasn't reached. This
              was caused by the fact that the samples that overwrote a previous value
              sometimes resulted in an increasing number of samples in the administration
              that counted the samples where the counter should have remained at the same value.
              <br><br>
              <b>Solution: The counter for the number of samples is now not increased any
               longer when the sample overwrites a previous value.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-808/<br>7136
          </td>
          <td>
            <b>Tuner does not allow to edit array elements</b><br/>
            <i>
              When connecting the Tuner and creating a reader/writer to a running
              system that contains a topic with array elements, it shows that in the Writer
              tab, the elements of the array can not be modified.
              <br><br>
              <b>Solution: The defect in the Writer tab is now fixed and array
              elements can be edited now.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-809/<br>8239
          </td>
          <td>
            <b>Durability does not allow to overrule alignmentKind for built in partition.</b><br/>
            <i>
              The durability does not allow to overrule alignmentKind for the name
              space with the built-in partition. If set to Lazy, durability will create
              an extra name space with the built-in partition and alignmentKind set
              to Initial_And_Aligner.
              <br><br>
              <b>Solution: The durability service now no longer overrides the behaviour
              for the built-in topics, but only if they are disabled in the Domain
              configuration (//OpenSplice/Domain/BuiltinTopics[@enabled]).</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-814/<br>
          </td>
          <td>
            <b>When spliced discovers a disconnecting node it only unregisters 1 DataWriter per partition-topic combination </b><br/>
            <i>
              When spliced discovers a disconnecting node by its missing heartbeatst,
              it unregisters all writers belonging to the disconnected node. However,
              maximally only 1 DataWriter is unregistered for a specific partition-topic
              combination. If the disconnected node has more than 1 writer attached to
              the same partition-topic combination, only one of them will be unregistered.
              <br><br>
              <b>Solution: The internal algorithm that takes care of the unregistration
              of disconnected DataWriters has been changed to ensure all DataWriters
              of the disconnected node are unregistered.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-820
          </td>
          <td>
            <b>ospl tool should ensure the environment is cleaned up before termination when started in blocking mode</b><br/>
            <i>
              When the ospl tool is started in blocking mode (ospl -f start), it should
              ensure that both the shared memory segment and the key-file are deleted before
              terminating. In the normal case on UNIX-platforms where spliced is requested
              to stop with a SIGQUIT or SIGTERM, that service will clean up all resources itself.
              However, when spliced is terminated with a SIGKILL, ospl should take care of the clean-up.
              <br><br>
              <b>Solution: Ospl now ensures that key-file and shm segment are cleaned up when
              started in blocking mode, even when spliced is killed with a SIGKILL</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-836
          </td>
          <td>
            <b>DDSI2 fails to report configuration errors in error log</b><br/>
            <i>
              DDSI2 configuration error handling lost the ability to write configuration-related
              error messages to the OpenSplice error log. It instead prints them on stderr
               which may be noticed on Unix boxes when using "ospl start", but is
               typically lost on windows.
              <br><br>
              <b>Solution: DDSI2 now reports configuration errors to the error log again.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-844
          </td>
          <td>
            <b>Issues with starting OpenSplice Tuner and OpenSplice DCG</b><br/>
            <i>
              Setting the SPLICE_TARGET environment variable could result in problems
              starting the OSPL Tuner and DCG tools.
              <br><br>
              <b>Solution: The SPLICE_TARGET is no longer set by the Tuner and dcg scripts.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-846
          </td>
          <td>
            <b>SampleInfo should be extended with reception_timestamp for all language bindings</b><br/>
            <i>
              For all language bindings, a new field has been added to the SampleInfo
              called reception_timestamp. This field represents the local time at which
              the corresponding sample has been delivered to the reader from which
              it is being accessed.
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-854/<br>10511
          </td>
          <td>
            <b>read/take returns NO_DATA while the reader has data</b><br/>
            <i>
              In case the data sequence and sample info sequence _maximum are set to
              0 and the _release flag is set to TRUE, for a read or take call with max
              samples set to unlimited, The read or take call will always return with code NO_DATA.
              <br><br>
              <b>Solution: The defect in the read/take call is solved and now returns the correct data.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-890/<br>10558
          </td>
          <td>
            <b>Queried meta descriptor incorrect </b><br/>
            <i>
              Sometimes a queried meta-descriptor string seems incomplete and is not equivalent with the original meta-descriptor.
              <br><br>
              <b>Solution: An internal error in the XML meta-descriptor (de)serializer
              has been repaired to fix incorrect behaviour on types with inter-module dependencies</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-904
          </td>
          <td>
            <b>Reader returns NO_DATA, while samples are available and not read</b><br/>
            <i>
              When using read_next_instance_w_condition where
              <ul>
              <li>a handle is NIL</li>
              <li>readcondition is set with sample state NOT_READ, view state ANY,
              instance state ANY</li>
              <li>the reader has more than one instance with one sample per instance</li>
              </ul>
              then on first call it delivers the sample of the first instance, but
              the second time it returns NO_DATA although sufficient samples are available.
              <br><br>
              <b>Solution: The defect in the read function has been fixed and
              now returns a sample when available.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-908
          </td>
          <td>
            <b>Crash when calling DDS_DataReader_create_view in combination with content filtered topics</b><br/>
            <i>
              When calling the DDS_DataReader_create_view function and the system contains content filtered topics a crash can occur.
              <br><br>
              <b>Solution: The defect in the DDS_DataReader_create_view call has been fixed and will no longer crash.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-924
          </td>
          <td>
            <b>Faulty masks evaluation on dataView</b><br/>
            <i>
               The masks evaluation on dataView was faulty due to this queries could continuesly be triggering.
              <br><br>
              <b>Solution: The defect is now fixed and the masks are evaluated properly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-942
          </td>
          <td>
            <b>mmstat maintenance</b><br/>
            <i>
               Minor work to improve mmstat
               <ul>
               <li>It crashes when running in -t mode but without having a Domain running.</li>
               <li>When running in -M mode, a new header is generated for each line.
               This garbles up the output so it should be similar to the -m mode, where a
               header is printed every 15 lines</li>
               <li>When using the -o flag without the -n flag, no items are displayed.
               By default it should just display everything when no explicit number is specified</li>
               <li>The -n flag only works in combination with the -o flag. This should be indicated in the help.</li>
               </ul>
              <b>Solution: Changes applied for the above issues.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-958
          </td>
          <td>
            <b>User clock module dlopen error in error log file.</b><br/>
            <i>
               When a UserClockService is configured the configured library does not
               load and an error (dlopen error) gets reported in the error log.
              <br><br>
              <b>Solution: The configured library resolving algorithm contained a
              fault and is now fixed. The library resolving now follows the following rules:
              <ul>
              <li>when the library name contains a path open this path. If this fails,</li>
              <li>add lib in front of the name and .suffix (depending on the host OS
              this can be so, ddl or dylib) behind it and try to open this library</li>
              <li>if that fails, try to open the given library name</li>
              </ul></b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-970
          </td>
          <td>
            <b>Improvements to DDSI2 network interface selection</b><br/>
            <i>
               In multi-homed systems it is often necessary to instruct DDSI2 which
               network interface to use using the General/NetworkInterfaceAddress
               setting. Until now, this required specifying the exact IP address
               of the host for that network interface, in essence requiring each
               host to have its own configuration file.
              <br><br>
              <b>Solution: New options for specifying the network interface have been added:
              <ul>
              <li>The name of an interface (e.g., eth1) is now acceptable</li>
              <li>For IPv4 networks, if there is no exact match on the full IPv4 address,
              DDSI2 will attempt to match on the network portion only as determined by
              the network masks of the interfaces. It requires that the host portion
              of the specified address is all-zero for such a match.</li>
              </ul>
              With these changes, in the vast majority of cases, there is no longer a
              need for a configuration file per host. Furthermore, a warning is appended
              to the info log whenever DDSI2 automatically selects an interface out
              of several available that have equal ranking. The ranking itself is unchanged,
              with multicast-capable networks preferred over others, then anything
              non-point-to-point, point-to-point and loopback interfaces.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-977
          </td>
          <td>
            <b>DDSI2 can crash when a remote entity disappears</b><br/>
            <i>
               DDSI2 can potentially crash when a remote entity disappears. This
               requires its lease to expire in parallel with processing an
               explicit notification of that remote entity being deleted.
              <br><br>
              <b>Solution: A fix is applied to prevent the crash.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1008
          </td>
          <td>
            <b>Applications may deadlock or crash during signal handling</b><br/>
            <i>
              Applications may deadlock or crash when a signal is received within
              internal signal handling algorithm.
              <br><br>
              <b>Solution: The internal signal handling algorithm has been adapted
              to allow users to set their own signal handler. Furthermore, the algorithm
              ensures that handling of asynchronous signals is performed in a
              dedicated thread to prevent deadlocks during clean-up of DDS entities..</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1021<br>10758
          </td>
          <td>
            <b>Clash with TEST macro from c_mmbase.h</b><br/>
            <i>
              The database code referenced a macro named TEST which could possibly
              clash with other external software and application code.

              <b>Solution: This has been renamed to a better scoped C_MM_STATS macro.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1033<br>/10767
          </td>
          <td>
            <b>Reference manuals incorrect for create_particpant
            <i>
              The reference manuals still had the V5 API definition documented for
              create_particpant API.
              <br><br>
              <b>Solution: Reference manuals now correctly state the domainId is an integer.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1035<br>/10764
          </td>
          <td>
            <b>DDSI2 failure to deserialize bounded strings of maximum length</b><br/>
            <i>
              DDSI2 validates all data coming in over the network, but the input validator
              erroneously considered strings in received network packets of which the
              length equalled the specified bound as being oversize.
              <br><br>
              <b>Solution: DDSI2 now handles this correctly.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1049<br>/107678
          </td>
          <td>
            <b>Java application crash on type register</b><br/>
            <i>
              Randomly, on type register, a java application can crash
              with the following notice: malloc(): memory corruption.
              <br><br>
              <b>Solution: The type register memory allocation algorithm for java
              has been fixed and will no longer crash.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1077<br>
          </td>
          <td>
            <b>The dynamic discovery protocol does not detect all nodes for RT networking.</b><br/>
            <i>
              Dynamic discovery protocol makes use of unicast communication to find all
              the nodes and roles. However the data is sent to the wrong port number which
              depending on the configuration may mean not all nodes are detected.
              <br><br>
              <b>Solution: When sending point-to-point data on a best-effort channel
              the destination port should be the primary port of the channel.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1085<br>
          </td>
          <td>
            <b>Secure Networking parsing of Credentials tag in XML configuration fails</b><br/>
            <i>
              <b>Solution: The spelling mistakes in the secure network configuration files
              have been corrected to be "Credentials" rather than "Credentails".</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1115<br>
          </td>
          <td>
            <b>Specific configuration may cause a crash of secure RT networking.</b><br/>
            <i>
              When the configuration of secure networking does not contain
              specific elements then secure networking may crash because it frees
              un-allocated memory.
              <br><br>
              <b>Solution: Check if configuration elements are allocated or not.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1125<br>
          </td>
          <td>
            <b>The OpenSplice installer no longer uses Windows SDK Global Assembly
            Cache Utility (gacutil.exe) to install the C# binding assembly.</b><br/>
            <i>
              Previously, selecting an option when running the OpenSplice installer
              used the Windows SDK Global Assembly Cache Utility (gacutil.exe) to
              install the C# binding assembly into the Global Assembly Cache.
              The MSDN documentation says the following regarding this tool:
              "In deployment scenarios, use Windows Installer 2.0 to install
              assemblies into the global assembly cache. Use the Global Assembly
              Cache tool only in development scenarios, because it does not provide
              assembly reference counting and other features provided when using
              the Windows Installer."
              <br><br>
              <b>Solution: The installer option of using the gacutil.exe to install
              the assembly has been removed. If users wish to install the C# binding
              assembly into the Global Assembly Cache for development purposes
              they may use the gacutil.exe themselves to do this. Instructions
              for doing this are included in the Deployment Guide. Users should
              use a suitable approved method to install the assembly in deployment
              scenarios, if required.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1127<br>
          </td>
          <td>
            <b>The supported Access Control Module for Secure RT networking is
            identified as "MAC" which is not correct in the implementation.</b><br/>
            <i>
              In the configuration the currently supported Access Control Module is MAC.
              In the implementation an other name is used.
              <br><br>
              <b>Solution: The name of the supported Access Control Module is changed to "MAC".</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1131<br>
          </td>
          <td>
            <b>DDS_string_dup method added to the SAC DCPS mapping</b><br/>
            <i>
              The DCPS SAC mapping did not have a convenient function that could
              be used to duplicate strings.
              <br><br>
              <b>Solution: A DDS_string_dup function has been added to the SAC
              DCPS mapping. Its signature is <code>
              DDS_char* DDS_string_dup (const DDS_char* src);</code>.
              The memory allocated must be freed using DDS_free().</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1148<br>
          </td>
          <td>
            <b>end_coherent_changes() may crash if the publisher contains a writer that
            did not write any samples in that coherent update</b><br/>
            <i>
              There was an issue where calling end_coherent_changes() on a publisher may
              cause a crash if that publisher contained a data writer that had not written
              any samples as part of that coherent update. The algorithm assumed that
              every writer belonging to a coherent publisher would write samples within
              each coherent update.
              <br><br>
              <b>Solution: The algorithm in end_coherent_changes() has been fixed to also
              support data writers that were not active during that coherent update.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1156<br>
          </td>
          <td>
            <b>The Secure RT Networking security policy file is not parsed correctly.</b><br/>
            <i>
              The security profile should contain a classification element in the
              resource section. This element is not parsed correctly.
              <br><br>
              <b>Solution: Add the parsing of the classification element to the
              security profile parser.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1157<br>
          </td>
          <td>
            <b>Secure RT networking may drop messages when authorization is enabled..</b><br/>
            <i>
              The secure RT networking implementation uses libcrypto. Further secure networking
              is multi threaded which may cause that multiple threads may access
              this library concurrently which causes that the checking of the RSA
              signature fails.
              <br><br>
              <b>Solution: To operate libcrypto using concurrent threads callbacks
              are implemented which provide the locking mechanism.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1158<br>
          </td>
          <td>
            <b>DDSI2 could fail to provide historical transient local data in
            configurations without durability</b><br/>
            <i>
              In OpenSplice delivering historical data to new non-volatile readers
              is normally handled by the OpenSplice kernel and durability service.
              However, the DDSI specification prescribes an implementation of
              transient-local data that DDSI2 adheres to, in allowing many
              minimum-profile application to run on OpenSplice without a durability
              service present. In this configuration, DDSI2 does not always provide
              the historical data, instead discarding a sample as if it were a
              notification of a lost sample.
              <br><br>
              <b>Solution: This issue has been fixed by correctly setting the relevant data.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1310
          </td>
          <td>
            <b>Configurator tool does not allow configuring the Report element</b><br/>
            <i>
               The //OpenSplice/Domain/Report element is missing in the configurator tool.
              <br><br>
              <b>Solution: The configuration element plus its attributes have been added to the configurator tool.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1312
          </td>
          <td>
            <b>Default Lease ExpiryTime and updateFactor are incorrect in configurator tool</b><br/>
            <i>
               The default Lease ExpiryTime is set to 10.0 in the splice daemon but to 20.0 in configurator tool.
               The updateFactor is set to 0.2 in the splice daemon but to 0.1 in the configurator tool.
              <br><br>
              <b>Solution: The configurator tool has been updated to match splice daemon implementation.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1314
          </td>
          <td>
            <b>Inconsistent linkage of OpenSSL to secure networking services on Windows x86-64</b><br/>
            <i>
              On the above platform the secure networking services were dynamically linking Open SSL requiring the
              user to install Open SSL in order to use them. This was at odds with the behaviour on all other platforms.
              <br><br>
              <b>Solution: The secure networking services statically link a version of Open SSL on Windows x86-64.
              The secure networking services will be changed to dynamically link to Open SSL in a future version of
              of the product.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    </p>

    <p><h2>6.1.1p4</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1017/<br>10755
          </td>
          <td>
            <b>Added support for Windriver Linux 4.3 on Freescale MPC8308</b><br/>
            <i>
              <br><br>
              <b>Solution: Implementated platform support for windriver linux 4.3 on mpc8308</b>
            </i>
          </td>
        </tr>
      </table>
    </p>

    <p><h2>6.1.1p3</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1017/<br>10755
          </td>
          <td>
            <b>Durability crashes when alignment data contains only unregister messages </b><br/>
            <i>
              The 6.1.1p2 release failed on this issue.
              <br><br>
              <b>Solution: Correctly applied patch to resolve the issue.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1092/<br>10791
          </td>
          <td>
            <b>Liveliness notifications to data readers created on multiple partitions may not happen</b><br/>
            <i>
               If a data reader is created on two partitions, say "A" and "B", then it will only receive liveliness
               notifications for the partition that is last lexicographically, i.e. only for "B", despite a new data
               writer being created on partition A..
              <br><br>
              <b>Solution:  The algorithm that determined whether a data reader's partitions match that of a data
              writer has been fixed to ensure that it supports a data reader with multiple partitions. This means
              that a notification can now happen for more than just the final partition in the list</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.1p2</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-1017/<br>10755
          </td>
          <td>
            <b>Durability crashes when alignment data contains only unregister messages </b><br/>
            <i>
              In the scenario where there are only unregister messages aligned
              by the durability service for a given partition-topic combination,
              the service would crash because an instance lookup would fail.
              <br><br>
              <b>Solution: Durability is made robust against this kind of
              scenario.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1035/<br>10764
          </td>
          <td>
            <b>DDSI2 failure to deserialize bounded strings of maximum length </b><br/>
            <i>
              DDSI2 validates all data coming in over the network, but the input
              validator erroneously considered strings in received network
              packets of which the length equalled the specified bound as being
              oversized.
              <br><br>
              <b>Solution: The error in the validator has been fixed</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.1p1</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>OSPL-892/<br>10091
          </td>
          <td>
            <b>DDSI2 can respond with incorrect set fragments to retransmission requests </b><br/>
            <i>
              The DDSI protocol allows reliable readers to request retransmission of individual fragments
              of large samples. DDSI2 could respond with fragments other than the ones requested, which could
              generally be masked by an eventual request for the full sample to be retransmitted, but would
              cause the communication to come to a complete standstill if a particular fragment always
              got lost. For example, when the rapid (re)transmission of a large amount of data systematically
              lead to a switch throwing away a whole range of packets.
              <br><br>
              <b>Solution: DDSI2 now always responds with the requested fragments.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-919<br>
          </td>
          <td>
            <b>C# Examples do not compile on Windows 64 bit platforms </b><br/>
            <i>
              The C# Examples do not compile on Windows 64 bit platforms.  This was caused by 2 problems:
              <br></br>
              - missing entries in the solution file
              <br></br>
              - bugs in the example source code
              <br><br>
              <b>Solution: The missing entries have been added to the solution file and the bugs fixed in the example source code.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.1</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>
            OSPL-301/<br>10110
          </td>
          <td>
            <b>handle.serial exceeds HANDLE_SERIAL_MASK</b><br/>
            <i>
              Two different perceptions of the maximum serial number for a handle
              exist within the product. Once the serial becomes larger than the
              smallest maximum, the reported error occurs.
              <br><br>
              <b>Solution: The maximum serial number to be used is now to same in
              both locations in the product.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-508/<br>9816
          </td>
          <td>
            <b>Idlpp crashes on invalid recursion.</b><br/>
            <i>
              <br><br>
              <b>Solution: idlpp will check if a type is defined before it is used
              - except for sequences where recursion is allowed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-635<br>10466
          </td>
          <td>
            <b>Unable to determine adapter name in windows</b><br/>
            <i>
              It was possible that when using windows OpenSplice is unable
              to determine the network adapter name.
              <br><br>
              <b>Solution: The defect in the name resolving mechanism is fixed
              according to the MSDN page solution and a proper adapter name is resolved.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-637<br>10466
          </td>
          <td>
            <b>Handling loopback data in native networking</b><br/>
            <i>
              When using shared memory with native networking the configuration
              will never 'see' a co-located single-process instance on the same node.
              Secondly data that loops back (i.e. 'own sent data') is detected in a
              sub-optimal way.
              <br><br>
              <b>Solution: The defect in native networking is solved and a new
              configuration item 'EnableMulticastLoopback' for native networking
              is introduced to optimize loopback data. EnableMulticastLoopback specifies
              whether the networking service will allow IP multicast packets within
              the node to be visible to all networking participants in the node,
              including itself. It must be TRUE for intra-node multicast communications,
              but if a node runs only a single OpenSplice networking service and does not host
              any other networking-capable programs, it may be set to FALSE for
              improved performance</b>
            </i>
          </td>
        </tr>
        <tr>
         <td>
           9963/</br>
           dds3426 / OSPL-646
         </td>
         <td>
           <b>DDSI2 used incorrect encoding for fragment data message headers</b><br/>
           <i>
          DDSI2 incorrectly used to generate and interpret fragmented data
          message headers as if they were slightly extended versions of the
          non-fragmented data message headers. This caused DDSI2 to be
          non-compliant with respect to the standard and to fail to
          interoperate with other vendors' implementations for large samples.
          <br><br>
          <b>Solution: the setting and interpretation has been corrected. This
          breaks backwards compatibility, but because DDSI2 is still in beta,
          this does not constitute a change of policy. For those exceptional
          cases where backwards compatibility is currently an issue, a setting
          Unsupported/LegacyFragmentation has been introduced, which may be set
          to true to continue using and interpreting the old message
          format.</b>
           </i>
         </td>
        </tr>
        <tr>
          <td>
            OSPL-780<br>
          </td>
          <td>
            <b>Incorrect handling of ioctl return code</b><br/>
            <i>
              The return result from a call to ioctl should be interpreted as successful
              if the result is not equal to ERROR. The existing code uses result equal
              to zero as the criteria for success but positive non-zero values are also valid.
              <br><br>
              <b>Solution: The defect in the abstraction layer is now fixed and
              positive non-zero values values are now also valid.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-783<br>
          </td>
          <td>
            <b>IPv6 DontRoute emulation incorrect</b><br/>
            <i>
              Due to lack of support for SO_DONTROUTE on some IPv6 stacks,
              networking tried to emulate the behaviour by setting the hop-count
              to 1. This is not functionally equivalent to SO_DONTROUTE.
              <br><br>
              <b>Solution: The emulation of the option has been removed. When
              set on an IPv6 configured networking service the option will be ignored.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-784<br>
          </td>
          <td>
            <b>Erroneous messages logging when sending initial ACKs if multiple AC
            messages were bundled in a single packet </b><br/>
            <i>
              Erroneous messages were logged when multiple ACK messages were bundled
              into a single packet in the case of multiple partitons with Partition 1
              only has a <b>first ACK</b> to be sent (no pending ACKs) and partition 2
              has a <b>first ACK</b> and/or pending ACKs.
              <br><br>
              <b>Solution: The defect in the ACK logging mechanism has been fixed
              and now logs the correct messages.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-813<br>
          </td>
          <td>
            <b>Consistent final value not always guaranteed with BY_SOURCE_TIMESTAMP</b><br/>
            <i>
              In the unusual scenario where a single writer updates an instance with the
              same timestamp, a consistent final value for that instance was not guaranteed
              across all subscribers. According to the DDS v1.2 spec this should be
              guaranteed in this case.
              <br><br>
              <b>Solution: When updating the administration of the readers the consistent
              final value is guaranteed by incorporating a writer-generated
              sequence number.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-821/<br>10098
          </td>
          <td>
            <b>DDSI2 socket receive buffer sizes are be too small to handle large packet bursts</b><br/>
            <i>
              Large incoming packet bursts could overwhelm the configured buffer
              capacity of the network sockets in DDSI2 networking service. This was
              dependent on many factors, in particular also on scheduling latencies
              at the OS level.
              <br><br>
              <b>Solution: The default receive buffer size is now the minimum of the new
              Unsupported/MinimumSocketReceiveBufferSize option and the operating system
              default UDP socket buffer size. The default value of 64kB should
              suffice for most systems but can be increased where needed.</b>
            </i>
          </td>
        </tr>
          <td>
            OSPL-827
          </td>
          <td>
            <b>liveliness count issue with multiple partitions</b><br/>
            <i>
              When using data readers accros multiple partitions the liveliness count
              increments for all writers instead of only the writers connected to
              the selected partition.
              <br><br>
              <b>Solution: The defect in the liveliness count algorithm is now fixed
              and now only the liveliness count for the writers in the selected
              partition are updated.</b>
            </i>
          </td>
        <tr>
        </tr>
      </table>
    </p>

    <p><h2>6.1.0g2</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>
            dds3554<br>
          </td>
          <td>
            <b>e500v2 based builds need to include the VX_SPE_TASK option when spawning RTPs.</b><br/>
            <i>
           Updated e500v2 based builds to include the VX_SPE_TASK option when spawning RTPs.
            </i>
          </td>
        </tr>
      </table>
    </p>

    <p><h2>6.1.0g1</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>
            dds3554<br>
          </td>
          <td>
            <b>PowerPC P2020 and PPC32 vxworks 6.8, linux hosted builds now supported.</b><br/>
            <i>
           Added PowerPC P2020 and PPC32 vxworks 6.8, linux hosted builds.
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.0p8</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        <tr>
          <td>
            dds3508/<br>10342
          </td>
          <td>
            <b>spliced not running on LynxOS 5.</b><br/>
            <i>
              Problems with spliced not executing correctly on LynxOS 5.
              <br><br>
              <b>Solution: Multiple fixes required, invalid makesystem options and some extra LynxOS posix support required.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>


    <p><h2>6.1.0p7</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            8840 /</br>
            9350 /</br>
            dds2973 /</br>
            dds2590
          </td>
          <td>
            <b>Memory leaks during shutdown of native networking.</b><br/>
            <i>
             During the shutdown of native networking not all administration structures are freed.
              <br><br>
              <b>Solution: The defect in the network termination algorithm is now fixed and all administration is correctly freed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            10065 /</br>
            dds3306
          </td>
          <td>
            <b>DDSI2 may erroneously decide CDR serialized data is invalid</b><br/>
            <i>
             The DDSI2 deserializer attempts to avoid allocating memory for sequences of obviously bogus
             lengths by checking whether the remaining number of bytes is sufficient to encode some sequence
             of the declared length. Unfortunately, it uses the wrong notion of the size of an element, which
             can cause it to incorrectly declare a length to be bogus. This is dependent on platform, type and
             the type and contents of any data preceding the sequence.
              <br><br>
              <b>Solution: Always deserialize all data until the input is really exhausted.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            10064 /</br>
            dds3339
          </td>
          <td>
            <b>Reliable communication fails with multiple connected RT networking services on one node</b><br/>
            <i>
             The statically configured portnumbers collide with multiple instances, causing unicast data not to
             be received by all RT networking instances. For reliability related packets (ACK's, resends, etc.)
             unicast is used on a static port (data-port + 1), even when broad- or multicast is configured. This
             implies that multiple instances try to use the same IP-addres-portnumber combination, which is never
             possible. This causes only the latest bound RT networking service to receive all ACK's and resends for
             any instance on the same IP-address-portnumber combination, breaking reliable communication and causing
             a lot of warnings to be reported regarding unexpected ACK's, etc.
              <br><br>
              <b>Solution: RT networking uses a dynamically assigned portnumber instead of a statically configured portnumber for the reliability related packets.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            10125 /</br>
            dds3391
          </td>
          <td>
            <b>OpenSplice DDS configured with multiple native network services and IgnoredPartitions configuration option set.</b><br/>
            <i>
             In the case that OpenSplice is used with multiple native networking services and IgnoredPartitions is configurated.
             Writing multiple instances with a single dataWriter fails. The first instance will be written all further instances
             will not be written.
              <br><br>
              <b>Solution: The defect in the network administation algorithm in combination with ignored partitions is now fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            9963 /</br>
            dds3426<br>
            dds3478
          </td>
          <td>
            <b>DDSI2 used incorrect encoding for fragment data message headers</b><br/>
            <i>
             DDSI2 incorrectly used to generate and interpret fragmented data message headers
             as if they were slightly extended versions of the non-fragmented data message headers.
             This caused DDSI2 to be non-compliant with respect to the standard and to fail to
             interoperate with other vendors' implementations for large samples.
              <br><br>
              <b>Solution: The setting and interpretation has been corrected. This breaks backwards
              compatibility. For those exceptional cases where backwards compatibility is currently
              an issue, a setting Unsupported/LegacyFragmentation has been introduced,
              which may be set to true to continue using and interpreting the old message format.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.0p6</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            9963 /</br>
            dds3426<br>
            dds3478
          </td>
          <td>
            <b>DDSI2 fragment size is now configurable</b><br/>
            <i>
             DDSI2 never creates data messages containing a payload larger than the
             FragmentSize, any sample larger than the FragmentSize gets split into
             multiple fragments of FragmentSize each. These fragments are then
             transported independently (but may yet be merged into larger UDP datagrams).
              <br><br>
              <b>Solution: This size is now configurable using Unsupported/FragmentSize,
              with a default of 1280 bytes. Values below 1025 bytes violate the DDSI2 specification,
              above approximately 65000 bytes it (probably) won't fit inside a single UDP datagram.
              Increasing the size will shift more fragmenting and reassembling to the IP stack, which
              is generally more efficient because it is done inside the network stack,
              but which is incapable of retransmitting individual lost fragments.
              <b>Increasing it may also allow operating without any fragmenting at the DDSI
              level, which may help avoid interoperability issues.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3486
          </td>
          <td>
            <b>Third party licenses updates</b><br/>
            <i>
             OpenSplice Tester third party tool licenses were not documented in release notes.
              <br><br>
              <b>Solution: Updated docs/html/third_party_licenses.html.</b>
            </i>
          </td>
        </tr>
       </table>
    </p>
    <p><h2>6.1.0p5</h2></p>
     <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            dds3467/<br>10259
          </td>
          <td>
            <b>Signal handling failing on DENX.</b><br/>
            <i>
              Signals were not being correctly handled for the DENX target
              <br><br>
              <b>Solution: Use the generic POSIX implementation.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3217/9881<br>
            dds3430
          </td>
          <td>
            <b>Spliced shared memory leak on remote durability shutdown</b><br/>
            <i>
              A memory leak was found whenever a remote node was started and
              then stopped while durability was also running.
              <br><br>
              <b>Solution: When durability detected a remote node, it would
              write an update into the system (and thus the shared memory on
              the local node). The logic for this write wrongly performed a
              double string creation for the same string. The second string
              creation overwrote the pointer to the first created string, which
              resulted in the first created string to be never freed when the
              remote node disconnected. The solution was to remove the first
              string creation, as it was superfluous. </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3248/<br>9908
          </td>
          <td>
            <b>Partition expressions not matched properly.</b><br/>
            <i>
              In case an application publishes/subscribes to multiple partitions
              and the list of partitions has names that are sub-strings of one
              of the others in the list, some partitions may be ignored. Even
              though communication within a node and communication over the
              network using the native networking service still works correctly,
              this issue causes communication over the network to fail when
              using the ddsi networking service and it also causes listeners
              not to be triggered on matched subscriptions and publications.
              <br><br>
              <b>Solution: The error in the pattern matching algorithm in the
              kernel has been repaired.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3399/<br>10139
          </td>
          <td>
            <b>Incorrect network statistics.</b><br/>
            <i>
              The networking service optionally keeps track of statistics that
              can be inspected by means of the Tuner tool. After inspection it
              turned out that the maxNumberOfPacketsResentToOneNode and
              maxNumberOfBytesResentToOneNode statistics are showing wrong
              values.
              <br><br>
              <b>Solution: The maxNumberOfPacketsResentToOneNode and
              maxNumberOfBytesResentToOneNode statistics are now showing the
              correct values.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3416/<br>10152
          </td>
          <td>
            <b>Manual liveliness not working correctly</b><br/>
            <i>
              Specific circumstances, such as a manual liveliness, could trigger
              a bug in the lease manager that would eventually cause the lease
              manager thread to hang indefinitely. The result is that periodic
              leases, such as the sending and receiving of heartbeats, is
              stopped.
              <br><br>
              <b>Solution: The lease manager was fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3432/<br>10187
          </td>
          <td>
            <b>Applications SEGFAULT when the domain is not started.</b><br/>
            <i>
              In the case no DDS domain is running but there is still a dirty
              shared memory segement present from an old instance of Spliced.
              It can happen that when an application is started, the application
              can crash with a SEGFAULT.
              <br><br>
              <b>Solution: The defect is fixed and the application will no
              longer crash because of this.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3455
          </td>
          <td>
            <b>The durability service should improve the memory used during alignment.</b><br/>
            <i>
             The durability service temporarily caches received alignment data
             until the set for a specific partition-topic combination is
             complete. The algorithm implemented there could be improved to
             reduce the amount of memory used during this phase.
              <br><br>
              <b>Solution: The durability service now stores unregistrations is
              a much more memory-efficient way reducing the memory overhead for
              alignment to a minimum.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3457/10241<br>
            dds3458/10242<br>
          </td>
          <td>
            <b>Crash of networking and/or durability during due to memory exhaustion.</b><br/>
            <i>
              The networking and durability services could crash when the shared
              memory was exhausted.
              <br><br>
              <b>Solution: The services now check the available
              memory threshold and does not claim more memory when the threshold
              has been reached. Furthermore they will terminate when no more
              memory becomes available within a few seconds.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.0p4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            dds3443/</br>
          </td>
          <td>
            <b>Added support for vxWorks 6.8.2 on MIPSI32R2sf</b><br/>
            <i>
          There was no port for vxWorks 6.8.2 on mips.
              <br><br>
              <b>Solution: A build for vxworks 6.8.2 on MIPSI32R2sf has been added</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.0p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            dds3217/</br>9881
          </td>
          <td>
            <b>Spliced shared memory leak on remote node shutdown</b><br/>
            <i>
              When a remote node comes (e.g., ospl start is executed on a remote node) and then
              leaves (e.g., ospl stop is executed on a remote node) the system, a structural increase
              in shared memory is observed.
              <br><br>
              <b>Solution: A memory leak was fixed dealing with heartbeat message of the splice daemon
              being leaked. And a memory leak was fixed where durability history_kind was wrongly
              mixed with regular history depth instead of the value of the durability history depth.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3391/</br>10125
          </td>
          <td>
            <b>OpenSplice DDS configured with multiple native network services and IgnoredPartitions configuration option set</b><br/>
            <i>
              In the case that OpenSplice is used with multiple native networking
              services and IgnoredPartitions is configured. Writing multiple
              instances with a single dataWriter fails. The first instance will
              be written all further instances will not be written.
              <br><br>
              <b>Solution: The network administration algorithm is now
              taking ignored partitions into account as well.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.0p2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            dds3240</br>
          </td>
          <td>
            <b>Usability improvement for OpenSplice RMI</b><br/>
            <i>
              The rmipp compilation requires a step to also compile its output with idlpp. There is
              no need for a user to inititate this.
              <br><br>
              <b>Solution: rmipp compilation now directly calls the idlpp step to
              remove unnecessary user action.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3299</br>
          </td>
          <td>
            <b>Added Support for IPv6 on vxworks 6.7 and 6.8</b><br/>
            <i>
               Added Support for IPv6 on vxworks 6.7 and 6.8, including workarounds for issues with
               vxworks inet_pton, and inet_ntop ( Windriver TSR#1085626 )
              <br><br>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <p><h2>6.1.0p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            dds3348</br>
          </td>
          <td>
            <b>Robustness of secure-networking service against malformed incoming packets</b><br/>
            <i>
              The secure networking service should be robust against malformed incoming packets,
              but certain cases were (although detected) not handled correctly and could lead
              to a crash of the secure-networking service.
              <br><br>
              <b>Solution: Handling of malformed incoming packets has been fixed, so that they are correctly ignored.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3354/</br>10083
          </td>
          <td>
            <b>DBMSConnect - Bounded strings would get mapped by DbmsConnect to a VARCHAR column with width 6000</b><br/>
            <i>
              DBMSConnect ignored the length of a bounded string and treated it as an unbounded string, which is mapped to
              a SQL'99 VARCHAR column with a width of 6000.
              <br><br>
              <b>Solution: DBMSConnect uses the specified maximum length of a bounded string to determine the
              appropriate width of a column.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3363/</br>10094
          </td>
          <td>
            <b>Installation of the OpenSplice daemon as a Windows service from the Visual Studio 2010 distribution fails with an abort.</b><br/>
            <i>
              Installation of the OpenSplice daemon as a Windows service from the Visual Studio 2010 distribution failed with an abort as
              the installer used an obsolete API.
              <br><br>
              <b>Solution: Installation of the OpenSplice daemon as a Windows service from the Visual Studio 2010 distribution no
              longer fails with an abort as the installer no longer uses an obsolete API.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3365/</br>10098
          </td>
          <td>
            <b>DDSI - Unnecessary packet drops for large "best-effort" messages (>8Kb) when using DDSI2 on windows platform.</b><br/>
            <i>
              When receiving "best-effort" messages larger than 8kB via the DDSI2 service on the windows platform,
              a packetdrop-rate was observed that was higher than expected. This was caused by the UDP receivebuffer
              size on this platform, that defaults to just 8kB.
              <br><br>
              <b>Solution: An enhancement has been implemented that will set the UDP receivebuffer size to 64kB if the
              default value of a platform is lower than that. This will ensure that the UDP receivebuffers are big
              enough for the largest supported packetsize.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3366</br>
          </td>
          <td>
            <b>Shared Memory leaks in OpenSplice RMI</b><br/>
            <i>
              In OpenSplice RMI, each request and each reply uses a different instance. These instances
              were not disposed and unregistered after the request/reply. The reply readers were not taking the samples
              that were destinated to other clients. These two problems were leading to a shm leak : shm memory
              consumption growing for each request/reply until the process ended.
              <br><br>
              <b>Solution: After writing the request/reply, the corresponding writer disposes and unregisters
              the corresponding instance. After taking it's reply, the reply reader takes all samples destinated to other clients.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>

    <p><h2>6.1.0</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            dds2723</br>
          </td>
          <td>
            <b>DataReader does not support the TIME_BASED_FILTER QoS policy</b><br/>
            <i>
              The time-based filter QoS policy allows for pacing of a DataReader, based on a minimum separation time when receiving samples.
              <br><br>
              <b>Solution: The feature is now implemented and described in the OpenSpliceDDS Reference Manuals.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds2784</br>
          </td>
          <td>
            <b>The DomainParticipant find_topic() function blocks for the full timeout period</b><br/>
            <i>
              Where the topic of interest in the DomainParticipant find_topic() call
              did not exist, the find_topic method was blocking for the full
              timeout period even if the topic became available earlier.
              <br><br>
              <b>Solution: If no matching topic is found on find_topic() call,
              the function now checks for a matching topic every 10ms until a matching
              topic is found or the timeout is reached.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds2846</br>
          </td>
          <td>
            <b>Issues with multiple domains on Windows</b><br/>
            <i>
              A number of problems exist in the Windows abstraction layer that become noticeable
              if multiple domains are active and the default SHM mapping address is used.
              File locks are not released causing problems at shutdown and kernel objects
              (condition events, semaphores) are mixed up between the active domains.
              <br><br>
              <b>Solution: A number of improvements were made to ensure a correct operation of OpenSplice on Windows.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3003</br>
          </td>
          <td>
            <b>Cleanup problem in C++ PSM</b><br/>
            <i>
              When an application is terminated while particular entities are still in use by the C++ PSM, these
              entities are not properly cleaned-up and freed. Depending on the context this could cause a crash
              or hang during shutdown.
              <br><br>
              <b>Solution: The cleanup routines of the affected entities were changed to take into account
              additional constraints to ensure a proper cleanup under all circumstances.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3023</br>
          </td>
          <td>
            <b>Crash in writer history cache management</b><br/>
            <i>
              Under some circumstances the set of samples belonging to an instance
              in the writer history cache could become corrupted and cause a crash.
              <br><br>
              <b>Solution: The area of code was reviewed and improved to remove this problem.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3062</br>
          </td>
          <td>
            <b>Synchronization error while shutting down spliced</b><br/>
            <i>
              A thread tried to access the spliced user-space object after it was destroyed.
              <br><br>
              <b>Solution: Spliced now waits until the thread has finished before
              destroying the spliced user-space object.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3125/</br>9463
          </td>
          <td>
            <b>Idlpp does not handle -I correctly and does not include the directory
            of where the IDL is located</b><br/>
            <i>
              idlpp does not handle -I correctly when the paths end with a backslash.
              This typically happens in Visual Studio, because macros always end with
              a backslash. Also the include the directory of where the IDL is
              located is missing from the include path.
              <br><br>
              <b>Solution: Each last backslash from the include path is now removed
              and the include the directory of where the IDL is located is now
              added to the include path.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3142</br>
          </td>
          <td>
            <b>Durability crashed if no configuration was provided</b><br/>
            <i>
              Because durability did not load the default values when a configuration (URI) was omitted on
              the commandline, the configuration was corrupt, which crashed the service.
              <br><br>
              <b>Solution: The service now also loads default settings when a configuration is not available.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3147/dds3189/</br>9725
          </td>
          <td>
            <b>DCPS C++ TypeSupportFactory object leakage</b><br/>
            <i>
              The C++ TypeSupportFactory leaks in case its associated TypeSupport
              is registered with a DomainParticipant due to a missing decrease of
              the reference count of that object in the API itself when the
              DomainParticipant is deleted.
              <br><br>
              <b>Solution: The reference count of the object is now correctly decreased when a DomainParticipant, that has the type registered, is deleted.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3148/</br>9726
          </td>
          <td>
            <b>Maximum UDP payload size could not be configured</b><br/>
            <i>
              DDSI2 used a default maximum payload size for the payload of UDP
              packets which could cause issues on networks that don't support
              fragmentation of UDP frames.
              <br><br>
              <b>Solution: A configuration option has been added that allows the
              maximum payload size to be configured. The
              DDSI2Service/Unsupported/MaxMessageSize element specifies the
              maximum size of the UDP payload ([def] 4096) that will be used by
              DDSI2. DDSI2 will maintain this limit within the bounds of the DDSI
              specification. This currently means that even though MaxMessageSize
              is set below 1192, messages with a payload of up to 1192 may still
              be observed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3154/</br>9756
          </td>
          <td>
            <b>Heap memory leakage due to Java based query conditions.</b><br/>
            <i>
              The copy routines for a sequence of strings for the DCPS Java API
              contained a memory leak due to the fact a release flag wasn't set to true.
              <br><br>
              <b>Solution: The release flag has been set to true.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3156/</br>9753
          </td>
          <td>
            <b>Reliablility problem in networking when running multiple nodes.</b><br/>
            <i>
              When running real time networking and nodes are communicating over a reliable channel,
              it is possible that when a node is shutdown other nodes in the network
              start to lose communication with each other due to a corruption in the
              network administration.
              <br><br>
              <b>Solution: The defect in the network administation algorithm is now fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3168/</br>9771
          </td>
          <td>
            <b>Destruction of ErrorInfo in C++ could cause crash.</b><br/>
            <i>
              Due to a bug in the C++ language binding for the (unsupported) ErrorInfo
              API a crash could occur.
              <br><br>
              <b>Solution: The bug has been resolved, allowing proper destruction
              of ErrorInfo objects in the C++ language binding.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3221</br>
          </td>
          <td>
            <b>Using hostname as address of a NetworkPartition fails</b><br/>
            <i>
              When a valid or invalid hostname is used as the address of a
              NetworkPartition of the network service, the error 'ignoring invalid
              networking address ' is reported and no communication takes place.
              <br><br>
              <b>Solution: The defect was in the hostname resolving algorithm and is now fixed.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3245</br>
          </td>
          <td>
            <b>Durability MMF store leaks unregistrations on injection of persistent data</b><br/>
            <i>
              To make sure that instances are not registered by DataWriters that no longer exist
              (they cannot because the system has been restarted), the durability service will
              inject a self created unregister message for each unique DataWriter for an instance
              and that is proper behavior. However, the unregister message needs to be freed after
              it has been injected and that part is missing. This causes every unregister message
              on instance injection to leak.
              <br><br>
              <b>Solution: Unregister messages are now freed after re-publishing them.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            dds3266</br>
          </td>
          <td>
            <b>Crash in signal handler due to uninitialized value</b><br/>
            <i>
              During code analysis, a possible scenario was detected that could results in
              a crash in the signal handler if it's thread received a signal before a
              value was initialized.
              <br><br>
              <b>Solution: The code has been updated to avoid this possible scenario.</b>
            </i>
          </td>
        </tr>
      </table>
    </p>
    <h2><a name="issues_api">Fixed Bugs and Changes affecting API / behaviour</a></h2>
    <h2>6.7.2</h2>
    <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6636 / 17203
          </td>
          <td>
            <b>In the Isocpp2 API a deadlock may occur when a listener is removed when a listener callback is in progress.</b><br/>
            <i>
            When using the Isocpp2 API a deadlock may occur when removing a listener. Before a listener callback is called a mutex 
            lock is taken to prevent that the listener can be destoyed when the listener callback is active. 
            When in the listener callback an Isocpp2 operation on the corresponding entity is called the associated entity 
            mutex lock may be taken. When at the same time the entity is being closed from another thread a deadlock may occur 
            because both involved mutex locks are performed in different order.
             <br/>
             <b>Solution: For all listener operations a separate mutex is used to prevent the listener to be removed when a listener callback is active. 
             This lock is used outside of the normal entity lock which ensures that no deadlock can occur.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9654 / 17532
          </td>
          <td>
            <b>Missing function prototypes on C APIs.</b><br/>
            <i>
            When using the missing-prototypes compiler warning flag, the generated code for SAC, C99 and FACE will trigger that compile warning. 
             <br/>
             <b>Solution: Add prototypes just before the related generated function definitions.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-9672 / 17534
          </td>
          <td>
            <b>Unnecessary error trace in the classic C++ API.</b><br/>
            <i>
             An error will be traced when getting the listener of a Subscriber, Publisher, DataWriter or DataReader when no listener was set. 
             This isn't really an error and the traces are unnecessary and even unwanted. This error is also traced when calling Subscriber::notify_datareaders().</i>
             <br/>
             <b>Solution: The error traces are removed.
             </b>
            </i>
          </td>
        </tr>
         <tr>
         <td>
            OSPL-9730 / 17556
          </td>
          <td>
            <b>Out of sync dispose_all_data.</b><br/>
            <i>
           The dispose_all_data is an asynchronous C&M operation that is not part of a coherent update. 
           It is advised to use BY_SOURCE_TIMESTAMP as destination_order when using dispose_all_data to negate the asynchronous 
           behaviour of it. However, the dispose_all_data still uses the BY_RECEPTION_TIMESTAMP of the built-in C&M operation. 
           This introduces a small timing issue when another node executes the dispose_all_data and immediately writes a new sample. 
           If the write overtakes the dispose_all_data, it is possible that the dispose_all_data will dispose of that sample, 
           while in fact the sample is newer than the dispose and should be retained.</i>
             <br/>
             <b>Solution: The dispose_all_data handling now uses the source timestamp in stead of the reception timestamp regardless 
             of the built-in topic destination_order.
             </b>
            </i>
          </td>
        </tr>
         <tr>
         <td>
            OSPL-9823
          </td>
          <td>
            <b>C# API reports "Detected invalid handle" in error log during onDataAvailable callback.</b><br/>
            <i>
          When the C# API performs an onDataAvailable callback on a DataReaderListener, it also logs a message in the error log that states 
          that an invalid handle was detected. The error reported is harmless and totally irrelevant, and your application will continue to function properly.</i>
             <br/>
             <b>Solution: An onDataAvailable callback will no longer cause this error to show up in the log.
             </b>
            </i>
          </td>
        </tr>
     </table>
    <h2>6.7.0</h2>
    <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
            OSPL-6636
          </td>
          <td>
            <b>Invalid return code of offered- and requested-deadline-missed get status operations in classic C++ language binding</b><br/>
            <i>
            The DataWriter::get_offered_deadline_missed_status and DataReader::get_requested_deadline_missed_status return an error
            code if the total count is 0. The status output-parameter is still filled correctly. Because there's no
            last_instance_handle when no deadline is missed yet, the language-binding would incorrectly determine the instance
            handle is invalid causing the error retcode.
             <br/>
             <b>Solution: The check was fixed to allow for this special case.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
            OSPL-6647
          </td>
          <td>
            <b>Wrong status reset for on_data_available and on_data_on_readers</b><br/>
            <i>
            When an on_data_available event is handled in the DCPS API, it
            should reset the data_available state before calling the listener.
            This is done, but it is done on the entity associated with the
            listener instead of the entity where the event originates from. This
            means for instance, that when a DomainParticipant receives an
            on_data_available event, it tries to reset the data_available state
            of itself instead of the related reader. The Subscriber has the same
            problem. In the DataReader, the entity is the source itself, so it
            isn't a problem there. The same happens when a on_data_on_readers
            event is received. But this has an additional problem that it resets
            the data_available status, while it should reset the
            on_data_on_readers status of the related subscriber. This problem
            exists on all DCPS language bindings.
             <br/>
             <b>Solution: Fixed behaviour of on_data_available and
             on_data_on_readers to reset the status of the entity where the
             event originated. Also fixed on_data_on_readers so that it resets
             the correct on_data_on_readers status and not the
             on_data_available status. This was done for all API's.
             </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7343</td>
          <td>
            <b>Topics can be created with invalid names.</b><br/>
            <i>
             Topics can be created with invalid characters in their names.
             However, the subsequent creation of DataWriters or DataReaders will
             fail when using such a Topic that has an invalid name. A topic name
             can consist out of the following characters: ‘a’-‘z’, ‘A’-‘Z’,
             ‘0’-‘9’ and ‘_’, but it may not start with a digit.
             <br/>
             <b>Solution: The Topic creation fails when a name with invalid
             characters is used.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7560 / 15776</td>
          <td>
            <b>When the TimeBasedFilterQos policy is used and the writer stops
            publishing after having published a series of samples, readers do
            not receive the latest state that was published after the
            minimum_separation delay has passed.</b><br/>
            <i>
             The TimeBasedFilterQosPolicy can be used to control the rate at
             which readers receive samples. In particular, if there is a high
             frequency writer and receiving applications cannot keep up, then
             the TimeBasedFilterQosPolicy can be used to reduce the rate at
             which application readers receive the samples. In the previous
             implementation readers would not receive the latest state that was
             published in case the writer stops publishing after the
             minimum_separation delay has passed.
             <br/>
             <b>Solution: Readers now receive the latest state that was
             published after the minimum_separation delay has passed if the
             instance has changed in the meanwhile and the reader's
             ReliabilityQosPolicy is set to RELIABLE.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.4</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>TSTTOOL-396</td>
          <td>
            <b>Query condition support in Python Scripting Engine</b><br/>
            <i>
             The Python scripting engine now supports query conditions.
            </i>
          </td>
        </tr>
        <tr>
          <td>TSTTOOL-397</td>
          <td>
            <b>Status condition support in Python Scripting Engine</b><br/>
            <i>
             The Python scripting engine now supports status conditions.
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.2p1</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7688</td>
          <td>
            <b>Query interface supports non-ISO compliant '!=' operator again.</b><br/>
            <i>
             In a previous release the non-ISO compliant operator '!=' was
             removed from the supported SQL syntax. This was done in an effort
             to provide one common and standards compliant SQL parser for all
             the various services/languages in the product (where previously
             different components could use different parsers that supported
             different syntaxes). However, although the operator in question was
             non-ISO compliant and never advertised in our documentation, many
             users already used in in their applications.
             <br/>
             <b>Solution: The '!=' operator has been added to the supported
             SQL syntax of our SQL parser.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.2</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7688</td>
          <td>
            <b>Waiting for historical data with condition was not available for
            IsoCpp2.</b><br/>
            <i>
             The proprietary function wait_for_historical_data_w_condition() was
             not implemented on the AnyDataReaderDelegate.
             <br/>
             <b>Solution: The function wait_for_historical_data_w_condition()
             has been introduced on the AnyDataReaderDelegate.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7872</td>
          <td>
            <b>Java5 DDS API lack support for conditional
            wait_for_historical_data operation on DataReader</b><br/>
            <i>
             Java5 DDS API lack support for conditional
             wait_for_historical_data operation on DataReader.
             <br/>
             <b>Solution:  The proprietary conditional waiting for historical
             data has been implemented and can be used by casting the DataReader
             to the OpenSplice-specific org.opensplice.dds.sub.DataReader and
             invoking waitForHistoricalData() method with the desired
             parameters.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.0p3</h2>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7676 / 15818</td>
          <td>
            <b>Several issues with new ISOCPP2 backend for idlpp.</b><br/>
            <i>
             The new backend for ISOCPP2 was not working correctly for anonymous
             and inner types, and generated code had include dependencies on
             classes that were part of the classic IDL-C++ language mapping.
             <br/>
             <b>Solution: Anonymous and inner types are now fully supported and
             all depencencies on the classic C++ language mapping have been
             removed.</b>
            </i>
          </td>
        </tr>
    </table>
    <h2>6.6.0p1</h2>
    <p>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-7572 / 15788</td>
          <td>
            <b>ISOCPP2 not using C++11 compliant idlpp backend.</b><br/>
            <i>
             The C++11 Backend used by idlpp was not fully compliant with the
             new IDL-C++11 mapping as mandated by the OMG.
              <br/>
              <b>Solution: A new C++11 backend has been provided for the
              isocpp2/isoc++2 target of idlpp that is truly complying with the
              OMG mandated IDL to C++11 language mapping. This results to a
              number of relevant and incompatible changes with respect to the
              previous C++11 backend:
              <ul>
                <li>boolean in IDL now truly maps to the C++ bool type, so only
                "true" and "false" are now allowed.</li>
                <li>enum in IDL now behaves like the "enum class", which means
                the enums labels will be scoped with the type of the enum.</li>
                <li>The default (empty) constructor for structs and unions now
                initializes all attributes to valid values. (e.g. 0 for the
                primitive types, strings are empty, vectors are zero-length)</li>
              </ul>
              For backward compatibility with the previous isocpp2 backend, a
              new flag is available on idlpp ("-o deprecated-c++11-mapping")
              that forces it to use the previous backend.</b>
            </i>
        </tr>
      </table>
    <h2>6.6.0</h2>
    <p>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6024 / 14394
          </td>
          <td>
            <b>DCPS Java5 API provides incorrect default LatencyBudget QosPolicy and DataWriterQos.reliability QosPolicy</b>
            <br/>
            <i>
            The DCPS Java5 API uses infinite LatencyBudget as default instead of
            zero duration and BEST_EFFORT reliability for DataWriterQos
            reliability instead of RELIABLE.
             <br/>
             <b>Solution: Default LatencyBudget is now zero and default
             DataWriterQos reliability is now RELIABLE.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-7528</td>
          <td>
            <b>WaitSet::wait() and the WaitSet::dispatch() behave different on
            time-out</b><br/>
            <i>
             Make sure that both the WaitSet::wait() and the
             WaitSet::dispatch() behave identical, so let the WaitSet::wait()
             also throw a TimeoutException when the timeout occurs.
             <br/>
              <b>Solution: WaitSet::dispatch() and WaitSet::wait() would differ
              in how they responded to a timeout: the first would throw a
              TimeoutError, the second would not. That behavior has been made
              more consistent now: the WaitSet::wait() also throws a
              TimeoutError when the indicated timeout elapses.</b>
            </i>
        </tr>
      </table>
    </p>
    <h2>6.5.1</h2>
    <p>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6024 / 14394
          </td>
          <td>
            <b>Ignore outstanding loans when deleting entities</b>
            <br/>
            <i>
            A datareader keeps history about open loans. Open loans are memory
            given to the user by a take or a read action on a datareader and are
            not (yet) returned with a call to return_loan. A datareader cannot
            be deleted if there are open loans. The delete_datareader operation
            will return a RETCODE_PRECONDITION_NOT_MET returncode. There is a
            need to support deletion of a DataReader when open loans still
            exist though.
             <br/>
             <b>Solution: To be able to ignore any open loans in case of a
             deletion of a datareader a new property "ignoreLoansOnDeletion" has
             been introduced on the datareader, with a default value false. If
             this property is set to true using the set_property operation, the
             datareader will ignore its open loans in case of a delete action.
             The value is interpreted as a boolean (i.e., value must be either
             'true' or 'false')  "false" (default). In case the property is set
             to false, the datareader will check for open loans in case of a
             delete action and will return PRECONDITION_NOT_MET in case of open
             loans. In case the property is set to "true",  the datareader will
             ignore open loans and can be deleted in such a situation. This new
             property is only available on classic Java and C++ API's, both
             standalone and in CORBA-cohabitation mode.</b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-6342</td>
          <td>
            <b>Support for dynamic network partitions for RTNetworking</b><br/>
            <i>
              The RTNetworking service needs to support for changing
              network partitions dynamically at run-time.
              <br/>
              <b>Solution:This feature allows users to amend the configuration
              of the RTNetworking service at runtime by means of a Topic-API.
              For more details, check the
              <a href="./rtn_dynamic_partitions.html">RTNetworking dynamic partitions guide</a></b>
            </i>
          </tr>
        <tr>
          <td>OSPL-7021</td>
          <td>
            <b>The behavior of the dispose_all operation is different from the behavior of the dispose
              operation for reader on the same federation.</b><br/>
            <i>
              The dispose_all operation is performed asynchronously. This may cause that readers on the
              same federation my still read alive samples after the execution of the dispose_all operation.
              With this respect the dispose_all operation behaves differently from that of the dispose operation.
              The dispose operation has immediate effect for readers on the same federation.<br/>
              <b>Solution: For readers on the same federation the dispose_all operation is performed synchronously.
                The state of the readers connected ot the same federation is updated during the dispose_all operation.
              </b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-7027 / 14930</td>
          <td>
            <b>100% CPU Usage network service when resending locally rejected samples</b><br/>
            <i>
              When a local datareader uses resource limits and runs into these
              resource limits, the networking service may go to 100% load when
              attempting to re-deliver a sample that is rejected due to the
              fact that the maximum resource limits have been reached.
              <br/>
              <b>Solution: The networking service now reschedules the delivery
              of a sample to a datareader for the next resolution tick instead
              of attempting the re-deliver in a busy-wait loop until it is
              delivered.</b>
            </i>
        </tr>
      </table>
    </p>
    <h2>6.5.0p5</h2>
    <p>
      <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-6034 / 14399
          </td>
          <td>
            <b>Adding support to timed out asyncronous calls in OpenSpliceRMI-Java</b>
            <br/>
            <i>
            OpenSpliceRMI waits for asynchronous calls replies infinitely by
            default. The setTimeout operation on the service proxy does not
            apply on asynchronous calls. So there is no way to control the time
            OpenSpliceRMI should wait for asynchronous replies.
             <br/>
             <b>Solution:Henceforth, synchronous and asynchronous calls are
             waited for during the same default duration whose value is 10 min.
             A new method has been added to the base asynchronous reply handler
             defined as
             "public void handleException(org.opensplice.DDS_RMI.SystemException exp)"
             to notify the user of timeout expiration (TIMEOUT exception) or of
             any other error occurred during the asynchronous reply delivery.
             The user reply handler should override this method to handle the
             errors. Asynchronous replies timeout could be set either with the
             CRuntime object, or with the proxy class, or with the reply handler
             class using the "setRepliesTimeout" method. The
             existing setTimeout() operation on the proxy class has been
             deprecated and replaced by setRepliesTimeout(). A new command line
             option called "--RMIClientSchedulingModel =<priority>" has been
             added to allow the user to set a priority to all the threads
             created at the client side including the thread that waits for the
             asynchronous replies. Please, refer to the OpenSpliceRMI
             documentation for more details. In order to benefit from these new
             features, the user should regenerate the rmipp code. Note that this
             asynchronous requests timeout support is currently available in
             Java only. The C++ support will be implemented in a future release.</b>
             </i>
           </td>
        </tr>
      </table>
    </p>
    <p>
      <h2>6.5.0</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-5899-1
          </td>
          <td>
            <b>Listener and waitset trigger order</b>
            <br/>
            <i>
             In the previous releases before V6.5 listeners and waitsets where
             triggered in random order but the specification prescribes that
             listeners are triggered before waitsets.
             <br/>
             <b>Solution: In V6.5 we have partly corrected the behavior, as it
             now behaves deterministic in the following order:
             <ul>
                <li>trigger the waitsets containing affected read and query condition.</li>
                <li>invoke the listeners.</li>
                <li>trigger waitsets containing affected status conditions.</li>
            <ul>
            In a future release we will correct the order of the first two items, as
            they should be in reverse order.<br>
            Important remark for existing applications:<br>
            Previously if a listener is set on a status condition of an entity and
            the same status condition is attached to a waitset then both the
            listener would be invoked and the waitset would return the status
            condition. In V6.5 the waitset will no longer return the status condition.
            The listener will reset the status condition before the waitset is
            triggered implying that the waitset will not return the status
            condition because it no longer evaluates true.<br>
            Additionally, if an application has not set a listener but accidentally
            has set the listener mask, the waitset will no longer return the
            status condtion. It appears that only a waitset is used on a status
            condition but in reality a default listener is set which will reset
            the status condition before the waitset is triggered.
            </b>
             </i>
           </td>
        </tr>
        <tr>
          <td>OSPL-5899-2
          </td>
          <td>
            <b>Listener mask behaviour change</b>
            <br/>
            <i>
            According to the DDS specification setting a listener mask
            for specific events without implementing a listener for those events
            (i.e. specifying a 'NULL listener') will act as if a listener is set
            and as a result will consume the events and not propagate events to
            listeners of its parents or waitsets. Before V6.5 setting a mask had
            no effect when not implementing a listener meaning that events where
            not consumed so parent listeners and waitsets where still triggered.
             <br/>
             <b>Solution: The issue is fixed, but applications must be aware that
             if the mask was set by accident, they might be missing events they
             have received before if listeners are set on parent entities or
             associated conditions are used in waitsets.</b>
             </i>
           </td>
        </tr>
     </table>
   </p>
     <p>
      <h2>6.4.1</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-4630/<br>12963
          </td>
          <td>
            <b>get_all_data_disposed_topic_status() method is now implemented</b><br/>
            <i>
            </br></br><b>Solution: The get_all_data_disposed_topic_status() method has
      been implemented in C++ and Java language bindings.
            </i>
          </td>
        </tr>
        </table>
      <h2>6.3.1p2</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-2733<br>
          </td>
          <td>
            <b>Change in QoS settings of Streams API</b><br/>
            <i>
            The QoS settings of the internal DataWriters of the Streams API allowed potentially
            unlimited memory usage, because the resource limits were not set.
            </br></br><b>Solution: The internal resource-limit QoS settings of a StreamDataWriter have been set to </br>max_samples = 10,
            to prevent potential unlimited memory usage.
            </i>
          </td>
        </tr>
        </table>
        </p>
    <p>
      <h2>6.3.0</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-505/OSPL-704<br>
          </td>
          <td>
            <b>OSPL_EXIT_CODEs are not useful in a shell.</b><br/>
            <i>
            When using ospl two return values OSPL_EXIT_CODE_RECOVERABLE_ERROR (-1) and
            OSPL_EXIT_CODE_UNRECOVERABLE_ERROR (-2) are indistinguishable in a shell.

            <b>Solution: The return codes are now postive +1 and +2</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>
          OSPL-1078/<br>10788
          </td>
          <td>
            <b>Support for parallel demarshalling should be available on Java an C++ </b><br/>
            <i>
               Demarshalling data to a language-binding specific format may take considerable
               processing depending on the type of the data. Since demarshalling is done by the
               application thread performing the read/take, demarshalling occurs single-threadedly,
               limiting the throughput that can be achieved.<br>
               <b>Solution:</b> The C++ and Java datareaders have been extended with support for
               demarshalling with multiple threads. The number of threads used can be controlled
               by using the new set_property(DDS::Property) operation on C++ and Java datareaders.
               The property "parallelReadThreadCount" accepts a string representing a positive integer
               (e.g., "4") as value. If the call was successful, successive read/take operations on
               that datareader will use the provided number of threads for the demarshalling step of
               the respective operations.
               <pre>
                  dr.set_property(new Property("parallelReadThreadCount", "4"));
               </pre>
            </i>
        </tr>
        <tr>
          <td>OSPL-1194<br>
          </td>
          <td>
            <b>Record and Replay: Removal of mode attribute</b><br/>
            <i>
            The 'mode' attribute of RnR XML storages has been removed because it
            was not clear how this setting should be used in different
            circumstances with new and/or existing storages.
            <b>Solution: The default behaviour is now to always append to a storage,
            when its resources (i.e. XML files) exist. If the resources do not exist
            they will be silently created, if possible. The overwrite-behaviour
            that was achieved by setting the mode to 'w', has been replaced by
            a separate TRUNCATE command. This command will remove all data from
            a storage, but it can only be processed if the storage is not open
            for recording and/or replaying.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1348<br>
          </td>
          <td>
            <b>Record and Replay: Removal of interest</b><br/>
            <i>
            <b>Solution: The RnR command API has been modified to allow removal
            of individual record/replay interest instead of forcibly removing all
            interest by stopping the scenario responsible for adding it. The API
            contains separate commands to either add or remove interest to record
            or replay data belonging to a specific partition/topic. For example,
            to stop recording data, a REMOVE_RECORD command must be issued,
            containing interest expressions that match exactly the expressions
            of the ADD_RECORD command that initiated the recording. Currently
            all interest will still be forcibly removed when a STOP_SCENARIO
            command is published but future releases will only allow interest
            to be removed by publishing remove commands.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1640<br>
          </td>
          <td>
            <b>Default for ospl changed</b><br/>
            <i>
            Inexperienced users finding ospl hard to use as the default isn't friendly.<br><br>
            <b>Solution: ospl used to stop by default. It now displays the usage.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1790<br>
          </td>
          <td>
            <b>Added missing functions to DataReaderView in the Standalone C++ language binding</b><br/>
            <i>
            Applications built on the Corba C++ language binding couldn't be built
            on the Standalone C++ language binding due to missing functions in the
            DataReaderView.
            <code><PRE>
            virtual ReadCondition_ptr create_readcondition (SampleStateMask sample_states,
              ViewStateMask view_states, InstanceStateMask instance_states) = 0;
            virtual QueryCondition_ptr create_querycondition (SampleStateMask sample_states,
              ViewStateMask view_states, InstanceStateMask instance_states,
              const DDS::Char* query_expression, const StringSeq& query_parameters) = 0;
            virtual ReturnCode_t delete_readcondition (ReadCondition_ptr a_condition) = 0;
            virtual ReturnCode_t delete_contained_entities () = 0;</PRE></code>
            <b>Solution: The DataReaderView in the Standalone C++ language binding
            is extended with the missing functions.</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-2019<br>
          </td>
          <td>
            <b>Function calls on deleted entities in ccpp and sacpp now correctly return already_deleted</b><br/>
            <i>
            Calling functions on deleted entities in ccpp and sacpp returned BAD_PARAMETER instead of ALREADY_DELETED. <br><br>
            <b>Solution: Code fixed to match OMG specification.</b>
            </i>
          </td>
        </tr>
      </table>

    <p>
      <h2>6.2.0</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>OSPL-347/<br>8919
          </td>
          <td>
            <b>Changes in Corba Co-habitation Java PSM</b><br/>
            <i>
            <b>Solution: In previous releases only a subset of the OpenSplice API was generated
              using a Corba ORB. Starting from this release, the entire Corba-Java API
              is generated by the IDL compiler that comes with the ORB. This enables
              customers to use internal types like ReturnCode_t and InstanceHandle_t
              in a Corba environment, i.e. Helper and Holder classes are now available
              for those types as well.

              Because all classes are now properly implementing Corba interfaces and
              inheriting from Corba base classes, derived classes are required to
              provide an implementation for all abstract operations mentioned in the
              generated interface. In case of Listeners, OpenSplice supplies the ListenerBase
              class as a convenience. When Listeners extend from this base class
              they are no longer required to implement these operations themselves.</b>

              <b>See detailed notes in 6.2 features section above</b>
            </i>
          </td>
        </tr>
        <tr>
          <td>OSPL-1160<br>
          </td>
          <td>
            <b>DDS_Service::register_interface C++ operation signature change</b><br/>
            <i>
              The DDS_Service::register_interface C++ operation accepts a raw pointer
              to the service implementation as parameter, whereas its implementation
              assigns it to a smart pointer. This situation can lead to a double free problem.
              <br><br>
              <b>Solution: The register_interface current signature is deprecated : <br><br>
                 <code>template<class T1, class T2> <br>
                 bool register_interface(T2 * svc_impl, const std::string & svc_name, int svc_instance_id)</code><br><br>
                 and adding a new one :<br><br>
                 <code>template <class T1, class T2> <br>
                 bool register_interface(shared_ptr<T2> svc_impl, const std::string & svc_name, int svc_instance_id)</code><br><br>
                 that accepts a smart pointer to the service implementation as paramter in stead of a raw one.</b>
            </i>
          </td>
        </tr>
      </table>
   <p>
       <h2>6.1.0p7</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
           <tr>
          <td>
            9963 /</br>
            dds3426<br>
            dds3478
          </td>
          <td>
            <b>DDSI2 used incorrect encoding for fragment data message headers</b><br/>
            <i>
             DDSI2 incorrectly used to generate and interpret fragmented data message headers
             as if they were slightly extended versions of the non-fragmented data message headers.
             This caused DDSI2 to be non-compliant with respect to the standard and to fail to
             interoperate with other vendors' implementations for large samples.
              <br><br>
              <b>Solution: The setting and interpretation has been corrected. This breaks backwards
              compatibility. For those exceptional cases where backwards compatibility is currently
              an issue, a setting Unsupported/LegacyFragmentation has been introduced,
              which may be set to true to continue using and interpreting the old message format.</b>
            </i>
          </td>
        </tr>
       </table>
     </p>
    <p>
       <h2>6.1.0</h2>
       <table width="90%">
        <tr>
          <th width="25%">
            Report ID.
          </th>
          <th width="75%">
            Description
          </th>
        </tr>
        <tr>
          <td>
          dds751
          </td>
          <td>
            <b>Change in ReaderLifecycle QoS (invalid sample setting)</b><br/>
            <i>
               The QoS setting that determines if OpenSplice creates invalid samples
               to communicate state changes, called 'enable_invalid_samples', is now deprecated.
               Invalid samples, in the past, could either be enabled or disabled.
               There is a replacement QoS setting, called 'invalid_sample_visibility',
               which accepts three values:<br>
               - MINIMUM_INVALID_SAMPLES: Acts like the old enable_invalid_samples = true, an invalid sample will be created if there's no regular sample on which the state change can be piggy-backed (this is the default behavior).<br>
               - NO_INVALID_SAMPLES: Acts like the old enable_invalid_samples = false, no invalid samples are created.<br>
               - ALL_INVALID_SAMPLES: Currently not implemented, but in the future will create an invalid samples for all state changes even if a regular sample is available.<br>
               Using the QoS deprecated setting will cause a message to be logged to the info log. This QoS will be removed from the product in the future. If applicable it is recommended to switch application code
               to the new invalid_sample_visibility setting instead.
              <br><br>
              <b>Solution: The default DataWriter reliability QoS is now set to RELIABLE in OpenSplice's API.</b></b>
          </td>
        </tr>
        <tr>
          <td>
          dds2676
          </td>
          <td>
            <b>Default DataWriter reliability QoS is set to BEST_EFFORT</b><br/>
            <i>
               OpenSplice default DataWriter reliability QoS was set to BEST_EFFORT.
               This was conflicting with the spec which states that default DataWriter
               reliability QoS should be RELIABLE.
              <br><br>
              <b>Solution: The default DataWriter reliability QoS is now set to RELIABLE in OpenSplice's API.</b></b>
          </td>
        </tr>
        <tr>
          <td>
          dds2806
          </td>
          <td>
            <b>Domain ID change from string to integer</b><br/>
            <i>
               The DDS API prescribes a create_participant() method to create a DomainParticipant.
               A DomainParticipant is the entry-point for an application to a specific Domain. To
               be able to create a DomainParticipant for a specific Domain, the create_participant()
               call requires a DomainId_t as parameter. In the specification the type of this DomainId_t
               is 'native' meaning that every vendor is free to choose its own type. In OpenSpliceDDS
               the type of a DomainId_t was a string and represented either the name of the Domain or a
               URI that points to the location of the configuration for the Domain to connect too.
               This has now become an integer domain ID. With this change OpenSpliceDDS now complies with
               other vendors on the DomainId_t type and the DDS interoperability specification (DDS-I)
               can use it now to select the port-number for discovery for a specific Domain.
              <br><br>
              <b>Solution: The feature is now implemented and described in the OpenSpliceDDS Reference Manuals.</b></b>
          </td>
        </tr>
        <tr>
          <td>
          dds3061
          </td>
          <td>
            <b>DDSI v1 removed from the codebase</b><br/>
            <i>
               The deprecated DDSIv1 code is now officially removed from the source code base and product APIs. DDSI2 should be used.
              <br><br>
              <b>Solution: The feature is now implemented and described in the OpenSpliceDDS Reference Manuals.</b>
          </td>
        </tr>
       </table>
     </p>
   <p>
    <div class="clear">
    <hr/>
    </div>
    <div id="footer">
      <div class="xright">
       Copyright &#169; 2015 <a href="http://www.prismtech.com">PrismTech</a>. All Rights Reserved.
      </div>
     <div class="clear">
     <hr/>
     </div>
    </div>
  </body>
</html>
